<div class="grid-displaycards">


    <div class="displaycards touchup-date" id="event-73464">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-0"></span>

        <script>
        add_bookmark_click(
            73464,
             1,
            'bookmark-number-0',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73464">Renku: a platform for sustainable data science</a>
        </div>
        <div class="type_display_name_virtual_card">Poster</div>
        <div class="author-str">Rok Roškar · Chandrasekhar Ramakrishnan · Michele Volpi · Fernando Perez-Cruz · Lilian
            Gasser · Firat Ozdemir · Patrick Paitz · Mohammad Alisafaee · Philipp Fischer · Ralf Grubenmann · Eliza
            Harris · Tasko Olevski · Carl Remlinger · Luis Salamanca · Elisabet Capon Garcia · Lorenzo Cavazzi · Jakub
            Chrobasik · Darlin Cordoba Osnas · Alessandro Degano · Jimena Dupre · Wesley Johnson · Eike Kettner · Laura
            Kinkead · Sean Murphy · Flora Thiebaut · Olivier Verscheure
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73464">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/73464-thumb.png?t=1701962720.2042656" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73464" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73464" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73464">
                    Abstract <i id="caret-73464" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73464">
            <div class="abstract-display">
                <p>Data and code working together is fundamental to machine learning (ML), but the context around
                    datasets and interactions between datasets and code are in general captured only rudimentarily.
                    Context such as how the dataset was prepared and created, what source data were used, what code was
                    used in processing, how the dataset evolved, and where it has been used and reused can provide much
                    insight, but this information is often poorly documented. That is unfortunate since it makes
                    datasets into black-boxes with potentially hidden characteristics that have downstream consequences.
                    We argue that making dataset preparation more accessible and dataset usage easier to record and
                    document would have significant benefits for the ML community: it would allow for greater diversity
                    in datasets by inviting modification to published sources, simplify use of alternative datasets and,
                    in doing so, make results more transparent and robust, while allowing for all contributions to be
                    adequately credited. We present a platform, Renku, designed to support and encourage such
                    sustainable development and use of data, datasets, and code, and we demonstrate its benefits through
                    a few illustrative projects which span the spectrum from dataset creation to dataset consumption and
                    showcasing.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71081">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-1"></span>

        <script>
        add_bookmark_click(
            71081,
             1,
            'bookmark-number-1',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71081">Semi-Supervised Domain Generalization with Known
                and Unknown Classes</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Lei Zhang · Ji-Fu Li · Wei Wang</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71081">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71081-thumb.png?t=1698305213.973598" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71081" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71081" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71081">
                    Abstract <i id="caret-71081" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71081">
            <div class="abstract-display">
                <p>Semi-Supervised Domain Generalization (SSDG) aims to learn a model that is generalizable to an unseen
                    target domain with only a few labels, and most existing SSDG methods assume that unlabeled training
                    and testing samples are all known classes. However, a more realistic scenario is that known classes
                    may be mixed with some unknown classes in unlabeled training and testing data. To deal with such a
                    scenario, we propose the Class-Wise Adaptive Exploration and Exploitation (CWAEE) method. In
                    particular, we explore unlabeled training data by using one-vs-rest classifiers and class-wise
                    adaptive thresholds to detect known and unknown classes, and exploit them by adopting consistency
                    regularization on augmented samples based on Fourier Transformation to improve the unseen domain
                    generalization. The experiments conducted on real-world datasets verify the effectiveness and
                    superiority of our method.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70489">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-2"></span>

        <script>
        add_bookmark_click(
            70489,
             1,
            'bookmark-number-2',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70489">Practical Sharpness-Aware Minimization Cannot
                Converge All the Way to Optima</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Dongkuk Si · Chulhee Yun</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70489">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70489" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70489" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70489">
                    Abstract <i id="caret-70489" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70489">
            <div class="abstract-display">
                Sharpness-Aware Minimization (SAM) is an optimizer that takes a descent step based on the gradient at a
                perturbation $y_t = x_t + \rho \frac{\nabla f(x_t)}{\lVert \nabla f(x_t) \rVert}$ of the current point
                $x_t$. Existing studies prove convergence of SAM for smooth functions, but they do so by assuming
                decaying perturbation size $\rho$ and/or no gradient normalization in $y_t$, which is detached from
                practice. To address this gap, we study deterministic/stochastic versions of SAM with practical
                configurations (i.e., constant $\rho$ and gradient normalization in $y_t$) and explore their convergence
                properties on smooth functions with (non)convexity assumptions.Perhaps surprisingly, in many scenarios,
                we find out that SAM has limited capability to converge to global minima or stationary points.For smooth
                strongly convex functions, we show that while deterministic SAM enjoys tight global convergence rates of
                $\tilde \Theta(\frac{1}{T^2})$, the convergence bound of stochastic SAM suffers an inevitable additive
                term $\mathcal O(\rho^2)$, indicating convergence only up to neighborhoods of optima.In fact, such
                $\mathcal O(\rho^2)$ factors arise for stochastic SAM in all the settings we consider, and also for
                deterministic SAM in nonconvex cases; importantly, we prove by examples that such terms are
                unavoidable.Our results highlight vastly different characteristics of SAM with vs. without decaying
                perturbation size …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72118">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-3"></span>

        <script>
        add_bookmark_click(
            72118,
             1,
            'bookmark-number-3',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72118">Implicit Bias of Gradient Descent for Logistic
                Regression at the Edge of Stability</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jingfeng Wu · Vladimir Braverman · Jason Lee</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72118">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72118-thumb.png?t=1699131547.4422717" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72118" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72118" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72118">
                    Abstract <i id="caret-72118" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72118">
            <div class="abstract-display">
                <p>Recent research has observed that in machine learning optimization, gradient descent (GD) often
                    operates at the edge of stability (EoS) [Cohen et al., 2021], where the stepsizes are set to be
                    large, resulting in non-monotonic losses induced by the GD iterates. This paper studies the
                    convergence and implicit bias of constant-stepsize GD for logistic regression on linearly separable
                    data in the EoS regime. Despite the presence of local oscillations, we prove that the logistic loss
                    can be minimized by GD with any constant stepsize over a long time scale. Furthermore, we prove that
                    with any constant stepsize, the GD iterates tend to infinity when projected to a max-margin
                    direction (the hard-margin SVM direction) and converge to a fixed vector that minimizes a strongly
                    convex potential when projected to the orthogonal complement of the max-margin direction. In
                    contrast, we also show that in the EoS regime, GD iterates may diverge catastrophically under the
                    exponential loss, highlighting the superiority of the logistic loss. These theoretical findings are
                    in line with numerical simulations and complement existing theories on the convergence and implicit
                    bias of GD for logistic regression, which are only applicable when the stepsizes are sufficiently
                    small.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70171">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-4"></span>

        <script>
        add_bookmark_click(
            70171,
             1,
            'bookmark-number-4',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70171">Uncovering motifs of concurrent signaling across
                multiple neuronal populations</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Evren Gokcen · Anna Jasper · Alison Xu · Adam Kohn · Christian Machens · Byron M Yu
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70171">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70171-thumb.png?t=1702056912.036062" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70171" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70171" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70171">
                    Abstract <i id="caret-70171" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70171">
            <div class="abstract-display">
                <p>Modern recording techniques now allow us to record from distinct neuronal populations in different
                    brain networks. However, especially as we consider multiple (more than two) populations, new
                    conceptual and statistical frameworks are needed to characterize the multi-dimensional, concurrent
                    flow of signals among these populations. Here, we develop a dimensionality reduction framework that
                    determines (1) the subset of populations described by each latent dimension, (2) the direction of
                    signal flow among those populations, and (3) how those signals evolve over time within and across
                    experimental trials. We illustrate these features in simulation, and further validate the method by
                    applying it to previously studied recordings from neuronal populations in macaque visual areas V1
                    and V2. Then we study interactions across select laminar compartments of areas V1, V2, and V3d,
                    recorded simultaneously with multiple Neuropixels probes. Our approach uncovered signatures of
                    selective communication across these three areas that related to their retinotopic alignment. This
                    work advances the study of concurrent signaling across multiple neuronal populations.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71542">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-5"></span>

        <script>
        add_bookmark_click(
            71542,
             1,
            'bookmark-number-5',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71542">Bayesian target optimisation for high-precision
                holographic optogenetics</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Marcus Triplett · Marta Gajowa · Hillel Adesnik · Liam Paninski</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71542">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71542-thumb.png?t=1702056211.8027954" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71542" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71542" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71542">
                    Abstract <i id="caret-71542" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71542">
            <div class="abstract-display">
                <p>Two-photon optogenetics has transformed our ability to probe the structure and function of neural
                    circuits. However, achieving precise optogenetic control of neural ensemble activity has remained
                    fundamentally constrained by the problem of off-target stimulation (OTS): the inadvertent activation
                    of nearby non-target neurons due to imperfect confinement of light onto target neurons. Here we
                    propose a novel computational approach to this problem called Bayesian target optimisation. Our
                    approach uses nonparametric Bayesian inference to model neural responses to optogenetic stimulation,
                    and then optimises the laser powers and optical target locations needed to achieve a desired
                    activity pattern with minimal OTS. We validate our approach in simulations and using data from in
                    vitro experiments, showing that Bayesian target optimisation considerably reduces OTS across all
                    conditions we test. Together, these results establish our ability to overcome OTS, enabling
                    optogenetic stimulation with substantially improved precision.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70799">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-6"></span>

        <script>
        add_bookmark_click(
            70799,
             1,
            'bookmark-number-6',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70799">Distributionally Robust Linear Quadratic
                Control</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Bahar Taskesen · Dan Iancu · Çağıl Koçyiğit · Daniel Kuhn</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70799">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70799" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70799" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70799">
                    Abstract <i id="caret-70799" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70799">
            <div class="abstract-display">
                <p>Linear-Quadratic-Gaussian (LQG) control is a fundamental control paradigm that is studied in various
                    fields such as engineering, computer science, economics, and neuroscience. It involves controlling a
                    system with linear dynamics and imperfect observations, subject to additive noise, with the goal of
                    minimizing a quadratic cost function for the state and control variables. In this work, we consider
                    a generalization of the discrete-time, finite-horizon LQG problem, where the noise distributions are
                    unknown and belong to Wasserstein ambiguity sets centered at nominal (Gaussian) distributions. The
                    objective is to minimize a worst-case cost across all distributions in the ambiguity set, including
                    non-Gaussian distributions. Despite the added complexity, we prove that a control policy that is
                    linear in the observations is optimal for this problem, as in the classic LQG problem. We propose a
                    numerical solution method that efficiently characterizes this optimal control policy. Our method
                    uses the Frank-Wolfe algorithm to identify the least-favorable distributions within the Wasserstein
                    ambiguity sets and computes the controller's optimal policy using Kalman filter estimation under
                    these distributions.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70527">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-7"></span>

        <script>
        add_bookmark_click(
            70527,
             1,
            'bookmark-number-7',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70527">Robust Distributed Learning: Tight Error Bounds and
                Breakdown Point under Data Heterogeneity</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Youssef Allouah · Rachid Guerraoui · Nirupam Gupta · Rafael Pinot · Geovani Rizk</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70527">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70527" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70527" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70527">
                    Abstract <i id="caret-70527" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70527">
            <div class="abstract-display">
                The theory underlying robust distributed learning algorithms, designed to resist adversarial machines,
                matches empirical observations when data is homogeneous. Under data heterogeneity however, which is the
                norm in practical scenarios, established lower bounds on the learning error are essentially vacuous and
                greatly mismatch empirical observations. This is because the heterogeneity model considered is too
                restrictive and does not cover basic learning tasks such as least-squares regression. We consider in
                this paper a more realistic heterogeneity model, namely $(G,B)$-gradient dissimilarity, and show that it
                covers a larger class of learning problems than existing theory. Notably, we show that the breakdown
                point under heterogeneity is lower than the classical fraction $\frac{1}{2}$. We also prove a new lower
                bound on the learning error of any distributed learning algorithm. We derive a matching upper bound for
                a robust variant of distributed gradient descent, and empirically show that our analysis reduces the gap
                between theory and practice.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73472">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-8"></span>

        <script>
        add_bookmark_click(
            73472,
             1,
            'bookmark-number-8',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73472">LightZero: A Unified Benchmark for Monte Carlo Tree
                Search in General Sequential Decision Scenarios</a>
        </div>
        <div class="type_display_name_virtual_card">Poster</div>
        <div class="author-str">Yazhe Niu · YUAN PU · Zhenjie Yang · Xueyan Li · Tong Zhou · Jiyuan Ren · Shuai Hu ·
            Hongsheng Li · Yu Liu
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73472">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/73472-thumb.png?t=1702049923.4565887" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73472" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73472" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73472">
                    Abstract <i id="caret-73472" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73472">
            <div class="abstract-display">
                <p>Building agents based on tree-search planning capabilities with learned models has achieved
                    remarkable success in classic decision-making problems, such as Go and Atari.However, it has been
                    deemed challenging or even infeasible to extend Monte Carlo Tree Search (MCTS) based algorithms to
                    diverse real-world applications, especially when these environments involve complex action spaces
                    and significant simulation costs, or inherent stochasticity.In this work, we introduce LightZero,
                    the first unified benchmark for deploying MCTS/MuZero in general sequential decision scenarios.
                    Specificially, we summarize the most critical challenges in designing a general MCTS-style
                    decision-making solver, then decompose the tightly-coupled algorithm and system design of
                    tree-search RL methods into distinct sub-modules.By incorporating more appropriate exploration and
                    optimization strategies, we can significantly enhance these sub-modules and construct powerful
                    LightZero agents to tackle tasks across a wide range of domains, such as board games, Atari, MuJoCo,
                    MiniGrid and GoBigger.Detailed benchmark results reveal the significant potential of such methods in
                    building scalable and efficient decision intelligence.The code is available as part of OpenDILab at
                    https://github.com/opendilab/LightZero.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72416">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-9"></span>

        <script>
        add_bookmark_click(
            72416,
             1,
            'bookmark-number-9',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72416">Newton–Cotes Graph Neural Networks: On the Time
                Evolution of Dynamic Systems</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Lingbing Guo · Weiqing Wang · Zhuo Chen · Ningyu Zhang · Zequn Sun · Yixuan Lai · Qiang
            Zhang · Huajun Chen
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72416">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72416-thumb.png?t=1697712116.0855134" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72416" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72416" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72416">
                    Abstract <i id="caret-72416" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72416">
            <div class="abstract-display">
                <p>Reasoning system dynamics is one of the most important analytical approaches for many scientific
                    studies. With the initial state of a system as input, the recent graph neural networks (GNNs)-based
                    methods are capable of predicting the future state distant in time with high accuracy. Although
                    these methods have diverse designs in modeling the coordinates and interacting forces of the system,
                    we show that they actually share a common paradigm that learns the integration of the velocity over
                    the interval between the initial and terminal coordinates. However, their integrand is constant
                    w.r.t. time. Inspired by this observation, we propose a new approach to predict the integration
                    based on several velocity estimations with Newton–Cotes formulas and prove its effectiveness
                    theoretically. Extensive experiments on several benchmarks empirically demonstrate consistent and
                    significant improvement compared with the state-of-the-art methods.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72412">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-10"></span>

        <script>
        add_bookmark_click(
            72412,
             1,
            'bookmark-number-10',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72412">List and Certificate Complexities in Replicable
                Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Peter Dixon · A. Pavan · Jason Vander Woude · N. V. Vinodchandran</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72412">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72412" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72412" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72412">
                    Abstract <i id="caret-72412" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72412">
            <div class="abstract-display">
                We investigate replicable learning algorithms. Informally a learning algorithm is replicable if the
                algorithm outputs the same canonical hypothesis over multiple runs with high probability, even when
                different runs observe a different set of samples from the unknown data distribution. In general, such a
                strong notion of replicability is not achievable. Thus we consider two feasible notions of replicability
                called {\em list replicability} and {\em certificate replicability}. Intuitively, these notions capture
                the degree of (non) replicability. The goal is to design learning algorithms with optimal list and
                certificate complexities while minimizing the sample complexity. Our contributions are the following.1.
                We first study the learning task of estimating the biases of $d$ coins, up to an additive error of
                $\varepsilon$, by observing samples. For this task, we design a $(d+1)$-list replicable algorithm. To
                complement this result, we establish that the list complexity is optimal, i.e there are no learning
                algorithms with a list size smaller than $d+1$ for this task. We also design learning algorithms with
                certificate complexity $\tilde{O}(\log d)$. The sample complexity of both these algorithms is
                $\tilde{O}(\frac{d^2}{\varepsilon^2})$ where $\varepsilon$ is the approximation error parameter (for a
                constant error probability). 2. In the PAC model, we show that any hypothesis …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73072">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-11"></span>

        <script>
        add_bookmark_click(
            73072,
             1,
            'bookmark-number-11',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73072">Stochastic Multi-armed Bandits: Optimal Trade-off
                among Optimality, Consistency, and Tail Risk</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">David Simchi-Levi · Zeyu Zheng · Feng Zhu</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73072">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/73072-thumb.png?t=1697228594.0541027" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73072" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73072" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73072">
                    Abstract <i id="caret-73072" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73072">
            <div class="abstract-display">
                We consider the stochastic multi-armed bandit problem and fully characterize the interplays among three
                desired properties for policy design: worst-case optimality, instance-dependent consistency, and
                light-tailed risk. We show how the order of expected regret exactly affects the decaying rate of the
                regret tail probability for both the worst-case and instance-dependent scenario. A novel policy is
                proposed to achieve the optimal regret tail risk for any regret threshold. Concretely, for any given
                $\alpha\in[1/2, 1)$ and $\beta\in[0, 1)$, our policy achieves a worst-case expected regret of $\tilde
                O(T^\alpha)$ and instance-dependent expected regret of $\tilde O(T^\beta)$, while enjoys a probability
                of incurring an $\Omega(T^\delta)$ regret that decays exponentially with a polynomial $T$ term. Such
                decaying rate is proved to be best achievable. We also generalize our analysis to the stochastic
                multi-armed bandit problem with non-stationary baseline rewards, where in each time period $t$, the
                decision maker pulls one of $K$ arms and collects a reward which is the sum of three terms: the mean of
                the pulled arm, an independent noise, and a non-stationary baseline reward as a function of $t$. Our
                results reveal insights on the trade-off between expected regret and tail risk for both worst-case and
                instance-dependent scenario, indicating that more …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71377">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-12"></span>

        <script>
        add_bookmark_click(
            71377,
             1,
            'bookmark-number-12',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71377">On the Planning Abilities of Large Language Models
                - A Critical Investigation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Karthik Valmeekam · Matthew Marquez · Sarath Sreedharan · Subbarao Kambhampati</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71377">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71377" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71377" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71377">
                    Abstract <i id="caret-71377" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71377">
            <div class="abstract-display">
                <p>Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in
                    this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the
                    effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the
                    potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning
                    tasks. We conduct a systematic study by generating a suite of instances on domains similar to the
                    ones employed in the International Planning Competition and evaluate LLMs in two distinct modes:
                    autonomous and heuristic. Our findings reveal that LLMs’ ability to generate executable plans
                    autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12%
                    across the domains. However, the results in the heuristic mode show more promise. In the heuristic
                    mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound
                    planners and additionally show that external verifiers can help provide feedback on the generated
                    plans and back-prompt the LLM for better plan generation.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71215">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-13"></span>

        <script>
        add_bookmark_click(
            71215,
             1,
            'bookmark-number-13',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71215">Differentially Private Approximate Near Neighbor
                Counting in High Dimensions</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Alexandr Andoni · Piotr Indyk · Sepideh Mahabadi · Shyam Narayanan</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71215">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71215" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71215" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71215">
                    Abstract <i id="caret-71215" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71215">
            <div class="abstract-display">
                Range counting (e.g., counting the number of data points falling into a given query ball) under
                differential privacy has been studied extensively. However, the current algorithms for this problem are
                subject to the following dichotomy. One class of algorithms suffers from an additive error that is a
                fixed polynomial in the number of points. Another class of algorithms allows for polylogarithmic
                additive error, but the error grows exponentially in the dimension. To achieve the latter, the problem
                is relaxed to allow a “fuzzy” definition of the range boundary, e.g., a count of the points in a ball of
                radius $r$ might also include points in a ball of radius $cr$ for some $c&gt;1$. In this paper we
                present an efficient algorithm that offers a sweet spot between these two classes. The algorithm has an
                additive error that is an arbitrary small power of the data set size, depending on how fuzzy the range
                boundary is, as well as a small ($1+o(1)$) multiplicative error. Crucially, the amount of noise added
                has no dependence on the dimension. Our algorithm introduces a variant of Locality-Sensitive Hashing,
                utilizing it in a novel manner.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71202">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-14"></span>

        <script>
        add_bookmark_click(
            71202,
             1,
            'bookmark-number-14',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71202">Feature Adaptation for Sparse Linear Regression</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jonathan Kelner · Frederic Koehler · Raghu Meka · Dhruv Rohatgi</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71202">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71202" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71202" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71202">
                    Abstract <i id="caret-71202" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71202">
            <div class="abstract-display">
                Sparse linear regression is a central problem in high-dimensional statistics. We study the correlated
                random design setting, where the covariates are drawn from a multivariate Gaussian $N(0,\Sigma)$, and we
                seek an estimator with small excess risk. If the true signal is $t$-sparse, information-theoretically,
                it is possible to achieve strong recovery guarantees with only $O(t\log n)$ samples. However,
                computationally efficient algorithms have sample complexity linear in (some variant of) the *condition
                number* of $\Sigma$. Classical algorithms such as the Lasso can require significantly more samples than
                necessary even if there is only a single sparse approximate dependency among the covariates.We provide a
                polynomial-time algorithm that, given $\Sigma$, automatically adapts the Lasso to tolerate a small
                number of approximate dependencies. In particular, we achieve near-optimal sample complexity for
                constant sparsity and if $\Sigma$ has few ``outlier'' eigenvalues.Our algorithm fits into a broader
                framework of *feature adaptation* for sparse linear regression with ill-conditioned covariates. With
                this framework, we additionally provide the first polynomial-factor improvement over brute-force search
                for constant sparsity $t$ and arbitrary covariance $\Sigma$.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73453">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-15"></span>

        <script>
        add_bookmark_click(
            73453,
             1,
            'bookmark-number-15',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73453">Holistic Evaluation of Text-to-Image Models</a>
        </div>
        <div class="type_display_name_virtual_card">Poster</div>
        <div class="author-str">Tony Lee · Michihiro Yasunaga · Chenlin Meng · Yifan Mai · Joon Sung Park · Agrim Gupta
            · Yunzhi Zhang · Deepak Narayanan · Hannah Teufel · Marco Bellagente · Minguk Kang · Taesung Park · Jure
            Leskovec · Jun-Yan Zhu · Fei-Fei Li · Jiajun Wu · Stefano Ermon · Percy Liang
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73453">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/73453-thumb.png?t=1701416414.071598" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73453" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73453" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73453">
                    Abstract <i id="caret-73453" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73453">
            <div class="abstract-display">
                <p>The stunning qualitative improvement of text-to-image models has led to their widespread attention
                    and adoption. However, we lack a comprehensive quantitative understanding of their capabilities and
                    risks. To fill this gap, we introduce a new benchmark, Holistic Evaluation of Text-to-Image Models
                    (HEIM). Whereas previous evaluations focus mostly on image-text alignment and image quality, we
                    identify 12 aspects, including text-image alignment, image quality, aesthetics, originality,
                    reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. We
                    curate 62 scenarios encompassing these aspects and evaluate 26 state-of-the-art text-to-image models
                    on this benchmark. Our results reveal that no single model excels in all aspects, with different
                    models demonstrating different strengths. We release the generated images and human evaluation
                    results for full transparency at https://crfm.stanford.edu/heim/latest and the code at
                    https://github.com/stanford-crfm/helm, which is integrated with the HELM codebase</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71244">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-16"></span>

        <script>
        add_bookmark_click(
            71244,
             1,
            'bookmark-number-16',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71244">ResShift: Efficient Diffusion Model for Image
                Super-resolution by Residual Shifting</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zongsheng Yue · Jianyi Wang · Chen Change Loy</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71244">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71244-thumb.png?t=1699871487.999814" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71244" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71244" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71244">
                    Abstract <i id="caret-71244" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71244">
            <div class="abstract-display">
                <p>Diffusion-based image super-resolution (SR) methods are mainly limited by the low inference speed due
                    to the requirements of hundreds or even thousands of sampling steps. Existing acceleration sampling
                    techniques inevitably sacrifice performance to some extent, leading to over-blurry SR results. To
                    address this issue, we propose a novel and efficient diffusion model for SR that significantly
                    reduces the number of diffusion steps, thereby eliminating the need for post-acceleration during
                    inference and its associated performance deterioration. Our method constructs a Markov chain that
                    transfers between the high-resolution image and the low-resolution image by shifting the residual
                    between them, substantially improving the transition efficiency. Additionally, an elaborate noise
                    schedule is developed to flexibly control the shifting speed and the noise strength during the
                    diffusion process. Extensive experiments demonstrate that the proposed method obtains superior or at
                    least comparable performance to current state-of-the-art methods on both synthetic and real-world
                    datasets, \textit{\textbf{even only with 20 sampling steps}}. Our code and model will be made
                    publicly.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71784">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-17"></span>

        <script>
        add_bookmark_click(
            71784,
             1,
            'bookmark-number-17',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71784">Differentially Private Image Classification by
                Learning Priors from Random Processes</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Xinyu Tang · Ashwinee Panda · Vikash Sehwag · Prateek Mittal</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71784">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71784-thumb.png?t=1700625254.4424179" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71784" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71784" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71784">
                    Abstract <i id="caret-71784" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71784">
            <div class="abstract-display">
                In privacy-preserving machine learning, differentially private stochastic gradient descent (DP-SGD)
                performs worse than SGD due to per-sample gradient clipping and noise addition.A recent focus in private
                learning research is improving the performance of DP-SGD on private data by incorporating priors that
                are learned on real-world public data.In this work, we explore how we can improve the privacy-utility
                tradeoff of DP-SGD by learning priors from images generated by random processes and transferring these
                priors to private data. We propose DP-RandP, a three-phase approach. We attain new state-of-the-art
                accuracy when training from scratch on CIFAR10, CIFAR100, MedMNIST and ImageNet for a range of privacy
                budgets $\\varepsilon \\in [1, 8]$. In particular, we improve the previous best reported accuracy on
                CIFAR10 from $60.6 \\%$ to $72.3 \\%$ for $\\varepsilon=1$.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72520">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-18"></span>

        <script>
        add_bookmark_click(
            72520,
             1,
            'bookmark-number-18',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72520">Extraction and Recovery of Spatio-Temporal
                Structure in Latent Dynamics Alignment with Diffusion Model</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yule Wang · Zijing Wu · Chengrui Li · Anqi Wu</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72520">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72520-thumb.png?t=1701561495.2937324" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72520" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72520" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72520">
                    Abstract <i id="caret-72520" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72520">
            <div class="abstract-display">
                <p>In the field of behavior-related brain computation, it is necessary to align raw neural signals
                    against the drastic domain shift among them. A foundational framework within neuroscience research
                    posits that trial-based neural population activities rely on low-dimensional latent dynamics, thus
                    focusing on the latter greatly facilitates the alignment procedure. Despite this field's progress,
                    existing methods ignore the intrinsic spatio-temporal structure during the alignment phase. Hence,
                    their solutions usually lead to poor quality in latent dynamics structures and overall performance.
                    To tackle this problem, we propose an alignment method ERDiff, which leverages the expressivity of
                    the diffusion model to preserve the spatio-temporal structure of latent dynamics. Specifically, the
                    latent dynamics structures of the source domain are first extracted by a diffusion model. Then,
                    under the guidance of this diffusion model, such structures are well-recovered through a maximum
                    likelihood alignment procedure in the target domain. We first demonstrate the effectiveness of our
                    proposed method on a synthetic dataset. Then, when applied to neural recordings from the non-human
                    primate motor cortex, under both cross-day and inter-subject settings, our method consistently
                    manifests its capability of preserving the spatio-temporal structure of latent dynamics and
                    outperforms existing approaches in alignment goodness-of-fit and neural decoding performance.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71943">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-19"></span>

        <script>
        add_bookmark_click(
            71943,
             1,
            'bookmark-number-19',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71943">Training shallow ReLU networks on noisy data using
                hinge loss: when do we overfit and is it benign?</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Erin George · Michael Murray · William Swartworth · Deanna Needell</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71943">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71943-thumb.png?t=1701378369.6921606" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71943" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71943" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71943">
                    Abstract <i id="caret-71943" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71943">
            <div class="abstract-display">
                <p>We study benign overfitting in two-layer ReLU networks trained using gradient descent and hinge loss
                    on noisy data for binary classification. In particular, we consider linearly separable data for
                    which a relatively small proportion of labels are corrupted or flipped. We identify conditions on
                    the margin of the clean data that give rise to three distinct training outcomes: benign overfitting,
                    in which zero loss is achieved and with high probability test data is classified correctly;
                    overfitting, in which zero loss is achieved but test data is misclassified with probability lower
                    bounded by a constant; and non-overfitting, in which clean points, but not corrupt points, achieve
                    zero loss and again with high probability test data is classified correctly. Our analysis provides a
                    fine-grained description of the dynamics of neurons throughout training and reveals two distinct
                    phases: in the first phase clean points achieve close to zero loss, in the second phase clean points
                    oscillate on the boundary of zero loss while corrupt points either converge towards zero loss or are
                    eventually zeroed by the network. We prove these results using a combinatorial approach that
                    involves bounding the number of clean versus corrupt updates during these phases of training.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70403">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-20"></span>

        <script>
        add_bookmark_click(
            70403,
             1,
            'bookmark-number-20',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70403">Learning Generalizable Agents via Saliency-guided
                Features Decorrelation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Sili Huang · Yanchao Sun · Jifeng Hu · Siyuan Guo · Hechang Chen · Yi Chang · Lichao Sun
            · Bo Yang
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70403">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70403-thumb.png?t=1696914592.3011677" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70403" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70403" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70403">
                    Abstract <i id="caret-70403" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70403">
            <div class="abstract-display">
                <p>In visual-based Reinforcement Learning (RL), agents often struggle to generalize well to
                    environmental variations in the state space that were not observed during training. The variations
                    can arise in both task-irrelevant features, such as background noise, and task-relevant features,
                    such as robot configurations, that are related to the optimal decisions. To achieve generalization
                    in both situations, agents are required to accurately understand the impact of changed features on
                    the decisions, i.e., establishing the true associations between changed features and decisions in
                    the policy model. However, due to the inherent correlations among features in the state space, the
                    associations between features and decisions become entangled, making it difficult for the policy to
                    distinguish them. To this end, we propose Saliency-Guided Features Decorrelation (SGFD) to eliminate
                    these correlations through sample reweighting. Concretely, SGFD consists of two core techniques:
                    Random Fourier Functions (RFF) and the saliency map. RFF is utilized to estimate the complex
                    non-linear correlations in high-dimensional images, while the saliency map is designed to identify
                    the changed features. Under the guidance of the saliency map, SGFD employs sample reweighting to
                    minimize the estimated correlations related to changed features, thereby achieving decorrelation in
                    visual RL tasks. Our experimental results demonstrate that SGFD …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70678">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-21"></span>

        <script>
        add_bookmark_click(
            70678,
             1,
            'bookmark-number-21',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70678">Statistical Guarantees for Variational Autoencoders
                using PAC-Bayesian Theory</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Sokhna Diarra Mbacke · Florence Clerc · Pascal Germain</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70678">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70678-thumb.png?t=1701985569.3903756" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70678" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70678" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70678">
                    Abstract <i id="caret-70678" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70678">
            <div class="abstract-display">
                <p>Since their inception, Variational Autoencoders (VAEs) have become central in machine learning.
                    Despite their widespread use, numerous questions regarding their theoretical properties remain open.
                    Using PAC-Bayesian theory, this work develops statistical guarantees for VAEs. First, we derive the
                    first PAC-Bayesian bound for posterior distributions conditioned on individual samples from the
                    data-generating distribution. Then, we utilize this result to develop generalization guarantees for
                    the VAE's reconstruction loss, as well as upper bounds on the distance between the input and the
                    regenerated distributions. More importantly, we provide upper bounds on the Wasserstein distance
                    between the input distribution and the distribution defined by the VAE's generative model.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70086">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-22"></span>

        <script>
        add_bookmark_click(
            70086,
             1,
            'bookmark-number-22',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70086">Train Once, Get a Family: State-Adaptive Balances
                for Offline-to-Online Reinforcement Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Shenzhi Wang · Qisen Yang · Jiawei Gao · Matthieu Lin · HAO CHEN · Liwei Wu · Ning Jia ·
            Shiji Song · Gao Huang
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70086">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70086-thumb.png?t=1697171314.1079817" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70086" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70086" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70086">
                    Abstract <i id="caret-70086" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70086">
            <div class="abstract-display">
                <p>Offline-to-online reinforcement learning (RL) is a training paradigm that combines pre-training on a
                    pre-collected dataset with fine-tuning in an online environment. However, the incorporation of
                    online fine-tuning can intensify the well-known distributional shift problem. Existing solutions
                    tackle this problem by imposing a policy constraint on the policy improvement objective in both
                    offline and online learning. They typically advocate a single balance between policy improvement and
                    constraints across diverse data collections. This one-size-fits-all manner may not optimally
                    leverage each collected sample due to the significant variation in data quality across different
                    states. To this end, we introduce Family Offline-to-Online RL (FamO2O), a simple yet effective
                    framework that empowers existing algorithms to determine state-adaptive improvement-constraint
                    balances. FamO2O utilizes a universal model to train a family of policies with different
                    improvement/constraint intensities, and a balance model to select a suitable policy for each state.
                    Theoretically, we prove that state-adaptive balances are necessary for achieving a higher policy
                    performance upper bound. Empirically, extensive experiments show that FamO2O offers a statistically
                    significant improvement over various existing methods, achieving state-of-the-art performance on the
                    D4RL benchmark. Codes are available at https://github.com/LeapLabTHU/FamO2O.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73455">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-23"></span>

        <script>
        add_bookmark_click(
            73455,
             1,
            'bookmark-number-23',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73455">Stable Bias: Evaluating Societal Representations in
                Diffusion Models</a>
        </div>
        <div class="type_display_name_virtual_card">Poster</div>
        <div class="author-str">Sasha Luccioni · Christopher Akiki · Margaret Mitchell · Yacine Jernite</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73455">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73455" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73455" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73455">
                    Abstract <i id="caret-73455" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73455">
            <div class="abstract-display">
                <p>As machine learning-enabled Text-to-Image (TTI) systems are becoming increasingly prevalent and
                    seeing growing adoption as commercial services, characterizing the social biases they exhibit is a
                    necessary first step to lowering their risk of discriminatory outcomes. This evaluation, however, is
                    made more difficult by the synthetic nature of these systems’ outputs: common definitions of
                    diversity are grounded in social categories of people living in the world, whereas the artificial
                    depictions of fictive humans created by these systems have no inherent gender or ethnicity. To
                    address this need, we propose a new method for exploring the social biases in TTI systems. Our
                    approach relies on characterizing the variation in generated images triggered by enumerating gender
                    and ethnicity markers in the prompts, and comparing it to the variation engendered by spanning
                    different professions. This allows us to (1) identify specific bias trends, (2) provide targeted
                    scores to directly compare models in terms of diversity and representation, and (3) jointly model
                    interdependent social variables to support a multidimensional analysis. We leverage this method to
                    analyze images generated by 3 popular TTI systems (Dall·E 2 , Stable Diffusion v 1.4 and 2) and find
                    that while all of their outputs show correlations with US labor …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72587">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-24"></span>

        <script>
        add_bookmark_click(
            72587,
             1,
            'bookmark-number-24',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72587">Reinforcement-Enhanced Autoregressive Feature
                Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Dongjie Wang · Meng Xiao · Min Wu · pengfei wang · Yuanchun Zhou · Yanjie Fu</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72587">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72587-thumb.png?t=1701768535.6482239" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72587" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72587" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72587">
                    Abstract <i id="caret-72587" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72587">
            <div class="abstract-display">
                <p>Feature transformation aims to generate new pattern-discriminative feature space from original
                    features to improve downstream machine learning (ML) task performances. However, the discrete search
                    space for the optimal feature explosively grows on the basis of combinations of features and
                    operations from low-order forms to high-order forms. Existing methods, such as exhaustive search,
                    expansion reduction, evolutionary algorithms, reinforcement learning, and iterative greedy, suffer
                    from large search space. Overly emphasizing efficiency in algorithm design usually sacrifice
                    stability or robustness. To fundamentally fill this gap, we reformulate discrete feature
                    transformation as a continuous space optimization task and develop an
                    embedding-optimization-reconstruction framework. This framework includes four steps: 1)
                    reinforcement-enhanced data preparation, aiming to prepare high-quality transformation-accuracy
                    training data; 2) feature transformation operation sequence embedding, intending to encapsulate the
                    knowledge of prepared training data within a continuous space; 3) gradient-steered optimal embedding
                    search, dedicating to uncover potentially superior embeddings within the learned space; 4)
                    transformation operation sequence reconstruction, striving to reproduce the feature transformation
                    solution to pinpoint the optimal feature space. Finally, extensive experiments and case studies are
                    performed to demonstrate the effectiveness and robustness of the proposed method. The code and data
                    are publicly accessible
                    https://www.dropbox.com/sh/imh8ckui7va3k5u/AACulQegVx0MuywYyoCqSdVPa?dl=0.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70416">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-25"></span>

        <script>
        add_bookmark_click(
            70416,
             1,
            'bookmark-number-25',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70416">PRODIGY: Enabling In-context Learning Over
                Graphs</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Qian Huang · Hongyu Ren · Peng Chen · Gregor Kržmanc · Daniel Zeng · Percy Liang · Jure
            Leskovec
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70416">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70416-thumb.png?t=1701715523.1969802" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70416" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70416" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70416">
                    Abstract <i id="caret-70416" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70416">
            <div class="abstract-display">
                <p>In-context learning is the ability of a pretrained model to adapt to novel and diverse downstream
                    tasks by conditioning on prompt examples, without optimizing any parameters. While large language
                    models have demonstrated this ability, how in-context learning could be performed over graphs is
                    unexplored. In this paper, we develop \textbf{Pr}etraining \textbf{O}ver \textbf{D}iverse
                    \textbf{I}n-Context \textbf{G}raph S\textbf{y}stems (PRODIGY), the first pretraining framework that
                    enables in-context learning over graphs. The key idea of our framework is to formulate in-context
                    learning over graphs with a novel \emph{prompt graph} representation, which connects prompt examples
                    and queries. We then propose a graph neural network architecture over the prompt graph and a
                    corresponding family of in-context pretraining objectives. With PRODIGY, the pretrained model can
                    directly perform novel downstream classification tasks on unseen graphs via in-context learning. We
                    provide empirical evidence of the effectiveness of our framework by showcasing its strong in-context
                    learning performance on tasks involving citation networks and knowledge graphs. Our approach
                    outperforms the in-context learning accuracy of contrastive pretraining baselines with hard-coded
                    adaptation by 18\% on average across all setups. Moreover, it also outperforms standard finetuning
                    with limited data by 33\% on average with in-context learning.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71975">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-26"></span>

        <script>
        add_bookmark_click(
            71975,
             1,
            'bookmark-number-26',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71975">Explaining the Uncertain: Stochastic Shapley Values
                for Gaussian Process Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Siu Lun Chau · Krikamol Muandet · Dino Sejdinovic</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71975">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71975-thumb.png?t=1700004530.030351" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71975" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71975" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71975">
                    Abstract <i id="caret-71975" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71975">
            <div class="abstract-display">
                <p>We present a novel approach for explaining Gaussian processes (GPs) that can utilize the full
                    analytical covariance structure present in GPs. Our method is based on the popular solution concept
                    of Shapley values extended to stochastic cooperative games, resulting in explanations that are
                    random variables. The GP explanations generated using our approach satisfy similar favorable axioms
                    to standard Shapley values and possess a tractable covariance function across features and data
                    observations. This covariance allows for quantifying explanation uncertainties and studying the
                    statistical dependencies between explanations. We further extend our framework to the problem of
                    predictive explanation, and propose a Shapley prior over the explanation function to predict Shapley
                    values for new data based on previously computed ones. Our extensive illustrations demonstrate the
                    effectiveness of the proposed approach.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72108">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-27"></span>

        <script>
        add_bookmark_click(
            72108,
             1,
            'bookmark-number-27',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72108">Conditional independence testing under misspecified
                inductive biases</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Felipe Maia Polo · Yuekai Sun · Moulinath Banerjee</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72108">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72108" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72108" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72108">
                    Abstract <i id="caret-72108" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72108">
            <div class="abstract-display">
                <p>Conditional independence (CI) testing is a fundamental and challenging task in modern statistics and
                    machine learning. Many modern methods for CI testing rely on powerful supervised learning methods to
                    learn regression functions or Bayes predictors as an intermediate step; we refer to this class of
                    tests as regression-based tests. Although these methods are guaranteed to control Type-I error when
                    the supervised learning methods accurately estimate the regression functions or Bayes predictors of
                    interest, their behavior is less understood when they fail due to misspecified inductive biases; in
                    other words, when the employed models are not flexible enough or when the training algorithm does
                    not induce the desired predictors. Then, we study the performance of regression-based CI tests under
                    misspecified inductive biases. Namely, we propose new approximations or upper bounds for the testing
                    errors of three regression-based tests that depend on misspecification errors. Moreover, we
                    introduce the Rao-Blackwellized Predictor Test (RBPT), a regression-based CI test robust against
                    misspecified inductive biases. Finally, we conduct experiments with artificial and real data,
                    showcasing the usefulness of our theory and methods.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72517">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-28"></span>

        <script>
        add_bookmark_click(
            72517,
             1,
            'bookmark-number-28',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72517">Partial Counterfactual Identification of Continuous
                Outcomes with a Curvature Sensitivity Model</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Valentyn Melnychuk · Dennis Frauen · Stefan Feuerriegel</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72517">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72517-thumb.png?t=1701185231.972139" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72517" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72517" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72517">
                    Abstract <i id="caret-72517" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72517">
            <div class="abstract-display">
                <p>Counterfactual inference aims to answer retrospective "what if" questions and thus belongs to the
                    most fine-grained type of inference in Pearl's causality ladder. Existing methods for counterfactual
                    inference with continuous outcomes aim at point identification and thus make strong and unnatural
                    assumptions about the underlying structural causal model. In this paper, we relax these assumptions
                    and aim at partial counterfactual identification of continuous outcomes, i.e., when the
                    counterfactual query resides in an ignorance interval with informative bounds. We prove that, in
                    general, the ignorance interval of the counterfactual queries has non-informative bounds, already
                    when functions of structural causal models are continuously differentiable. As a remedy, we propose
                    a novel sensitivity model called Curvature Sensitivity Model. This allows us to obtain informative
                    bounds by bounding the curvature of level sets of the functions. We further show that existing point
                    counterfactual identification methods are special cases of our Curvature Sensitivity Model when the
                    bound of the curvature is set to zero. We then propose an implementation of our Curvature
                    Sensitivity Model in the form of a novel deep generative model, which we call Augmented
                    Pseudo-Invertible Decoder. Our implementation employs (i) residual normalizing flows with (ii)
                    variational augmentations. We empirically demonstrate the effectiveness …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70734">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-29"></span>

        <script>
        add_bookmark_click(
            70734,
             1,
            'bookmark-number-29',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70734">Provable benefits of annealing for estimating
                normalizing constants: Importance Sampling, Noise-Contrastive Estimation, and beyond</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Omar Chehab · Aapo Hyvarinen · Andrej Risteski</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70734">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70734-thumb.png?t=1702229402.4785864" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70734" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70734" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70734">
                    Abstract <i id="caret-70734" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70734">
            <div class="abstract-display">
                <p>Recent research has developed several Monte Carlo methods for estimating the normalization constant
                    (partition function) based on the idea of annealing. This means sampling successively from a path of
                    distributions which interpolate between a tractable "proposal" distribution and the unnormalized
                    "target" distribution. Prominent estimators in this family include annealed importance sampling and
                    annealed noise-contrastive estimation (NCE). Such methods hinge on a number of design choices: which
                    estimator to use, which path of distributions to use and whether to use a path at all; so far, there
                    is no definitive theory on which choices are efficient. Here, we evaluate each design choice by the
                    asymptotic estimation error it produces. First, we show that using NCE is more efficient than the
                    importance sampling estimator, but in the limit of infinitesimal path steps, the difference
                    vanishes. Second, we find that using the geometric path brings down the estimation error from an
                    exponential to a polynomial function of the parameter distance between the target and proposal
                    distributions. Third, we find that the arithmetic path, while rarely used, can offer optimality
                    properties over the universally-used geometric path. In fact, in a particular limit, the optimal
                    path is arithmetic. Based on this theory, we finally propose …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71039">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-30"></span>

        <script>
        add_bookmark_click(
            71039,
             1,
            'bookmark-number-30',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71039">Supervised Pretraining Can Learn In-Context
                Reinforcement Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jonathan Lee · Annie Xie · Aldo Pacchiano · Yash Chandak · Chelsea Finn · Ofir Nachum ·
            Emma Brunskill
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71039">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71039" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71039" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71039">
                    Abstract <i id="caret-71039" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71039">
            <div class="abstract-display">
                <p>Large transformer models trained on diverse datasets have shown a remarkable ability to learn
                    in-context, achieving high few-shot performance on tasks they were not explicitly trained to solve.
                    In this paper, we study the in-context learning capabilities of transformers in decision-making
                    problems, i.e., reinforcement learning (RL) for bandits and Markov decision processes. To do so, we
                    introduce and study the Decision-Pretrained Transformer (DPT), a supervised pretraining method where
                    a transformer predicts an optimal action given a query state and an in-context dataset of
                    interactions from a diverse set of tasks. While simple, this procedure produces a model with several
                    surprising capabilities. We find that the trained transformer can solve a range of RL problems
                    in-context, exhibiting both exploration online and conservatism offline, despite not being
                    explicitly trained to do so. The model also generalizes beyond the pretraining distribution to new
                    tasks and automatically adapts its decision-making strategies to unknown structure. Theoretically,
                    we show DPT can be viewed as an efficient implementation of Bayesian posterior sampling, a provably
                    sample-efficient RL algorithm. We further leverage this connection to provide guarantees on the
                    regret of the in-context algorithm yielded by DPT, and prove that it can learn faster than
                    algorithms used to generate …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71134">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-31"></span>

        <script>
        add_bookmark_click(
            71134,
             1,
            'bookmark-number-31',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71134">Kiki or Bouba? Sound Symbolism in
                Vision-and-Language Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Morris Alper · Hadar Averbuch-Elor</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71134">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71134-thumb.png?t=1701915870.6628902" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71134" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71134" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71134">
                    Abstract <i id="caret-71134" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71134">
            <div class="abstract-display">
                <p>Although the mapping between sound and meaning in human language is assumed to be largely arbitrary,
                    research in cognitive science has shown that there are non-trivial correlations between particular
                    sounds and meanings across languages and demographic groups, a phenomenon known as sound symbolism.
                    Among the many dimensions of meaning, sound symbolism is particularly salient and well-demonstrated
                    with regards to cross-modal associations between language and the visual domain. In this work, we
                    address the question of whether sound symbolism is reflected in vision-and-language models such as
                    CLIP and Stable Diffusion. Using zero-shot knowledge probing to investigate the inherent knowledge
                    of these models, we find strong evidence that they do show this pattern, paralleling the well-known
                    kiki-bouba effect in psycholinguistics. Our work provides a novel method for demonstrating sound
                    symbolism and understanding its nature using computational tools. Our code will be made publicly
                    available.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70694">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-32"></span>

        <script>
        add_bookmark_click(
            70694,
             1,
            'bookmark-number-32',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70694">Is Learning in Games Good for the Learners?</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">William Brown · Jon Schneider · Kiran Vodrahalli</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70694">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70694-thumb.png?t=1699982397.719647" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70694" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70694" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70694">
                    Abstract <i id="caret-70694" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70694">
            <div class="abstract-display">
                <p>We consider a number of questions related to tradeoffs between reward and regret in repeated gameplay
                    between two agents. To facilitate this, we introduce a notion of generalized equilibrium which
                    allows for asymmetric regret constraints, and yields polytopes of feasible values for each agent and
                    pair of regret constraints, where we show that any such equilibrium is reachable by a pair of
                    algorithms which maintain their regret guarantees against arbitrary opponents. As a central example,
                    we highlight the case one agent is no-swap and the other's regret is unconstrained. We show that
                    this captures an extension of Stackelberg equilibria with a matching optimal value, and that there
                    exists a wide class of games where a player can significantly increase their utility by deviating
                    from a no-swap-regret algorithm against a no-swap learner (in fact, almost any game without pure
                    Nash equilibria is of this form). Additionally, we make use of generalized equilibria to consider
                    tradeoffs in terms of the opponent's algorithm choice. We give a tight characterization for the
                    maximal reward obtainable against some no-regret learner, yet we also show a class of games in which
                    this is bounded away from the value obtainable against the class of common "mean-based" no-regret
                    …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70103">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-33"></span>

        <script>
        add_bookmark_click(
            70103,
             1,
            'bookmark-number-33',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70103">Imitation Learning from Imperfection: Theoretical
                Justifications and Algorithms</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Ziniu Li · Tian Xu · Zeyu Qin · Yang Yu · Zhi-Quan Luo</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70103">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70103-thumb.png?t=1697079884.7467904" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70103" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70103" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70103">
                    Abstract <i id="caret-70103" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70103">
            <div class="abstract-display">
                <p>Imitation learning (IL) algorithms excel in acquiring high-quality policies from expert data for
                    sequential decision-making tasks. But, their effectiveness is hampered when faced with limited
                    expert data. To tackle this challenge, a novel framework called (offline) IL with supplementary data
                    has emerged, which enhances learning by incorporating an additional yet imperfect dataset obtained
                    inexpensively from sub-optimal policies. Nonetheless, learning becomes challenging due to the
                    potential inclusion of out-of-expert-distribution samples. In this work, we pioneer the mathematical
                    formalization of this framework, uncovering its limitations. Our theoretical analysis reveals that a
                    naive approach—applying the behavioral cloning (BC) algorithm concept to the combined set of expert
                    and supplementary data—may fall short of vanilla BC, which solely relies on expert data. This
                    deficiency arises due to the distribution shift between the two data sources. To address this issue,
                    we propose a new importance-sampling-based technique for selecting data within the expert
                    distribution. We prove that the proposed method theoretically eliminates the gap of the naive
                    approach, highlighting its efficacy when handling imperfect data. Empirical studies demonstrate that
                    our method outperforms previous state-of-the-art methods in tasks including robotics locomotion
                    control, Atari video games, and image classification. Overall, our work underscores the potential of
                    improving IL by …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72247">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-34"></span>

        <script>
        add_bookmark_click(
            72247,
             1,
            'bookmark-number-34',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72247">Faith and Fate: Limits of Transformers on
                Compositionality</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Nouha Dziri · Ximing Lu · Melanie Sclar · Xiang (Lorraine) Li · Liwei Jiang · Bill
            Yuchen Lin · Sean Welleck · Peter West · Chandra Bhagavatula · Ronan Le Bras · Jena Hwang · Soumya Sanyal ·
            Xiang Ren · Allyson Ettinger · Zaid Harchaoui · Yejin Choi
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72247">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72247-thumb.png?t=1701836650.439229" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72247" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72247" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72247">
                    Abstract <i id="caret-72247" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72247">
            <div class="abstract-display">
                <p>Transformer large language models (LLMs) have sparked admiration for their exceptional performance on
                    tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on
                    surprisingly trivial problems. This begs the question: Are these errors incidental, or do they
                    signal more substantial limitations?In an attempt to demystify transformer LLMs, we investigate the
                    limits of these models across three representative compositional tasks---multi-digit multiplication,
                    logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems
                    down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional
                    tasks as computation graphs to systematically quantify the level of complexity, and break down
                    reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer
                    LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized
                    subgraph matching, without necessarily developing systematic problem-solving skills. To round off
                    our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that
                    highlight how autoregressive generations' performance can rapidly decay with increased task
                    complexity.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70211">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-35"></span>

        <script>
        add_bookmark_click(
            70211,
             1,
            'bookmark-number-35',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70211">Pre-Training Protein Encoder via Siamese
                Sequence-Structure Diffusion Trajectory Prediction</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zuobai Zhang · Minghao Xu · Aurelie Lozano · Vijil Chenthamarakshan · Payel Das · Jian
            Tang
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70211">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70211-thumb.png?t=1698700287.9187245" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70211" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70211" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70211">
                    Abstract <i id="caret-70211" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70211">
            <div class="abstract-display">
                <p>Self-supervised pre-training methods on proteins have recently gained attention, with most approaches
                    focusing on either protein sequences or structures, neglecting the exploration of their joint
                    distribution, which is crucial for a comprehensive understanding of protein functions by integrating
                    co-evolutionary information and structural characteristics. In this work, inspired by the success of
                    denoising diffusion models in generative tasks, we propose the DiffPreT approach to pre-train a
                    protein encoder by sequence-structure joint diffusion modeling. DiffPreT guides the encoder to
                    recover the native protein sequences and structures from the perturbed ones along the joint
                    diffusion trajectory, which acquires the joint distribution of sequences and structures. Considering
                    the essential protein conformational variations, we enhance DiffPreT by a method called Siamese
                    Diffusion Trajectory Prediction (SiamDiff) to capture the correlation between different conformers
                    of a protein. SiamDiff attains this goal by maximizing the mutual information between
                    representations of diffusion trajectories of structurally-correlated conformers. We study the
                    effectiveness of DiffPreT and SiamDiff on both atom- and residue-level structure-based protein
                    understanding tasks. Experimental results show that the performance of DiffPreT is consistently
                    competitive on all tasks, and SiamDiff achieves new state-of-the-art performance, considering the
                    mean ranks on all tasks. Code will be released upon acceptance.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73048">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-36"></span>

        <script>
        add_bookmark_click(
            73048,
             1,
            'bookmark-number-36',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73048">Model Sparsity Can Simplify Machine Unlearning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">jinghan jia · Jiancheng Liu · Parikshit Ram · Yuguang Yao · Gaowen Liu · Yang Liu ·
            PRANAY SHARMA · Sijia Liu
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73048">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/73048-thumb.png?t=1701922039.6356535" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73048" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73048" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73048">
                    Abstract <i id="caret-73048" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73048">
            <div class="abstract-display">
                <p>In response to recent data regulation requirements, machine unlearning (MU) has emerged as a critical
                    process to remove the influence of specific examples from a given model. Although exact unlearning
                    can be achieved through complete model retraining using the remaining dataset, the associated
                    computational costs have driven the development of efficient, approximate unlearning techniques.
                    Moving beyond data-centric MU approaches, our study introduces a novel model-based perspective:
                    model sparsification via weight pruning, which is capable of reducing the gap between exact
                    unlearning and approximate unlearning. We show in both theory and practice that model sparsity can
                    boost the multi-criteria unlearning performance of an approximate unlearner, closing the
                    approximation gap, while continuing to be efficient. This leads to a new MU paradigm, termed prune
                    first, then unlearn, which infuses a sparse prior to the unlearning process. Building on this
                    insight, we also develop a sparsity-aware unlearning method that utilizes sparsity regularization to
                    enhance the training process of approximate unlearning. Extensive experiments show that our
                    proposals consistently benefit MU in various unlearning scenarios. A notable highlight is the 77%
                    unlearning efficacy gain of fine-tuning (one of the simplest approximate unlearning methods) when
                    using our proposed sparsity-aware unlearning method. Furthermore, we showcase the practical …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71022">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-37"></span>

        <script>
        add_bookmark_click(
            71022,
             1,
            'bookmark-number-37',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71022">Slow and Weak Attractor Computation Embedded in
                Fast and Strong E-I Balanced Neural Dynamics</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Xiaohan Lin · Liyuan Li · Boxin Shi · Tiejun Huang · Yuanyuan Mi · Si Wu</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71022">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71022-thumb.png?t=1701849362.8652437" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71022" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71022" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71022">
                    Abstract <i id="caret-71022" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71022">
            <div class="abstract-display">
                <p>Attractor networks require neuronal connections to be highly structured in order to maintain
                    attractor states that represent information, while excitation and inhibition balanced networks
                    (E-INNs) require neuronal connections to be random and sparse to generate irregular neuronal
                    firings. Despite being regarded as canonical models of neural circuits, both types of networks are
                    usually studied in isolation, and it remains unclear how they coexist in the brain, given their very
                    different structural demands. In this study, we investigate the compatibility of continuous
                    attractor neural networks (CANNs) and E-INNs. In line with recent experimental data, we find that a
                    neural circuit can exhibit both the traits of CANNs and E-INNs if the neuronal synapses consist of
                    two sets: one set is strong and fast for irregular firing, and the other set is weak and slow for
                    attractor dynamics. Our results from simulations and theoretical analysis reveal that the network
                    also exhibits enhanced performance compared to the case of using only one set of synapses, with
                    accelerated convergence of attractor states and retained E-I balanced condition for localized input.
                    We also apply the network model to solve a real-world tracking problem and demonstrate that it can
                    track fast-moving objects well. We hope that …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71996">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-38"></span>

        <script>
        add_bookmark_click(
            71996,
             1,
            'bookmark-number-38',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71996">Demystifying Oversmoothing in Attention-Based Graph
                Neural Networks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Xinyi Wu · Amir Ajorlou · Zihui Wu · Ali Jadbabaie</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71996">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71996-thumb.png?t=1700691160.5333278" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71996" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71996" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71996">
                    Abstract <i id="caret-71996" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71996">
            <div class="abstract-display">
                <p>Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth
                    leads to homogeneous node representations. While previous work has established that Graph
                    Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether
                    the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive
                    answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as
                    nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of
                    products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to
                    popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive
                    power exponentially. The proposed framework extends the existing results on oversmoothing for
                    symmetric GCNs to a significantly broader class of GNN models, including random walk GCNs, Graph
                    Attention Networks (GATs) and (graph) transformers. In particular, our analysis accounts for
                    asymmetric, state-dependent and time-varying aggregation operators and a wide range of common
                    nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71697">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-39"></span>

        <script>
        add_bookmark_click(
            71697,
             1,
            'bookmark-number-39',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71697">ProPILE: Probing Privacy Leakage in Large Language
                Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Siwon Kim · Sangdoo Yun · Hwaran Lee · Martin Gubri · Sungroh Yoon · Seong Joon Oh</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71697">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71697-thumb.png?t=1701754095.7610903" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71697" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71697" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71697">
                    Abstract <i id="caret-71697" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71697">
            <div class="abstract-display">
                <p>The rapid advancement and widespread use of large language models (LLMs) have raised significant
                    concerns regarding the potential leakage of personally identifiable information (PII). These models
                    are often trained on vast quantities of web-collected data, which may inadvertently include
                    sensitive personal data. This paper presents ProPILE, a novel probing tool designed to empower data
                    subjects, or the owners of the PII, with awareness of potential PII leakage in LLM-based services.
                    ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy
                    intrusion in LLMs. We demonstrate its application on the OPT-1.3B model trained on the publicly
                    available Pile dataset. We show how hypothetical data subjects may assess the likelihood of their
                    PII being included in the Pile dataset being revealed. ProPILE can also be leveraged by LLM service
                    providers to effectively evaluate their own levels of PII leakage with more powerful prompts
                    specifically tuned for their in-house models. This tool represents a pioneering step towards
                    empowering the data subjects for their awareness and control over their own data on the web.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72501">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-40"></span>

        <script>
        add_bookmark_click(
            72501,
             1,
            'bookmark-number-40',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72501">Subspace Identification for Multi-Source Domain
                Adaptation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zijian Li · Ruichu Cai · Guangyi Chen · Boyang Sun · Zhifeng Hao · Kun Zhang</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72501">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72501" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72501" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72501">
                    Abstract <i id="caret-72501" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72501">
            <div class="abstract-display">
                <p>Multi-source domain adaptation (MSDA) methods aim to transfer knowledge from multiple labeled source
                    domains to an unlabeled target domain. Although current methods achieve target joint distribution
                    identifiability by enforcing minimal changes across domains, they often necessitate stringent
                    conditions, such as an adequate number of domains, monotonic transformation of latent variables, and
                    invariant label distributions. These requirements are challenging to satisfy in real-world
                    applications. To mitigate the need for these strict assumptions, we propose a subspace
                    identification theory that guarantees the disentanglement of domain-invariant and domain-specific
                    variables under less restrictive constraints regarding domain numbers and transformation properties
                    and thereby facilitating domain adaptation by minimizing the impact of domain shifts on invariant
                    variables. Based on this theory, we develop a Subspace Identification Guarantee (SIG) model that
                    leverages variational inference. Furthermore, the SIG model incorporates class-aware conditional
                    alignment to accommodate target shifts where label distributions change with the domain.
                    Experimental results demonstrate that our SIG model outperforms existing MSDA techniques on various
                    benchmark datasets, highlighting its effectiveness in real-world applications.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72949">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-41"></span>

        <script>
        add_bookmark_click(
            72949,
             1,
            'bookmark-number-41',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72949">CLIP-OGD: An Experimental Design for Adaptive
                Neyman Allocation in Sequential Experiments</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jessica Dai · Paula Gradu · Christopher Harshaw</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72949">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72949" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72949" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72949">
                    Abstract <i id="caret-72949" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72949">
            <div class="abstract-display">
                From clinical development of cancer therapies to investigations into partisan bias, adaptive sequential
                designs have become increasingly popular method for causal inference, as they offer the possibility of
                improved precision over their non-adaptive counterparts. However, even in simple settings (e.g. two
                treatments) the extent to which adaptive designs can improve precision is not sufficiently well
                understood. In this work, we study the problem of Adaptive Neyman Allocation in a design-based potential
                outcomes framework, where the experimenter seeks to construct an adaptive design which is nearly as
                efficient as the optimal (but infeasible) non-adaptive Neyman design, which has access to all potential
                outcomes. Motivated by connections to online optimization, we propose Neyman Ratio and Neyman Regret as
                two (equivalent) performance measures of adaptive designs for this problem. We present Clip-OGD, an
                adaptive design which achieves $\widetilde{\mathcal{O}}(\sqrt{T})$ expected Neyman regret and thereby
                recovers the optimal Neyman variance in large samples. Finally, we construct a conservative variance
                estimator which facilitates the development of asymptotically valid confidence intervals. To complement
                our theoretical results, we conduct simulations using data from a microeconomic experiment.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72994">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-42"></span>

        <script>
        add_bookmark_click(
            72994,
             1,
            'bookmark-number-42',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72994">Alternating Updates for Efficient Transformers</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Cenk Baykal · Dylan Cutler · Nishanth Dikkala · Nikhil Ghosh · Rina Panigrahy · Xin
            Wang
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72994">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72994-thumb.png?t=1699898310.031774" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72994" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72994" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72994">
                    Abstract <i id="caret-72994" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72994">
            <div class="abstract-display">
                It has been well established that increasing scale in deep transformer networks leads to improved
                quality and performance. However, this increase in scale often comes with prohibitive increases in
                compute cost and inference latency. We introduce Alternating Updates (AltUp), a simple-to-implement
                method to increase a model's capacity without the computational burden. AltUp enables the widening of
                the learned representation, i.e., the token embedding, while only incurring a negligible increase in
                latency. AltUp achieves this by working on a subblock of the widened representation at each layer and
                using a predict-and-correct mechanism to update the inactivated blocks. We present extensions of AltUp,
                such as its applicability to the sequence dimension, and demonstrate how AltUp can be synergistically
                combined with existing approaches, such as Sparse Mixture-of-Experts models, to obtain efficient models
                with even higher capacity. Our experiments on benchmark transformer models and language tasks
                demonstrate the consistent effectiveness of AltUp on a diverse set of scenarios. Notably, on SuperGLUE
                and SQuAD benchmarks, AltUp enables up to $87\%$ speedup relative to the dense baselines at the same
                accuracy.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71342">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-43"></span>

        <script>
        add_bookmark_click(
            71342,
             1,
            'bookmark-number-43',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71342">On the Minimax Regret for Online Learning with
                Feedback Graphs</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Khaled Eldowa · Emmanuel Esposito · Tom Cesari · Nicolò Cesa-Bianchi</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71342">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71342" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71342" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71342">
                    Abstract <i id="caret-71342" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71342">
            <div class="abstract-display">
                In this work, we improve on the upper and lower bounds for the regret of online learning with strongly
                observable undirected feedback graphs. The best known upper bound for this problem is
                $\mathcal{O}\bigl(\sqrt{\alpha T\ln K}\bigr)$, where $K$ is the number of actions, $\alpha$ is the
                independence number of the graph, and $T$ is the time horizon. The $\sqrt{\ln K}$ factor is known to be
                necessary when $\alpha = 1$ (the experts case). On the other hand, when $\alpha = K$ (the bandits case),
                the minimax rate is known to be $\Theta\bigl(\sqrt{KT}\bigr)$, and a lower bound
                $\Omega\bigl(\sqrt{\alpha T}\bigr)$ is known to hold for any $\alpha$. Our improved upper bound
                $\mathcal{O}\bigl(\sqrt{\alpha T(1+\ln(K/\alpha))}\bigr)$ holds for any $\alpha$ and matches the lower
                bounds for bandits and experts, while interpolating intermediate cases. To prove this result, we use
                FTRL with $q$-Tsallis entropy for a carefully chosen value of $q \in [1/2, 1)$ that varies with
                $\alpha$. The analysis of this algorithm requires a new bound on the variance term in the regret. We
                also show how to extend our techniques to time-varying graphs, without requiring prior knowledge of
                their independence numbers. Our upper bound is complemented by an improved $\Omega\bigl(\sqrt{\alpha
                T(\ln K)/(\ln\alpha)}\bigr)$ lower bound for …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71933">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-44"></span>

        <script>
        add_bookmark_click(
            71933,
             1,
            'bookmark-number-44',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71933">Sharp Spectral Rates for Koopman Operator
                Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Vladimir Kostic · Karim Lounici · Pietro Novelli · Massimiliano Pontil</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71933">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71933" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71933" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71933">
                    Abstract <i id="caret-71933" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71933">
            <div class="abstract-display">
                <p>Non-linear dynamical systems can be handily described by the associated Koopman operator, whose
                    action evolves every observable of the system forward in time. Learning the Koopman operator and its
                    spectral decomposition from data is enabled by a number of algorithms. In this work we present for
                    the first time non-asymptotic learning bounds for the Koopman eigenvalues and eigenfunctions. We
                    focus on time-reversal-invariant stochastic dynamical systems, including the important example of
                    Langevin dynamics. We analyze two popular estimators: Extended Dynamic Mode Decomposition (EDMD) and
                    Reduced Rank Regression (RRR). Our results critically hinge on novel {minimax} estimation bounds for
                    the operator norm error, that may be of independent interest. Our spectral learning bounds are
                    driven by the simultaneous control of the operator norm error and a novel metric distortion
                    functional of the estimated eigenfunctions. The bounds indicates that both EDMD and RRR have similar
                    variance, but EDMD suffers from a larger bias which might be detrimental to its learning rate. Our
                    results shed new light on the emergence of spurious eigenvalues, an issue which is well known
                    empirically. Numerical experiments illustrate the implications of the bounds in practice.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72028">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-45"></span>

        <script>
        add_bookmark_click(
            72028,
             1,
            'bookmark-number-45',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72028">Regularization properties of adversarially-trained
                linear regression</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Antonio Ribeiro · Dave Zachariah · Francis Bach · Thomas Schön</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72028">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72028-thumb.png?t=1699555424.1707234" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72028" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72028" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72028">
                    Abstract <i id="caret-72028" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72028">
            <div class="abstract-display">
                State-of-the-art machine learning models can be vulnerable to very small input perturbations that are
                adversarially constructed. Adversarial training is an effective approach to defend against it.
                Formulated as a min-max problem, it searches for the best solution when the training data were corrupted
                by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be
                observed and are the focus of our study. In this case, adversarial training leads to a convex
                optimization problem which can be formulated as the minimization of a finite sum. We provide a
                comparative analysis between the solution of adversarial training in linear regression and other
                regularization methods. Our main findings are that: (A) Adversarial training yields the minimum-norm
                interpolating solution in the overparameterized regime (more parameters than data), as long as the
                maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator
                is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent
                to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized
                region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed
                covariates. (C) For $\ell_\infty$-adversarial training---as in square-root Lasso---the choice of
                adversarial radius for optimal bounds …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70755">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-46"></span>

        <script>
        add_bookmark_click(
            70755,
             1,
            'bookmark-number-46',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70755">GloptiNets: Scalable Non-Convex Optimization with
                Certificates</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Gaspard Beugnot · Julien Mairal · Alessandro Rudi</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70755">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70755-thumb.png?t=1698680302.7800863" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70755" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70755" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70755">
                    Abstract <i id="caret-70755" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70755">
            <div class="abstract-display">
                <p>We present a novel approach to non-convex optimization with certificates, which handles smooth
                    functions on the hypercube or on the torus. Unlike traditional methods that rely on algebraic
                    properties, our algorithm exploits the regularity of the target function intrinsic in the decay of
                    its Fourier spectrum. By defining a tractable family of models, we allow {\em at the same time} to
                    obtain precise certificates and to leverage the advanced and powerful computational techniques
                    developed to optimize neural networks. In this way the scalability of our approach is naturally
                    enhanced by parallel computing with GPUs. Our approach, when applied to the case of polynomials of
                    moderate dimensions but with thousands of coefficients, outperforms the state-of-the-art
                    optimization methods with certificates, as the ones based on Lasserre's hierarchy, addressing
                    problems intractable for the competitors.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70131">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-47"></span>

        <script>
        add_bookmark_click(
            70131,
             1,
            'bookmark-number-47',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70131">Can semi-supervised learning use all the data
                effectively? A lower bound perspective</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Alexandru Tifrea · Gizem Yüce · Amartya Sanyal · Fanny Yang</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70131">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70131" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70131" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70131">
                    Abstract <i id="caret-70131" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70131">
            <div class="abstract-display">
                <p>Prior theoretical and empirical works have established that semi-supervised learning algorithms can
                    leverage the unlabeled data to improve over the labeled sample complexity of supervised learning
                    (SL) algorithms. However, existing theoretical work focuses on regimes where the unlabeled data is
                    sufficient to learn a good decision boundary using unsupervised learning (UL) alone. This begs the
                    question: Can SSL algorithms simultaneously improve upon both UL and SL? To this end, we derive a
                    tight lower bound for 2-Gaussian mixture models that explicitly depends on the labeled and the
                    unlabeled dataset size as well as the signal-to-noise ratio of the mixture distribution.
                    Surprisingly, our result implies that no SSL algorithm improves upon the minimax-optimal statistical
                    error rates of SL or UL algorithms for these distributions. Nevertheless, in our real-world
                    experiments, SSL algorithms can often outperform UL and SL algorithms. In summary, our work suggests
                    that while it is possible to prove the performance gains of SSL algorithms, this would require
                    careful tracking of constants in the theoretical analysis.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72256">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-48"></span>

        <script>
        add_bookmark_click(
            72256,
             1,
            'bookmark-number-48',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72256">Towards In-context Scene Understanding</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Ivana Balazevic · David Steiner · Nikhil Parthasarathy · Relja Arandjelović · Olivier
            Henaff
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72256">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72256-thumb.png?t=1701706872.2776513" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72256" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72256" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72256">
                    Abstract <i id="caret-72256" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72256">
            <div class="abstract-display">
                <p>In-context learning––the ability to configure a model's behavior with different prompts––has
                    revolutionized the field of natural language processing, alleviating the need for task-specific
                    models and paving the way for generalist models capable of assisting with any query. Computer
                    vision, in contrast, has largely stayed in the former regime: specialized decoders and finetuning
                    protocols are generally required to perform dense tasks such as semantic segmentation and depth
                    estimation. In this work we explore a simple mechanism for in-context learning of such scene
                    understanding tasks: nearest neighbor retrieval from a prompt of annotated features. We propose a
                    new pretraining protocol––leveraging attention within and across images––which yields
                    representations particularly useful in this regime. The resulting Hummingbird model, suitably
                    prompted, performs various scene understanding tasks without modification while approaching the
                    performance of specialists that have been finetuned for each task. Moreover, Hummingbird can be
                    configured to perform new tasks much more efficiently than finetuned models, raising the possibility
                    of scene understanding in the interactive assistant regime.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70908">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-49"></span>

        <script>
        add_bookmark_click(
            70908,
             1,
            'bookmark-number-49',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70908">Provable Guarantees for Nonlinear Feature Learning
                in Three-Layer Neural Networks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Eshaan Nichani · Alex Damian · Jason Lee</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70908">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70908-thumb.png?t=1701831722.72865" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70908" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70908" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70908">
                    Abstract <i id="caret-70908" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70908">
            <div class="abstract-display">
                <p>One of the central questions in the theory of deep learning is to understand how neural networks
                    learn hierarchical features. The ability of deep networks to extract salient features is crucial to
                    both their outstanding generalization ability and the modern deep learning paradigm of pretraining
                    and finetuneing. However, this feature learning process remains poorly understood from a theoretical
                    perspective, with existing analyses largely restricted to two-layer networks. In this work we show
                    that three-layer neural networks have provably richer feature learning capabilities than two-layer
                    networks. We analyze the features learned by a three-layer network trained with layer-wise gradient
                    descent, and present a general purpose theorem which upper bounds the sample complexity and width
                    needed to achieve low test error when the target has specific hierarchical structure. We instantiate
                    our framework in specific statistical learning settings -- single-index models and functions of
                    quadratic features -- and show that in the latter setting three-layer networks obtain a sample
                    complexity improvement over all existing guarantees for two-layer networks. Crucially, this sample
                    complexity improvement relies on the ability of three-layer networks to efficiently learn <em>nonlinear</em>
                    features. We then establish a concrete optimization-based depth separation by constructing a
                    function which is efficiently learnable via gradient …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72860">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-50"></span>

        <script>
        add_bookmark_click(
            72860,
             1,
            'bookmark-number-50',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72860">Generalization in the Face of Adaptivity: A
                Bayesian Perspective</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Moshe Shenfeld · Katrina Ligett</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72860">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72860-thumb.png?t=1701604378.9974945" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72860" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72860" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72860">
                    Abstract <i id="caret-72860" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72860">
            <div class="abstract-display">
                <p>Repeated use of a data sample via adaptively chosen queries can rapidly lead to overfitting, wherein
                    the empirical evaluation of queries on the sample significantly deviates from their mean with
                    respect to the underlying data distribution. It turns out that simple noise addition algorithms
                    suffice to prevent this issue, and differential privacy-based analysis of these algorithms shows
                    that they can handle an asymptotically optimal number of queries. However, differential privacy's
                    worst-case nature entails scaling such noise to the range of the queries even for
                    highly-concentrated queries, or introducing more complex algorithms.In this paper, we prove that
                    straightforward noise-addition algorithms already provide variance-dependent guarantees that also
                    extend to unbounded queries. This improvement stems from a novel characterization that illuminates
                    the core problem of adaptive data analysis. We show that the harm of adaptivity results from the
                    covariance between the new query and a Bayes factor-based measure of how much information about the
                    data sample was encoded in the responses given to past queries. We then leverage this
                    characterization to introduce a new data-dependent stability notion that can bound this
                    covariance.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72470">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-51"></span>

        <script>
        add_bookmark_click(
            72470,
             1,
            'bookmark-number-51',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72470">Participatory Personalization in Classification</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Hailey Joren · Chirag Nagpal · Katherine Heller · Berk Ustun</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72470">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72470" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72470" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72470">
                    Abstract <i id="caret-72470" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72470">
            <div class="abstract-display">
                <p>Machine learning models are often personalized based on information that is protected, sensitive,
                    self-reported, or costly to acquire. These models use information about people, but do not
                    facilitate nor inform their <em>consent</em>. Individuals cannot opt out of reporting information
                    that a model needs to personalize their predictions nor tell if they benefit from personalization in
                    the first place. We introduce a new family of prediction models, called participatory systems, that
                    let individuals opt into personalization at prediction time. We present a model-agnostic algorithm
                    to learn participatory systems for supervised learning tasks where models are personalized with
                    categorical group attributes. We conduct a comprehensive empirical study of participatory systems in
                    clinical prediction tasks, comparing them to common approaches for personalization and imputation.
                    Our results show that participatory systems can facilitate and inform consent in a way that improves
                    performance and privacy across all groups who report personal data.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72911">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-52"></span>

        <script>
        add_bookmark_click(
            72911,
             1,
            'bookmark-number-52',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72911">GLIME: General, Stable and Local LIME
                Explanation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zeren Tan · Yang Tian · Jian Li</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72911">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72911-thumb.png?t=1702178536.0742264" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72911" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72911" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72911">
                    Abstract <i id="caret-72911" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72911">
            <div class="abstract-display">
                <p>As black-box machine learning models become more complex and are applied in high-stakes settings, the
                    need for providing explanations for their predictions becomes crucial. Although Local Interpretable
                    Model-agnostic Explanations (LIME) \cite{ribeiro2016should} is a widely adopted method for
                    understanding model behavior, it suffers from instability with respect to random seeds
                    \cite{zafar2019dlime, shankaranarayana2019alime, bansal2020sam} and exhibits low local fidelity
                    (i.e., how the explanation explains model's local behaviors) \cite{rahnama2019study,
                    laugel2018defining}. Our study demonstrates that this instability is caused by small sample weights,
                    resulting in the dominance of regularization and slow convergence. Additionally, LIME's sampling
                    approach is non-local and biased towards the reference, leading to diminished local fidelity and
                    instability to references. To address these challenges, we propose \textsc{Glime}, an enhanced
                    framework that extends LIME and unifies several previous methods. Within the \textsc{Glime}
                    framework, we derive an equivalent formulation of LIME that achieves significantly faster
                    convergence and improved stability. By employing a local and unbiased sampling distribution,
                    \textsc{Glime} generates explanations with higher local fidelity compared to LIME, while being
                    independent of the reference choice. Moreover, \textsc{Glime} offers users the flexibility to choose
                    sampling distribution based on their specific scenarios.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70375">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-53"></span>

        <script>
        add_bookmark_click(
            70375,
             1,
            'bookmark-number-53',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70375">Learning from Active Human Involvement through
                Proxy Value Propagation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zhenghao Peng · Wenjie Mo · Chenda Duan · Quanyi Li · Bolei Zhou</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70375">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70375-thumb.png?t=1701737880.9824455" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70375" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70375" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70375">
                    Abstract <i id="caret-70375" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70375">
            <div class="abstract-display">
                <p>Learning from active human involvement enables the human subject to actively intervene and
                    demonstrate to the AI agent during training. The interaction and corrective feedback from human
                    brings safety and AI alignment to the learning process. In this work, we propose a new reward-free
                    active human involvement method called Proxy Value Propagation for policy optimization. Our key
                    insight is that a proxy value function can be designed to express human intents, wherein state-
                    action pairs in the human demonstration are labeled with high values, while those agents’ actions
                    that are intervened receive low values. Through the TD-learning framework, labeled values of
                    demonstrated state-action pairs are further propagated to other unlabeled data generated from
                    agents’ exploration. The proxy value function thus induces a policy that faithfully emulates human
                    behaviors. Human- in-the-loop experiments show the generality and efficiency of our method. With
                    minimal modification to existing reinforcement learning algorithms, our method can learn to solve
                    continuous and discrete control tasks with various human control devices, including the challenging
                    task of driving in Grand Theft Auto V. Demo video and code are available at:
                    https://metadriverse.github.io/pvp.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73722">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-54"></span>

        <script>
        add_bookmark_click(
            73722,
             1,
            'bookmark-number-54',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73722">BubbleML: A Multiphase Multiphysics Dataset and
                Benchmarks for Machine Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Poster</div>
        <div class="author-str">Sheikh Md Shakeel Hassan · Arthur Feeney · Akash Dhruv · Jihoon Kim · Youngjoon Suh ·
            Jaiyoung Ryu · Yoonjin Won · Aparna Chandramowlishwaran
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73722">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/73722-thumb.png?t=1701818679.758732" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73722" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73722" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73722">
                    Abstract <i id="caret-73722" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73722">
            <div class="abstract-display">
                <p>In the field of phase change phenomena, the lack of accessible and diverse datasets suitable for
                    machine learning (ML) training poses a significant challenge. Existing experimental datasets are
                    often restricted, with limited availability and sparse ground truth, impeding our understanding of
                    this complex multiphysics phenomena. To bridge this gap, we present the BubbleML dataset which
                    leverages physics-driven simulations to provide accurate ground truth information for various
                    boiling scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled boiling. This
                    extensive dataset covers a wide range of parameters, including varying gravity conditions, flow
                    rates, sub-cooling levels, and wall superheat, comprising 79 simulations. BubbleML is validated
                    against experimental observations and trends, establishing it as an invaluable resource for ML
                    research. Furthermore, we showcase its potential to facilitate the exploration of diverse downstream
                    tasks by introducing two benchmarks: (a) optical flow analysis to capture bubble dynamics, and (b)
                    neural PDE solvers for learning temperature and flow dynamics. The BubbleML dataset and its
                    benchmarks aim to catalyze progress in ML-driven research on multiphysics phase change phenomena,
                    providing robust baselines for the development and comparison of state-of-the-art techniques and
                    models.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71752">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-55"></span>

        <script>
        add_bookmark_click(
            71752,
             1,
            'bookmark-number-55',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71752">Behavior Alignment via Reward Function
                Optimization</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Dhawal Gupta · Yash Chandak · Scott Jordan · Philip Thomas · Bruno C. da Silva</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71752">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71752" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71752" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71752">
                    Abstract <i id="caret-71752" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71752">
            <div class="abstract-display">
                <p>Designing reward functions for efficiently guiding reinforcement learning (RL) agents toward specific
                    behaviors is a complex task.This is challenging since it requires the identification of reward
                    structures that are not sparse and that avoid inadvertently inducing undesirable behaviors. Naively
                    modifying the reward structure to offer denser and more frequent feedback can lead to unintended
                    outcomes and promote behaviors that are not aligned with the designer's intended goal. Although
                    potential-based reward shaping is often suggested as a remedy, we systematically investigate
                    settings where deploying it often significantly impairs performance. To address these issues, we
                    introduce a new framework that uses a bi-level objective to learn \emph{behavior alignment reward
                    functions}. These functions integrate auxiliary rewards reflecting a designer's heuristics and
                    domain knowledge with the environment's primary rewards. Our approach automatically determines the
                    most effective way to blend these types of feedback, thereby enhancing robustness against heuristic
                    reward misspecification. Remarkably, it can also adapt an agent's policy optimization process to
                    mitigate suboptimalities resulting from limitations and biases inherent in the underlying RL
                    algorithms. We evaluate our method's efficacy on a diverse set of tasks, from small-scale
                    experiments to high-dimensional control challenges. We investigate heuristic auxiliary rewards of
                    varying quality---some of which are beneficial …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71170">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-56"></span>

        <script>
        add_bookmark_click(
            71170,
             1,
            'bookmark-number-56',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71170">Stable Nonconvex-Nonconcave Training via Linear
                Interpolation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Thomas Pethick · Wanyun Xie · Volkan Cevher</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71170">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71170" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71170" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71170">
                    Abstract <i id="caret-71170" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71170">
            <div class="abstract-display">
                <p>This paper presents a theoretical analysis of linear interpolation as a principled method for
                    stabilizing (large-scale) neural network training. We argue that instabilities in the optimization
                    process are often caused by the nonmonotonicity of the loss landscape and show how linear
                    interpolation can help by leveraging the theory of nonexpansive operators. We construct a new
                    optimization scheme called relaxed approximate proximal point (RAPP), which is the first explicit
                    method to achieve last iterate convergence rates for the full range of cohypomonotone problems. The
                    construction extends to constrained and regularized settings. By replacing the inner optimizer in
                    RAPP we rediscover the family of Lookahead algorithms for which we establish convergence in
                    cohypomonotone problems even when the base optimizer is taken to be gradient descent ascent. The
                    range of cohypomonotone problems in which Lookahead converges is further expanded by exploiting that
                    Lookahead inherits the properties of the base optimizer. We corroborate the results with experiments
                    on generative adversarial networks which demonstrates the benefits of the linear interpolation
                    present in both RAPP and Lookahead.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71605">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-57"></span>

        <script>
        add_bookmark_click(
            71605,
             1,
            'bookmark-number-57',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71605">Plug-and-Play Stability for Intracortical
                Brain-Computer Interfaces: A One-Year Demonstration of Seamless Brain-to-Text Communication</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Chaofei Fan · Nick Hahn · Foram Kamdar · Donald Avansino · Guy Wilson · Leigh Hochberg ·
            Krishna V Shenoy · Jaimie Henderson · Francis Willett
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71605">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71605-thumb.png?t=1702155300.3790953" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71605" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71605" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71605">
                    Abstract <i id="caret-71605" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71605">
            <div class="abstract-display">
                <p>Intracortical brain-computer interfaces (iBCIs) have shown promise for restoring rapid communication
                    to people with neurological disorders such as amyotrophic lateral sclerosis (ALS). However, to
                    maintain high performance over time, iBCIs typically need frequent recalibration to combat changes
                    in the neural recordings that accrue over days. This requires iBCI users to stop using the iBCI and
                    engage in supervised data collection, making the iBCI system hard to use. In this paper, we propose
                    a method that enables self-recalibration of communication iBCIs without interrupting the user. Our
                    method leverages large language models (LMs) to automatically correct errors in iBCI outputs. The
                    self-recalibration process uses these corrected outputs ("pseudo-labels") to continually update the
                    iBCI decoder online. Over a period of more than one year (403 days), we evaluated our Continual
                    Online Recalibration with Pseudo-labels (CORP) framework with one clinical trial participant. CORP
                    achieved a stable decoding accuracy of 93.84% in an online handwriting iBCI task, significantly
                    outperforming other baseline methods. Notably, this is the longest-running iBCI stability
                    demonstration involving a human participant. Our results provide the first evidence for long-term
                    stabilization of a plug-and-play, high-performance communication iBCI, addressing a major barrier
                    for the clinical translation of iBCIs.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70142">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-58"></span>

        <script>
        add_bookmark_click(
            70142,
             1,
            'bookmark-number-58',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70142">HyenaDNA: Long-Range Genomic Sequence Modeling at
                Single Nucleotide Resolution</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Eric Nguyen · Michael Poli · Marjan Faizi · Armin Thomas · Michael Wornow · Callum
            Birch-Sykes · Stefano Massaroli · Aman Patel · Clayton Rabideau · Yoshua Bengio · Stefano Ermon ·
            Christopher Ré · Stephen Baccus
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70142">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70142-thumb.png?t=1702092300.3168235" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70142" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70142" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70142">
                    Abstract <i id="caret-70142" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70142">
            <div class="abstract-display">
                <p>Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein
                    synthesis. Similar to natural language models, researchers have proposed foundation models in
                    genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for
                    downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention,
                    previous Transformer-based genomic models have used 512 to 4k tokens as context (&lt;0.001% of the
                    human genome), significantly limiting the modeling of long-range interactions in DNA. In addition,
                    these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single
                    nucleotide resolution (i.e. DNA "characters") where subtle genetic variations can completely alter
                    protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model
                    based on implicit convolutions was shown to match attention in quality while allowing longer context
                    lengths and lower time complexity. Leveraging Hyena’s new long-range capabilities, we present
                    HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths
                    of up to 1 million tokens at the single nucleotide-level – an up to 500x increase over previous
                    dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to
                    160x faster than Transformer), uses single …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72715">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-59"></span>

        <script>
        add_bookmark_click(
            72715,
             1,
            'bookmark-number-59',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72715">AbDiffuser: full-atom generation of in-vitro
                functioning antibodies</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Karolis Martinkus · Jan Ludwiczak · WEI-CHING LIANG · Julien Lafrance-Vanasse · Isidro
            Hotzel · Arvind Rajpal · Yan Wu · Kyunghyun Cho · Richard Bonneau · Vladimir Gligorijevic · Andreas Loukas
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72715">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72715" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72715" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72715">
                    Abstract <i id="caret-72715" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72715">
            <div class="abstract-display">
                <p>We introduce AbDiffuser, an equivariant and physics-informed diffusion model for the joint generation
                    of antibody 3D structures and sequences. AbDiffuser is built on top of a new representation of
                    protein structure, relies on a novel architecture for aligned proteins, and utilizes strong
                    diffusion priors to improve the denoising process. Our approach improves protein diffusion by taking
                    advantage of domain knowledge and physics-based constraints; handles sequence-length changes; and
                    reduces memory complexity by an order of magnitude, enabling backbone and side chain generation. We
                    validate AbDiffuser in silico and in vitro. Numerical experiments showcase the ability of AbDiffuser
                    to generate antibodies that closely track the sequence and structural properties of a reference set.
                    Laboratory experiments confirm that all 16 HER2 antibodies discovered were expressed at high levels
                    and that 57.1% of the selected designs were tight binders.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70338">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-60"></span>

        <script>
        add_bookmark_click(
            70338,
             1,
            'bookmark-number-60',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70338">Aligning Synthetic Medical Images with Clinical
                Knowledge using Human Feedback</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Shenghuan Sun · Greg Goldgof · Atul Butte · Ahmed Alaa</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70338">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70338" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70338" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70338">
                    Abstract <i id="caret-70338" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70338">
            <div class="abstract-display">
                <p>Generative models capable of precisely capturing nuanced clinical features in medical images hold
                    great promise for facilitating clinical data sharing, enhancing rare disease datasets, and
                    efficiently synthesizing (annotated) medical images at scale. Despite their potential, assessing the
                    quality of synthetic medical images remains a challenge. While modern generative models can
                    synthesize visually-realistic medical images, the clinical plausibility of these images may be
                    called into question. Domain-agnostic scores, such as FID score, precision, and recall, cannot
                    incorporate clinical knowledge and are, therefore, not suitable for assessing clinical sensibility.
                    Additionally, there are numerous unpredictable ways in which generative models may fail to
                    synthesize clinically plausible images, making it challenging to anticipate potential failures and
                    design automated scores for their detection. To address these challenges, this paper introduces a
                    pathologist-in-the-loop framework for generating clinically-plausible synthetic medical images. Our
                    framework comprises three steps: (1) pretraining a conditional diffusion model to generate medical
                    images conditioned on a clinical concept, (2) expert pathologist evaluation of the generated images
                    to assess whether they satisfy clinical desiderata, and (3) training a reward model that predicts
                    human feedback on new samples, which we use to incorporate expert knowledge into the finetuning
                    objective of the diffusion model. Our results show …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73457">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-61"></span>

        <script>
        add_bookmark_click(
            73457,
             1,
            'bookmark-number-61',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73457">Exploring Why Object Recognition Performance
                Degrades Across Income Levels and Geographies with Factor Annotations</a>
        </div>
        <div class="type_display_name_virtual_card">Poster</div>
        <div class="author-str">Laura Gustafson · Megan Richards · Melissa Hall · Caner Hazirbas · Diane Bouchacourt ·
            Mark Ibrahim
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73457">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73457" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73457" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73457">
                    Abstract <i id="caret-73457" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73457">
            <div class="abstract-display">
                <p>Despite impressive advances in object-recognition, deep learning systems’ performance degrades
                    significantly across geographies and lower income levels---raising pressing concerns of inequity.
                    Addressing such performance gaps remains a challenge, as little is understood about why performance
                    degrades across incomes or geographies.We take a step in this direction by annotating images from
                    Dollar Street, a popular benchmark of geographically and economically diverse images, labeling each
                    image with factors such as color, shape, and background. These annotations unlock a new granular
                    view into how objects differ across incomes/regions. We then use these object differences to
                    pinpoint model vulnerabilities across incomes and regions.We study a range of modern vision models,
                    finding that performance disparities are most associated with differences in <em>texture,
                        occlusion</em>, and images with <em>darker lighting</em>.We illustrate how insights from our
                    factor labels can surface mitigations to improve models' performance disparities.As an example, we
                    show that mitigating a model's vulnerability to texture can improve performance on the lower income
                    level.<strong>We release all the factor annotations along with an interactive dashboardto facilitate
                        research into more equitable vision systems.</strong></p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71093">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-62"></span>

        <script>
        add_bookmark_click(
            71093,
             1,
            'bookmark-number-62',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71093">Precise asymptotic generalization for multiclass
                classification with overparameterized linear models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">David Wu · Anant Sahai</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71093">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71093-thumb.png?t=1702110458.7849197" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71093" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71093" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71093">
                    Abstract <i id="caret-71093" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71093">
            <div class="abstract-display">
                <p>We study the asymptotic generalization of an overparameterized linear model for multiclass
                    classification under the Gaussian covariates bi-level model introduced in Subramanian et al.
                    (NeurIPS'22), where the number of data points, features, and classes all grow together. We fully
                    resolve the conjecture posed in Subramanian et al. '22, matching the predicted regimes for which the
                    model does and does not generalize. Furthermore, our new lower bounds are akin to an
                    information-theoretic strong converse: they establish that the misclassification rate goes to 0 or 1
                    asymptotically. One surprising consequence of our tight results is that the min-norm interpolating
                    classifier can be asymptotically suboptimal relative to noninterpolating classifiers in the regime
                    where the min-norm interpolating regressor is known to be optimal. The key to our tight analysis is
                    a new variant of the Hanson-Wright inequality which is broadly useful for multiclass problems with
                    sparse labels. As an application, we show that the same type of analysis can be used to analyze the
                    related multi-label classification problem under the same bi-level ensemble.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72619">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-63"></span>

        <script>
        add_bookmark_click(
            72619,
             1,
            'bookmark-number-63',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72619">Distribution-Free Statistical Dispersion Control
                for Societal Applications</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zhun Deng · Thomas Zollo · Jake Snell · Toniann Pitassi · Richard Zemel</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72619">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72619" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72619" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72619">
                    Abstract <i id="caret-72619" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72619">
            <div class="abstract-display">
                <p>Explicit finite-sample statistical guarantees on model performance are an important ingredient in
                    responsible machine learning. Previous work has focused mainly on bounding either the expected loss
                    of a predictor or the probability that an individual prediction will incur a loss value in a
                    specified range. However, for many high-stakes applications it is crucial to understand and control
                    the \textit{dispersion} of a loss distribution, or the extent to which different members of a
                    population experience unequal effects of algorithmic decisions. We initiate the study of
                    distribution-free control of statistical dispersion measures with societal implications and propose
                    a simple yet flexible framework that allows us to handle a much richer class of statistical
                    functionals beyond previous work. Our methods are verified through experiments in toxic comment
                    detection, medical imaging, and film recommendation.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73420">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-64"></span>

        <script>
        add_bookmark_click(
            73420,
             1,
            'bookmark-number-64',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73420">Validated Image Caption Rating Dataset</a>
        </div>
        <div class="type_display_name_virtual_card">Poster</div>
        <div class="author-str">Lothar D Narins · Andrew Scott · Aakash Gautam · Anagha Kulkarni · Mar Castanon ·
            Benjamin Kao · Shasta Ihorn · Yue-Ting Siu · James M. Mason · Alexander Blum · Ilmi Yoon
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73420">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/73420-thumb.png?t=1699591655.6488643" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73420" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73420" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73420">
                    Abstract <i id="caret-73420" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73420">
            <div class="abstract-display">
                We present a new high-quality validated image caption rating (VICR) dataset. How well a caption fits an
                image can be difficult to assess due to the subjective nature of caption quality. How do we evaluate
                whether a caption is good? We generated a new dataset to help answer this question by using our new
                image caption rating system, which consists of a novel robust rating scale and gamified approach to
                gathering human ratings. We show that our approach is consistent and teachable. 113 participants were
                involved in generating the dataset, which is composed of 68,217 ratings among 15,646 image-caption
                pairs. Our new dataset has greater inter-rater agreement than the state of the art, and custom machine
                learning rating predictors that were trained on our dataset outperform previous metrics. We improve over
                Flickr8k-Expert in Kendall's $W$ by 12\% and in Fleiss' $\kappa$ by 19\%, and thus provide a new
                benchmark dataset for image caption rating.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70520">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-65"></span>

        <script>
        add_bookmark_click(
            70520,
             1,
            'bookmark-number-65',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70520">The Equivalence of Dynamic and Strategic Stability
                under Regularized Learning in Games</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Victor Boone · Panayotis Mertikopoulos</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70520">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70520" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70520" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70520">
                    Abstract <i id="caret-70520" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70520">
            <div class="abstract-display">
                <p>In this paper, we examine the long-run behavior of regularized, no-regret learning in finite N-player
                    games. A well-known result in the field states that the empirical frequencies of play under
                    no-regret learning converge to the game’s set of coarse correlated equilibria; however, our
                    understanding of how the players' <em>actual strategies</em> evolve over time is much more limited –
                    and, in many cases, non-existent. This issue is exacerbated further by a series of recent results
                    showing that <em>only</em> strict Nash equilibria are stable and attracting under regularized
                    learning, thus making the relation between learning and <em>pointwise</em> solution concepts
                    particularly elusive. In lieu of this, we take a more general approach and instead seek to
                    characterize the <em>setwise</em> rationality properties of the players' day-to-day trajectory of
                    play. To do so, we focus on one of the most stringent criteria of setwise strategic stability,
                    namely that any unilateral deviation from the set in question incurs a cost to the deviator – a
                    property known as <em>closedness under better replies</em> (club). In so doing, we obtain a
                    remarkable equivalence between strategic and dynamic stability: <em>a product of pure strategies is
                        closed under better replies if and only if its span is stable and attracting …</em></p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73425">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-66"></span>

        <script>
        add_bookmark_click(
            73425,
             1,
            'bookmark-number-66',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73425">How Far Can Camels Go? Exploring the State of
                Instruction Tuning on Open Resources</a>
        </div>
        <div class="type_display_name_virtual_card">Poster</div>
        <div class="author-str">Yizhong Wang · Hamish Ivison · Pradeep Dasigi · Jack Hessel · Tushar Khot · Khyathi
            Chandu · David Wadden · Kelsey MacMillan · Noah Smith · Iz Beltagy · Hannaneh Hajishirzi
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73425">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/73425-thumb.png?t=1702202161.2810826" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73425" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73425" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73425">
                    Abstract <i id="caret-73425" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73425">
            <div class="abstract-display">
                <p>In this work we explore recent advances in instruction-tuning language models on a range of open
                    instruction-following datasets. Despite recent claims that open models can be on par with
                    state-of-the-art proprietary models, these claims are often accompanied by limited evaluation,
                    making it difficult to compare models across the board and determine the utility of various
                    resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size,
                    trained on 12 instruction datasets ranging from manually curated (e.g., OpenAssistant) to synthetic
                    and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning,
                    multilinguality, coding, safety, and open-ended instruction following abilities through a collection
                    of automatic, model-based, and human-based metrics. We further introduce Tülu, our best performing
                    instruction-tuned model suite finetuned on a combination of high-quality open resources.Our
                    experiments show that different instruction-tuning datasets can uncover or enhance specific skills,
                    while no single dataset (or combination) provides the best performance across all evaluations.
                    Interestingly, we find that model and human preference-based evaluations fail to reflect differences
                    in model capabilities exposed by benchmark-based evaluations, suggesting the need for the type of
                    systemic evaluation performed in this work. Our evaluations show that the best model in any given
                    …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72805">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-67"></span>

        <script>
        add_bookmark_click(
            72805,
             1,
            'bookmark-number-67',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72805">Smoothed Analysis of Sequential Probability
                Assignment</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Alankrita Bhatt · Nika Haghtalab · Abhishek Shetty</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72805">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72805" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72805" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72805">
                    Abstract <i id="caret-72805" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72805">
            <div class="abstract-display">
                <p>We initiate the study of smoothed analysis for the sequential probability assignment problem with
                    contexts. We study information-theoretically optimal minmax rates as well as a framework for
                    algorithmic reduction involving the maximum likelihood estimator oracle. Our approach establishes a
                    general-purpose reduction from minimax rates for sequential probability assignment for smoothed
                    adversaries to minimax rates for transductive learning. This leads to optimal (logarithmic) fast
                    rates for parametric classes and classes with finite VC dimension. On the algorithmic front, we
                    develop an algorithm that efficiently taps into the MLE oracle, for general classes of functions. We
                    show that under general conditions this algorithmic approach yields sublinear regret.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72296">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-68"></span>

        <script>
        add_bookmark_click(
            72296,
             1,
            'bookmark-number-68',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72296">Does Localization Inform Editing? Surprising
                Differences in Causality-Based Localization vs. Knowledge Editing in Language Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Peter Hase · Mohit Bansal · Been Kim · Asma Ghandeharioun</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72296">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72296-thumb.png?t=1700237454.0344782" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72296" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72296" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72296">
                    Abstract <i id="caret-72296" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72296">
            <div class="abstract-display">
                <p>Language models learn a great quantity of factual information during pretraining, and recent work
                    localizes this information to specific model weights like mid-layer MLP weights. In this paper, we
                    find that we can change how a fact is stored in a model by editing weights that are in a different
                    location than where existing methods suggest that the fact is stored. This is surprising because we
                    would expect that localizing facts to specific model parameters would tell us where to manipulate
                    knowledge in models, and this assumption has motivated past work on model editing methods.
                    Specifically, we show that localization conclusions from representation denoising (also known as
                    Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order
                    to override an existing stored fact with a new one. This finding raises questions about how past
                    work relies on Causal Tracing to select which model layers to edit. Next, we consider several
                    variants of the editing problem, including erasing and amplifying facts. For one of our editing
                    problems, editing performance does relate to localization results from representation denoising, but
                    we find that which layer we edit is a far better predictor of performance. …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71116">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-69"></span>

        <script>
        add_bookmark_click(
            71116,
             1,
            'bookmark-number-69',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71116">Tight Risk Bounds for Gradient Descent on Separable
                Data</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Matan Schliserman · Tomer Koren</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71116">Tue 12 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71116" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71116" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71116">
                    Abstract <i id="caret-71116" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71116">
            <div class="abstract-display">
                We study the generalization properties of unregularized gradient methods applied to separable linear
                classification---a setting that has received considerable attention since the pioneering work of Soudry
                et al. (2018).We establish tight upper and lower (population) risk bounds for gradient descent in this
                setting, for any smooth loss function, expressed in terms of its tail decay rate.Our bounds take the
                form $\Theta(r_{\ell,T}^2 / \gamma^2 T + r_{\ell,T}^2 / \gamma^2 n)$, where $T$ is the number of
                gradient steps, $n$ is size of the training set, $\gamma$ is the data margin, and $r_{\ell,T}$ is a
                complexity term that depends on the tail decay rate of the loss function (and on $T$).Our upper bound
                greatly improves the existing risk bounds due to Shamir (2021) and Schliserman and Koren (2022), that
                either applied to specific loss functions or imposed extraneous technical assumptions, and applies to
                virtually any convex and smooth loss function.Our risk lower bound is the first in this context and
                establish the tightness of our general upper bound for any given tail decay rate and in all parameter
                regimes.The proof technique used to show these results is also markedly simpler compared to previous
                work, and is straightforward to extend to other gradient …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72513">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-70"></span>

        <script>
        add_bookmark_click(
            72513,
             1,
            'bookmark-number-70',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72513">Fast Approximation of Similarity Graphs with Kernel
                Density Estimation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Peter Macgregor · He Sun</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72513">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72513-thumb.png?t=1701565807.9856153" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72513" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72513" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72513">
                    Abstract <i id="caret-72513" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72513">
            <div class="abstract-display">
                Constructing a similarity graph from a set $X$ of data points in $ \mathbb{R}^d$ is the first step of
                many modern clustering algorithms. However, typical constructions of a similarity graph have high time
                complexity, and a quadratic space dependency with respect to $|X|$. We address this limitation and
                present a new algorithmic framework that constructs a sparse approximation of the fully connected
                similarity graph while preserving its cluster structure. Our presented algorithm is based on the kernel
                density estimation problem, and is applicable for arbitrary kernel functions. We compare our designed
                algorithm with the well-known implementations from the scikit-learn library and the FAISS library, and
                find that our method significantly outperforms the implementation from both libraries on a variety of
                datasets.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73019">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-71"></span>

        <script>
        add_bookmark_click(
            73019,
             1,
            'bookmark-number-71',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73019">Gaussian Partial Information Decomposition: Bias
                Correction and Application to High-dimensional Data</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Praveen Venkatesh · Corbett Bennett · Sam Gale · Tamina Ramirez · Greggory Heller ·
            Severine Durand · Shawn Olsen · Stefan Mihalas
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73019">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73019" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73019" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73019">
                    Abstract <i id="caret-73019" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73019">
            <div class="abstract-display">
                <p>Recent advances in neuroscientific experimental techniques have enabled us to simultaneously record
                    the activity of thousands of neurons across multiple brain regions. This has led to a growing need
                    for computational tools capable of analyzing how task-relevant information is represented and
                    communicated between several brain regions. Partial information decompositions (PIDs) have emerged
                    as one such tool, quantifying how much unique, redundant and synergistic information two or more
                    brain regions carry about a task-relevant message. However, computing PIDs is computationally
                    challenging in practice, and statistical issues such as the bias and variance of estimates remain
                    largely unexplored. In this paper, we propose a new method for efficiently computing and estimating
                    a PID definition on multivariate Gaussian distributions. We show empirically that our method
                    satisfies an intuitive additivity property, and recovers the ground truth in a battery of canonical
                    examples, even at high dimensionality. We also propose and evaluate, for the first time, a method to
                    correct the bias in PID estimates at finite sample sizes. Finally, we demonstrate that our Gaussian
                    PID effectively characterizes inter-areal interactions in the mouse brain, revealing higher
                    redundancy between visual areas when a stimulus is behaviorally relevant.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72915">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-72"></span>

        <script>
        add_bookmark_click(
            72915,
             1,
            'bookmark-number-72',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72915">Schema-learning and rebinding as mechanisms of
                in-context learning and emergence</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Sivaramakrishnan Swaminathan · Antoine Dedieu · Rajkumar Vasudeva Raju · Murray Shanahan
            · Miguel Lazaro-Gredilla · Dileep George
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72915">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72915" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72915" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72915">
                    Abstract <i id="caret-72915" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72915">
            <div class="abstract-display">
                <p>In-context learning (ICL) is one of the most powerful and most unexpected capabilities to emerge in
                    recent transformer-based large language models (LLMs). Yet the mechanisms that underlie it are
                    poorly understood. In this paper, we demonstrate that comparable ICL capabilities can be acquired by
                    an alternative sequence prediction learning method using clone-structured causal graphs (CSCGs).
                    Moreover, a key property of CSCGs is that, unlike transformer-based LLMs, they are {\em
                    interpretable}, which considerably simplifies the task of explaining how ICL works. Specifically, we
                    show that it uses a combination of (a) learning template (schema) circuits for pattern completion,
                    (b) retrieving relevant templates in a context-sensitive manner, and (c) rebinding of novel tokens
                    to appropriate slots in the templates. We go on to marshall evidence for the hypothesis that similar
                    mechanisms underlie ICL in LLMs. For example, we find that, with CSCGs as with LLMs, different
                    capabilities emerge at different levels of overparameterization, suggesting that
                    overparameterization helps in learning more complex template (schema) circuits. By showing how ICL
                    can be achieved with small models and datasets, we open up a path to novel architectures, and take a
                    vital step towards a more general understanding of the mechanics behind this important
                    capability.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72422">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-73"></span>

        <script>
        add_bookmark_click(
            72422,
             1,
            'bookmark-number-73',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72422">In-Context Impersonation Reveals Large Language
                Models' Strengths and Biases</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Leonard Salewski · Stephan Alaniz · Isabel Rio-Torto · Eric Schulz · Zeynep Akata</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72422">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72422-thumb.png?t=1701808578.2445805" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72422" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72422" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72422">
                    Abstract <i id="caret-72422" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72422">
            <div class="abstract-display">
                <p>In everyday conversations, humans can take on different roles and adapt their vocabulary to their
                    chosen roles. We explore whether LLMs can take on, that is impersonate, different roles when they
                    generate text in-context. We ask LLMs to assume different personas before solving vision and
                    language tasks. We do this by prefixing the prompt with a persona that is associated either with a
                    social identity or domain expertise. In a multi-armed bandit task, we find that LLMs pretending to
                    be children of different ages recover human-like developmental stages of exploration. In a
                    language-based reasoning task, we find that LLMs impersonating domain experts perform better than
                    LLMs impersonating non-domain experts. Finally, we test whether LLMs' impersonations are
                    complementary to visual information when describing different categories. We find that impersonation
                    can improve performance: an LLM prompted to be a bird expert describes birds better than one
                    prompted to be a car expert. However, impersonation can also uncover LLMs' biases: an LLM prompted
                    to be a man describes cars better than one prompted to be a woman. These findings demonstrate that
                    LLMs are capable of taking on diverse roles and that this in-context impersonation can be used to
                    uncover their strengths and hidden …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72561">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-74"></span>

        <script>
        add_bookmark_click(
            72561,
             1,
            'bookmark-number-74',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72561">Maximize to Explore: One Objective Function Fusing
                Estimation, Planning, and Exploration</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zhihan Liu · Miao Lu · WEI XIONG · Han Zhong · Hao Hu · Shenao Zhang · Sirui Zheng ·
            Zhuoran Yang · Zhaoran Wang
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72561">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72561" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72561" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72561">
                    Abstract <i id="caret-72561" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72561">
            <div class="abstract-display">
                <p>In reinforcement learning (RL), balancing exploration and exploitation is crucial for achieving an
                    optimal policy in a sample-efficient way. To this end, existing sample- efficient algorithms
                    typically consist of three components: estimation, planning, and exploration. However, to cope with
                    general function approximators, most of them involve impractical algorithmic components to
                    incentivize exploration, such as data-dependent level-set constraints or complicated sampling
                    procedures. To address this challenge, we propose an easy-to-implement RL framework called Maximize
                    to Explore (MEX), which only needs to optimize unconstrainedly a single objective that integrates
                    the estimation and planning components while balancing exploration and exploitation automatically.
                    Theoretically, we prove that the MEX achieves a sublinear regret with general function approximators
                    and is extendable to the zero-sum Markov game setting. Meanwhile, we adapt deep RL baselines to
                    design practical versions of MEX in both the model-based and model-free settings, which outperform
                    baselines in various MuJoCo environments with sparse reward by a stable margin. Compared with
                    existing sample-efficient algorithms with general function approximators, MEX achieves similar
                    sample efficiency while also enjoying a lower computational cost and is more compatible with modern
                    deep RL methods.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72639">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-75"></span>

        <script>
        add_bookmark_click(
            72639,
             1,
            'bookmark-number-75',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72639">No Change, No Gain: Empowering Graph Neural
                Networks with Expected Model Change Maximization for Active Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zixing Song · Yifei Zhang · Irwin King</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72639">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72639" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72639" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72639">
                    Abstract <i id="caret-72639" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72639">
            <div class="abstract-display">
                <p>Graph Neural Networks (GNNs) are crucial for machine learning applications with graph-structured
                    data, but their success depends on sufficient labeled data. We present a novel active learning (AL)
                    method for GNNs, extending the Expected Model Change Maximization (EMCM) principle to improve
                    prediction performance on unlabeled data. By presenting a Bayesian interpretation for the node
                    embeddings generated by GNNs under the semi-supervised setting, we efficiently compute the
                    closed-form EMCM acquisition function as the selection criterion for AL without re-training. Our
                    method establishes a direct connection with expected prediction error minimization, offering
                    theoretical guarantees for AL performance. Experiments demonstrate our method's effectiveness
                    compared to existing approaches, in terms of both accuracy and efficiency.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72793">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-76"></span>

        <script>
        add_bookmark_click(
            72793,
             1,
            'bookmark-number-76',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72793">The Goldilocks of Pragmatic Understanding:
                Fine-Tuning Strategy Matters for Implicature Resolution by LLMs</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Laura Ruis · Akbir Khan · Stella Biderman · Sara Hooker · Tim Rocktäschel · Edward
            Grefenstette
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72793">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72793" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72793" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72793">
                    Abstract <i id="caret-72793" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72793">
            <div class="abstract-display">
                <p>Despite widespread use of LLMs as conversational agents, evaluations of performance fail to capture a
                    crucial aspect of communication: interpreting language in context---incorporating its pragmatics.
                    Humans interpret language using beliefs and prior knowledge about the world. For example, we
                    intuitively understand the response "I wore gloves" to the question "Did you leave fingerprints?" as
                    meaning "No". To investigate whether LLMs have the ability to make this type of inference, known as
                    an implicature, we design a simple task and evaluate four categories of widely used state-of-the-art
                    models. We find that, despite only evaluating on utterances that require a binary inference (yes or
                    no), models in three of these categories perform close to random. However, LLMs instruction-tuned at
                    the example-level perform significantly better. These results suggest that certain fine-tuning
                    strategies are far better at inducing pragmatic understanding in models. We present our findings as
                    the starting point for further research into evaluating how LLMs interpret language in context and
                    to drive the development of more pragmatic and useful models of human discourse.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72666">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-77"></span>

        <script>
        add_bookmark_click(
            72666,
             1,
            'bookmark-number-77',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72666">Towards Automated Circuit Discovery for Mechanistic
                Interpretability</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Arthur Conmy · Augustine Mavor-Parker · Aengus Lynch · Stefan Heimersheim · Adrià
            Garriga-Alonso
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72666">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72666-thumb.png?t=1701482717.337568" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72666" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72666" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72666">
                    Abstract <i id="caret-72666" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72666">
            <div class="abstract-display">
                <p>Through considerable effort and intuition, several recent works have reverse-engineered nontrivial
                    behaviors oftransformer models. This paper systematizes the mechanistic interpretability process
                    they followed. First, researcherschoose a metric and dataset that elicit the desired model behavior.
                    Then, they apply activation patching to find whichabstract neural network units are involved in the
                    behavior. By varying the dataset, metric, and units underinvestigation, researchers can understand
                    the functionality of each component.We automate one of the process' steps: finding the connections
                    between the abstract neural network units that form a circuit. We propose several algorithms and
                    reproduce previous interpretability results to validate them. Forexample, the ACDC algorithm
                    rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes theGreater-Than
                    operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found
                    byprevious work. Our code is available at
                    https://github.com/ArthurConmy/Automatic-Circuit-Discovery</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72865">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-78"></span>

        <script>
        add_bookmark_click(
            72865,
             1,
            'bookmark-number-78',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72865">On the Variance, Admissibility, and Stability of
                Empirical Risk Minimization</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Gil Kur · Eli Putterman · Alexander Rakhlin</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72865">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72865" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72865" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72865">
                    Abstract <i id="caret-72865" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72865">
            <div class="abstract-display">
                It is well known that Empirical Risk Minimization (ERM) may attain minimax suboptimal rates in terms of
                the mean squared error (Birgé and Massart, 1993). In this paper, we prove that, under relatively mild
                assumptions, the suboptimality of ERM must be due to its bias. Namely, the variance error term of ERM
                (in terms of the bias and variance decomposition) enjoys the minimax rate. In the fixed design setting,
                we provide an elementary proof of this result using the probabilistic method. Then, we extend our proof
                to the random design setting for various models. In addition, we provide a simple proof of Chatterjee’s
                admissibility theorem (Chatterjee, 2014, Theorem 1.4), which states that in the fixed design setting,
                ERM cannot be ruled out as an optimal method, and then we extend this result to the random design
                setting. We also show that our estimates imply stability of ERM, complementing the main result of
                Caponnetto and Rakhlin (2006) for non-Donsker classes. Finally, we highlight the somewhat irregular
                nature of the loss landscape of ERM in the non-Donsker regime, by showing that functions can be close to
                ERM, in terms of $L_2$ distance, while still being far from almost-minimizers of the empirical loss.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72191">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-79"></span>

        <script>
        add_bookmark_click(
            72191,
             1,
            'bookmark-number-79',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72191">Score-based Generative Modeling through Stochastic
                Evolution Equations in Hilbert Spaces</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Sungbin Lim · EUN BI YOON · Taehyun Byun · Taewon Kang · Seungwoo Kim · Kyungjae Lee ·
            Sungjoon Choi
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72191">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72191-thumb.png?t=1700158779.4415946" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72191" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72191" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72191">
                    Abstract <i id="caret-72191" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72191">
            <div class="abstract-display">
                <p>Continuous-time score-based generative models consist of a pair of stochastic differential equations
                    (SDEs)—a forward SDE that smoothly transitions data into a noise space and a reverse SDE that
                    incrementally eliminates noise from a Gaussian prior distribution to generate data distribution
                    samples—are intrinsically connected by the time-reversal theory on diffusion processes. In this
                    paper, we investigate the use of stochastic evolution equations in Hilbert spaces, which expand the
                    applicability of SDEs in two aspects: sample space and evolution operator, so they enable
                    encompassing recent variations of diffusion models, such as generating functional data or replacing
                    drift coefficients with image transformation. To this end, we derive a generalized time-reversal
                    formula to build a bridge between probabilistic diffusion models and stochastic evolution equations
                    and propose a score-based generative model called Hilbert Diffusion Model (HDM). Combining with
                    Fourier neural operator, we verify the superiority of HDM for sampling functions from functional
                    datasets with a power of kernel two-sample test of 4.2 on Quadratic, 0.2 on Melbourne, and 3.6 on
                    Gridwatch, which outperforms existing diffusion models formulated in function spaces. Furthermore,
                    the proposed method shows its strength in motion synthesis tasks by utilizing the Wiener process
                    with values in Hilbert space. Finally, our empirical results …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73023">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-80"></span>

        <script>
        add_bookmark_click(
            73023,
             1,
            'bookmark-number-80',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73023">Banana: Banach Fixed-Point Network for Pointcloud
                Segmentation with Inter-Part Equivariance</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Congyue Deng · Jiahui Lei · William B Shen · Kostas Daniilidis · Leonidas Guibas</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73023">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73023" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73023" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73023">
                    Abstract <i id="caret-73023" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73023">
            <div class="abstract-display">
                <p>Equivariance has gained strong interest as a desirable network property that inherently ensures
                    robust generalization. However, when dealing with complex systems such as articulated objects or
                    multi-object scenes, effectively capturing inter-part transformations poses a challenge, as it
                    becomes entangled with the overall structure and local transformations. The interdependence of part
                    assignment and per-part group action necessitates a novel equivariance formulation that allows for
                    their co-evolution. In this paper, we present Banana, a Banach fixed-point network for equivariant
                    segmentation with inter-part equivariance by construction. Our key insight is to iteratively solve a
                    fixed-point problem, where point-part assignment labels and per-part SE(3)-equivariance co-evolve
                    simultaneously. We provide theoretical derivations of both per-step equivariance and global
                    convergence, which induces an equivariant final convergent state. Our formulation naturally provides
                    a strict definition of inter-part equivariance that generalizes to unseen inter-part configurations.
                    Through experiments conducted on both articulated objects and multi-object scans, we demonstrate the
                    efficacy of our approach in achieving strong generalization under inter-part transformations, even
                    when confronted with substantial changes in pointcloud geometry and topology.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72985">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-81"></span>

        <script>
        add_bookmark_click(
            72985,
             1,
            'bookmark-number-81',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72985">Unexpected Improvements to Expected Improvement for
                Bayesian Optimization</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Sebastian Ament · Samuel Daulton · David Eriksson · Maximilian Balandat · Eytan Bakshy
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72985">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72985" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72985" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72985">
                    Abstract <i id="caret-72985" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72985">
            <div class="abstract-display">
                <p>Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization
                    and has found countless successful applications, but its performance is often exceeded by that of
                    more recent methods. Notably, EI and its variants, including for the parallel and multi-objective
                    settings, are challenging to optimize because their acquisition values vanish numerically in many
                    regions. This difficulty generally increases as the number of observations, dimensionality of the
                    search space, or the number of constraints grow, resulting in performance that is inconsistent
                    across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of
                    acquisition functions whose members either have identical or approximately equal optima as their
                    canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that
                    numerical pathologies manifest themselves in “classic” analytic EI, Expected Hypervolume Improvement
                    (EHVI), as well as their constrained, noisy, and parallel variants, and propose corresponding
                    reformulations that remedy these pathologies. Our empirical results show that members of the LogEI
                    family of acquisition functions substantially improve on the optimization performance of their
                    canonical counterparts and surprisingly, are on par with or exceed the performance of recent
                    state-of-the-art acquisition functions, highlighting the understated role of numerical optimization
                    in the literature.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72772">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-82"></span>

        <script>
        add_bookmark_click(
            72772,
             1,
            'bookmark-number-82',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72772">Counterfactual Memorization in Neural Language
                Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Chiyuan Zhang · Daphne Ippolito · Katherine Lee · Matthew Jagielski · Florian Tramer ·
            Nicholas Carlini
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72772">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72772-thumb.png?t=1701468528.39064" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72772" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72772" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72772">
                    Abstract <i id="caret-72772" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72772">
            <div class="abstract-display">
                <p>Modern neural language models that are widely used in various NLP tasks risk memorizing sensitive
                    information from their training data.Understanding this memorization is important in real world
                    applications and also from a learning-theoretical perspective. An open question in previous studies
                    of language model memorization is how to filter out ``common'' memorization. In fact, most
                    memorization criteria strongly correlate with the number of occurrences in the training set,
                    capturing memorized familiar phrases, public knowledge, templated texts, or other repeated data.We
                    formulate a notion of counterfactual memorization which characterizes how a model's predictions
                    change if a particular document is omitted during training.We identify and study
                    counterfactually-memorized training examples in standard text datasets.We estimate the influence of
                    each memorized training example on the validation set and on generated texts, showing how this can
                    provide direct evidence of the source of memorization at test time.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72540">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-83"></span>

        <script>
        add_bookmark_click(
            72540,
             1,
            'bookmark-number-83',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72540">Dense and Aligned Captions (DAC) Promote
                Compositional Reasoning in VL Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Sivan Doveh · Assaf Arbelle · Sivan Harary · Roei Herzig · Donghyun Kim · Paola
            Cascante-Bonilla · Amit Alfassy · Rameswar Panda · Raja Giryes · Rogerio Feris · Shimon Ullman · Leonid
            Karlinsky
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72540">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72540-thumb.png?t=1701252608.5874212" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72540" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72540" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72540">
                    Abstract <i id="caret-72540" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72540">
            <div class="abstract-display">
                Vision and Language (VL) models offer an effective method for aligning representation spaces of images
                and text allowing for numerous applications such as cross-modal retrieval, visual and multi-hop question
                answering, captioning, and many more. However, the aligned image-text spaces learned by all the popular
                VL models are still suffering from the so-called 'object bias' - their representations behave as 'bags
                of nouns' mostly ignoring or downsizing the attributes, relations, and states of objects
                described/appearing in texts/images. Although some great attempts at fixing these `compositional
                reasoning' issues were proposed in the recent literature, the problem is still far from being solved. In
                this paper, we uncover two factors limiting the VL models' compositional reasoning performance. These
                two factors are properties of the paired VL dataset used for finetuning (or pre-training) the VL model:
                (i) the caption quality, or in other words 'image-alignment', of the texts; and (ii) the 'density' of
                the captions in the sense of mentioning all the details appearing on the image. We propose a fine-tuning
                approach for automatically treating these factors on a standard collection of paired VL data (CC3M).
                Applied to CLIP, we demonstrate its significant compositional reasoning performance increase of up to
                $\sim27$\% over the base …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72806">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-84"></span>

        <script>
        add_bookmark_click(
            72806,
             1,
            'bookmark-number-84',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72806">Debias Coarsely, Sample Conditionally: Statistical
                Downscaling through Optimal Transport and Probabilistic Diffusion Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zhong Yi Wan · Ricardo Baptista · Anudhyan Boral · Yi-Fan Chen · John Anderson · Fei Sha
            · Leonardo Zepeda-Núñez
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72806">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72806" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72806" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72806">
                    Abstract <i id="caret-72806" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72806">
            <div class="abstract-display">
                We introduce a two-stage probabilistic framework for statistical downscaling using unpaired data.
                Statistical downscaling seeks a probabilistic map to transform low-resolution data from a biased
                coarse-grained numerical scheme to high-resolution data that is consistent with a high-fidelity scheme.
                Our framework tackles the problem bycomposing two transformations: (i) a debiasing step via an optimal
                transport map, and (ii) an upsampling step achieved by a probabilistic diffusion model with a posteriori
                conditional sampling. This approach characterizes a conditional distribution without needing paired
                data, and faithfully recovers relevant physical statistics from biased samples. We demonstrate the
                utility of the proposed approach on one- and two-dimensional fluid flow problems, which are
                representative of the core difficulties present in numerical simulations of weather and climate. Our
                method produces realistic high-resolution outputs from low-resolution inputs, by upsampling resolutions
                of $8\times$ and $16\times$. Moreover, our procedure correctly matches the statistics of physical
                quantities, even when the low-frequency content of the inputs and outputs do not match, a crucial but
                difficult-to-satisfy assumption needed by current state-of-the-art alternatives. Code for this work is
                available at:
                https://github.com/google-research/swirl-dynamics/tree/main/swirl_dynamics/projects/probabilistic_diffusion.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72596">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-85"></span>

        <script>
        add_bookmark_click(
            72596,
             1,
            'bookmark-number-85',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72596">Mean-field Langevin dynamics: Time-space
                discretization, stochastic gradient, and variance reduction</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Taiji Suzuki · Denny Wu · Atsushi Nitanda</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72596">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72596" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72596" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72596">
                    Abstract <i id="caret-72596" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72596">
            <div class="abstract-display">
                The mean-field Langevin dynamics (MFLD) is a nonlinear generalization of the Langevin dynamics that
                incorporates a distribution-dependent drift, and it naturally arises from the optimization of two-layer
                neural networks via (noisy) gradient descent. Recent works have shown that MFLD globally minimizes an
                entropy-regularized convex functional in the space of measures. However, all prior analyses assumed the
                infinite-particle or continuous-time limit, and cannot handle stochastic gradient updates. We provide a
                general framework to prove a uniform-in-time propagation of chaos for MFLD that takes into account the
                errors due to finite-particle approximation, time-discretization, and stochastic gradient. To
                demonstrate the wide applicability of our framework, we establish quantitative convergence rate
                guarantees to the regularized global optimal solution for $(i)$ a wide range of learning problems such
                as mean-field neural network and MMD minimization, and $(ii)$ different gradient estimators including
                SGD and SVRG. Despite the generality of our results, we achieve an improved convergence rate in both the
                SGD and SVRG settings when specialized to the standard Langevin dynamics.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72486">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-86"></span>

        <script>
        add_bookmark_click(
            72486,
             1,
            'bookmark-number-86',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72486">Outlier-Robust Gromov-Wasserstein for Graph
                Data</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Lemin Kong · Jiajin Li · Jianheng Tang · Anthony Man-Cho So</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72486">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72486" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72486" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72486">
                    Abstract <i id="caret-72486" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72486">
            <div class="abstract-display">
                <p>Gromov-Wasserstein (GW) distance is a powerful tool for comparing and aligning probability
                    distributions supported on different metric spaces. Recently, GW has become the main modeling
                    technique for aligning heterogeneous data for a wide range of graph learning tasks. However, the GW
                    distance is known to be highly sensitive to outliers, which can result in large inaccuracies if the
                    outliers are given the same weight as other samples in the objective function. To mitigate this
                    issue, we introduce a new and robust version of the GW distance called RGW. RGW features
                    optimistically perturbed marginal constraints within a Kullback-Leibler divergence-based ambiguity
                    set. To make the benefits of RGW more accessible in practice, we develop a computationally efficient
                    and theoretically provable procedure using Bregman proximal alternating linearized minimization
                    algorithm. Through extensive experimentation, we validate our theoretical results and demonstrate
                    the effectiveness of RGW on real-world graph learning tasks, such as subgraph matching and partial
                    shape correspondence.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72814">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-87"></span>

        <script>
        add_bookmark_click(
            72814,
             1,
            'bookmark-number-87',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72814">Squeeze, Recover and Relabel: Dataset Condensation
                at ImageNet Scale From A New Perspective</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zeyuan Yin · Eric Xing · Zhiqiang Shen</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72814">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72814-thumb.png?t=1698446317.0889177" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72814" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72814" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72814">
                    Abstract <i id="caret-72814" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72814">
            <div class="abstract-display">
                We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that
                decouples the bilevel optimization of model and synthetic data during training, to handle varying scales
                of datasets, model architectures and image resolutions for efficient dataset condensation. The proposed
                method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms
                of arbitrary resolutions of synthesized images, low training cost and memory consumption with
                high-resolution synthesis, and the ability to scale up to arbitrary evaluation network architectures.
                Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our
                approach achieves the highest 42.5\% and 60.8\% validation accuracy on Tiny-ImageNet and ImageNet-1K,
                outperforming all previous state-of-the-art methods by margins of 14.5\% and 32.9\%, respectively. Our
                approach also surpasses MTT in terms of speed by approximately 52$\times$ (ConvNet-4) and 16$\times$
                (ResNet-18) faster with less memory consumption of 11.6$\times$ and 6.4$\times$ during data synthesis.
                Our code and condensed datasets of 50, 200 IPC with 4K recovery budget are available at
                https://github.com/VILA-Lab/SRe2L.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72658">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-88"></span>

        <script>
        add_bookmark_click(
            72658,
             1,
            'bookmark-number-88',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72658">Exploring Loss Functions for Time-based Training
                Strategy in Spiking Neural Networks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yaoyu Zhu · Wei Fang · Xiaodong Xie · Tiejun Huang · Zhaofei Yu</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72658">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72658" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72658" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72658">
                    Abstract <i id="caret-72658" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72658">
            <div class="abstract-display">
                <p>Spiking Neural Networks (SNNs) are considered promising brain-inspired energy-efficient models due to
                    their event-driven computing paradigm.The spatiotemporal spike patterns used to convey information
                    in SNNs consist of both rate coding and temporal coding, where the temporal coding is crucial to
                    biological-plausible learning rules such as spike-timing-dependent-plasticity.The time-based
                    training strategy is proposed to better utilize the temporal information in SNNs and learn in an
                    asynchronous fashion.However, some recent works train SNNs by the time-based scheme with
                    rate-coding-dominated loss functions.In this paper, we first map rate-based loss functions to
                    time-based counterparts and explain why they are also applicable to the time-based training
                    scheme.After that, we infer that loss functions providing adequate positive overall gradients help
                    training by theoretical analysis.Based on this, we propose the enhanced counting loss to replace the
                    commonly used mean square counting loss.In addition, we transfer the training of scale factor in
                    weight standardization into thresholds.Experiments show that our approach outperforms previous
                    time-based training methods in most datasets. Our work provides insights for training SNNs with
                    time-based schemes and offers a fresh perspective on the correlation between rate coding and
                    temporal coding.Our code is available at
                    https://github.com/zhuyaoyu/SNN-temporal-training-losses.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72550">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-89"></span>

        <script>
        add_bookmark_click(
            72550,
             1,
            'bookmark-number-89',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72550">Universal Online Learning with Gradient Variations:
                A Multi-layer Online Ensemble Approach</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yu-Hu Yan · Peng Zhao · Zhi-Hua Zhou</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72550">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72550-thumb.png?t=1701868032.292955" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72550" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72550" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72550">
                    Abstract <i id="caret-72550" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72550">
            <div class="abstract-display">
                In this paper, we propose an online convex optimization approach with two different levels of
                adaptivity. On a higher level, our approach is agnostic to the unknown types and curvatures of the
                online functions, while at a lower level, it can exploit the unknown niceness of the environments and
                attain problem-dependent guarantees. Specifically, we obtain $\mathcal{O}(\log V_T)$, $\mathcal{O}(d
                \log V_T)$ and $\hat{\mathcal{O}}(\sqrt{V_T})$ regret bounds for strongly convex, exp-concave and convex
                loss functions, respectively, where $d$ is the dimension, $V_T$ denotes problem-dependent gradient
                variations and the $\hat{\mathcal{O}}(\cdot)$-notation omits $\log V_T$ factors. Our result not only
                safeguards the worst-case guarantees but also directly implies the small-loss bounds in analysis.
                Moreover, when applied to adversarial/stochastic convex optimization and game theory problems, our
                result enhances the existing universal guarantees. Our approach is based on a multi-layer online
                ensemble framework incorporating novel ingredients, including a carefully designed optimism for unifying
                diverse function types and cascaded corrections for algorithmic stability. Notably, despite its
                multi-layer structure, our algorithm necessitates only one gradient query per round, making it favorable
                when the gradient evaluation is time-consuming. This is facilitated by a novel regret decomposition
                equipped with carefully designed surrogate losses.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72818">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-90"></span>

        <script>
        add_bookmark_click(
            72818,
             1,
            'bookmark-number-90',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72818">Common Ground in Cooperative Communication</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Xiaoran Hao · Yash Jhaveri · Patrick Shafto</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72818">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72818" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72818" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72818">
                    Abstract <i id="caret-72818" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72818">
            <div class="abstract-display">
                <p>Cooperative communication plays a fundamental role in theories of human-human interaction--cognition,
                    culture, development, language, etc.--as well as human-robot interaction. The core challenge in
                    cooperative communication is the problem of common ground: having enough shared knowledge and
                    understanding to successfully communicate. Prior models of cooperative communication, however,
                    uniformly assume the strongest form of common ground, perfect and complete knowledge sharing, and,
                    therefore, fail to capture the core challenge of cooperative communication. We propose a general
                    theory of cooperative communication that is mathematically principled and explicitly defines a
                    spectrum of common ground possibilities, going well beyond that of perfect and complete knowledge
                    sharing, on spaces that permit arbitrary representations of data and hypotheses. Our framework is a
                    strict generalization of prior models of cooperative communication. After considering a parametric
                    form of common ground and viewing the data selection and hypothesis inference processes of
                    communication as encoding and decoding, we establish a connection to variational autoencoding, a
                    powerful model in modern machine learning. Finally, we carry out a series of empirical simulations
                    to support and elaborate on our theoretical results.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72899">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-91"></span>

        <script>
        add_bookmark_click(
            72899,
             1,
            'bookmark-number-91',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72899">Birth of a Transformer: A Memory Viewpoint</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Alberto Bietti · Vivien Cabannes · Diane Bouchacourt · Herve Jegou · Leon Bottou</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72899">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72899" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72899" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72899">
                    Abstract <i id="caret-72899" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72899">
            <div class="abstract-display">
                <p>Large language models based on transformers have achieved great empirical successes. However, as they
                    are deployed more widely, there is a growing need to better understand their internal mechanisms in
                    order to make them more reliable. These models appear to store vast amounts of knowledge from their
                    training data, and to adapt quickly to new information provided in their context or prompt. We study
                    how transformers balance these two types of knowledge by considering a synthetic setup where tokens
                    are generated from either global or context-specific bigram distributions. By a careful empirical
                    analysis of the training process on a simplified two-layer transformer, we illustrate the fast
                    learning of global bigrams and the slower development of an "induction head" mechanism for the
                    in-context bigrams. We highlight the role of weight matrices as associative memories, provide
                    theoretical insights on how gradients enable their learning during training, and study the role of
                    data-distributional properties.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72604">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-92"></span>

        <script>
        add_bookmark_click(
            72604,
             1,
            'bookmark-number-92',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72604">The Behavior and Convergence of Local Bayesian
                Optimization</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Kaiwen Wu · Kyurae Kim · Roman Garnett · Jacob Gardner</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72604">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72604-thumb.png?t=1701992462.478645" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72604" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72604" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72604">
                    Abstract <i id="caret-72604" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72604">
            <div class="abstract-display">
                <p>A recent development in Bayesian optimization is the use of local optimization strategies, which can
                    deliver strong empirical performance on high-dimensional problems compared to traditional global
                    strategies. The "folk wisdom" in the literature is that the focus on local optimization sidesteps
                    the curse of dimensionality; however, little is known concretely about the expected behavior or
                    convergence of Bayesian local optimization routines. We first study the behavior of the local
                    approach, and find that the statistics of individual local solutions of Gaussian process sample
                    paths are surprisingly good compared to what we would expect to recover from global methods. We then
                    present the first rigorous analysis of such a Bayesian local optimization algorithm recently
                    proposed by Müller et al. (2021), and derive convergence rates in both the noisy and noiseless
                    settings.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72868">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-93"></span>

        <script>
        add_bookmark_click(
            72868,
             1,
            'bookmark-number-93',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72868">Learning to Receive Help: Intervention-Aware
                Concept Embedding Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Mateo Espinosa Zarlenga · Katie Collins · Krishnamurthy Dvijotham · Adrian Weller ·
            Zohreh Shams · Mateja Jamnik
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72868">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72868-thumb.png?t=1700764248.3385186" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72868" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72868" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72868">
                    Abstract <i id="caret-72868" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72868">
            <div class="abstract-display">
                <p>Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures by constructing and
                    explaining their predictions using a set of high-level concepts. A special property of these models
                    is that they permit concept interventions, wherein users can correct mispredicted concepts and thus
                    improve the model's performance. Recent work, however, has shown that intervention efficacy can be
                    highly dependent on the order in which concepts are intervened on and on the model's architecture
                    and training hyperparameters. We argue that this is rooted in a CBM's lack of train-time incentives
                    for the model to be appropriately receptive to concept interventions. To address this, we propose
                    Intervention-aware Concept Embedding models (IntCEMs), a novel CBM-based architecture and training
                    paradigm that improves a model's receptiveness to test-time interventions. Our model learns a
                    concept intervention policy in an end-to-end fashion from where it can sample meaningful
                    intervention trajectories at train-time. This conditions IntCEMs to effectively select and receive
                    concept interventions when deployed at test-time. Our experiments show that IntCEMs significantly
                    outperform state-of-the-art concept-interpretable models when provided with test-time concept
                    interventions, demonstrating the effectiveness of our approach.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72404">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-94"></span>

        <script>
        add_bookmark_click(
            72404,
             1,
            'bookmark-number-94',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72404">Non-Asymptotic Analysis of a UCB-based Top Two
                Algorithm</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Marc Jourdan · Rémy Degenne</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72404">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72404-thumb.png?t=1696926284.765641" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72404" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72404" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72404">
                    Abstract <i id="caret-72404" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72404">
            <div class="abstract-display">
                <p>A Top Two sampling rule for bandit identification is a method which selects the next arm to sample
                    from among two candidate arms, a <em>leader</em> and a <em>challenger</em>. Due to their simplicity
                    and good empirical performance, they have received increased attention in recent years. However, for
                    fixed-confidence best arm identification, theoretical guarantees for Top Two methods have only been
                    obtained in the asymptotic regime, when the error level vanishes. In this paper, we derive the first
                    non-asymptotic upper bound on the expected sample complexity of a Top Two algorithm, which holds for
                    any error level. Our analysis highlights sufficient properties for a regret minimization algorithm
                    to be used as leader. These properties are satisfied by the UCB algorithm, and our proposed
                    UCB-based Top Two algorithm simultaneously enjoys non-asymptotic guarantees and competitive
                    empirical performance.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73056">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-95"></span>

        <script>
        add_bookmark_click(
            73056,
             1,
            'bookmark-number-95',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73056">Improved Frequency Estimation Algorithms with and
                without Predictions</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Anders Aamand · Justin Chen · Huy Nguyen · Sandeep Silwal · Ali Vakilian</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73056">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73056" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73056" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73056">
                    Abstract <i id="caret-73056" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73056">
            <div class="abstract-display">
                <p>Estimating frequencies of elements appearing in a data stream is a key task in large-scale data
                    analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with
                    worst-case guarantees that probabilistically bound the error of the estimated frequencies for any
                    possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to
                    tailor sketching algorithms to the specific data distribution they are being run on. In particular,
                    their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which
                    predicts which elements will appear many times in the stream. We give a novel algorithm, which in
                    some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al.
                    <em>without</em> the use of any predictions. Augmenting our algorithm with heavy-hitter predictions
                    further reduces the error and improves upon the state of the art. Empirically, our algorithms
                    achieve superior performance in all experiments compared to prior approaches.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72625">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-96"></span>

        <script>
        add_bookmark_click(
            72625,
             1,
            'bookmark-number-96',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72625">Constant Approximation for Individual Preference
                Stable Clustering</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Anders Aamand · Justin Chen · Allen Liu · Sandeep Silwal · Pattara Sukprasert · Ali
            Vakilian · Fred Zhang
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72625">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72625" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72625" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72625">
                    Abstract <i id="caret-72625" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72625">
            <div class="abstract-display">
                Individual preference (IP) stability, introduced by Ahmadi et al. (ICML 2022), is a natural clustering
                objective inspired by stability and fairness constraints. A clustering is $\alpha$-IP stable if the
                average distance of every data point to its own cluster is at most $\alpha$ times the average distance
                to any other cluster. Unfortunately, determining if a dataset admits a $1$-IP stable clustering is
                NP-Hard. Moreover, before this work, it was unknown if an $o(n)$-IP stable clustering always exists, as
                the prior state of the art only guaranteed an $O(n)$-IP stable clustering. We close this gap in
                understanding and show that an $O(1)$-IP stable clustering always exists for general metrics, and we
                give an efficient algorithm which outputs such a clustering. We also introduce generalizations of IP
                stability beyond average distance and give efficient near optimal algorithms in the cases where we
                consider the maximum and minimum distances within and between clusters.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72959">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-97"></span>

        <script>
        add_bookmark_click(
            72959,
             1,
            'bookmark-number-97',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72959">Unifying Predictions of Deterministic and
                Stochastic Physics in Mesh-reduced Space with Sequential Flow Generative Model</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Luning Sun · Xu Han · Han Gao · Jian-Xun Wang · Liping Liu</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72959">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72959-thumb.png?t=1701672468.622343" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72959" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72959" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72959">
                    Abstract <i id="caret-72959" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72959">
            <div class="abstract-display">
                <p>Accurate prediction of dynamical systems in unstructured meshes has recently shown successes in
                    scientific simulations. Many dynamical systems have a nonnegligible level of stochasticity
                    introduced by various factors (e.g. chaoticity), so there is a need for a unified framework that
                    captures both deterministic and stochastic components in the rollouts of these systems. Inspired by
                    regeneration learning, we propose a new model that combines generative and sequential networks to
                    model dynamical systems. Specifically, we use an autoencoder to learn compact representations of
                    full-space physical variables in a low-dimensional space. We then integrate a transformer with a
                    conditional normalizing flow model to model the temporal sequence of latent representations. We
                    evaluate the new model in both deterministic and stochastic systems. The model outperforms several
                    competitive baseline models and makes more accurate predictions of deterministic systems. Its own
                    prediction error is also reflected in its uncertainty estimations. When predicting stochastic
                    systems, the proposed model generates high-quality rollout samples. The mean and variance of these
                    samples well match the statistics of samples computed from expensive numerical simulations.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72982">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-98"></span>

        <script>
        add_bookmark_click(
            72982,
             1,
            'bookmark-number-98',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72982">Computing a human-like reaction time metric from
                stable recurrent vision models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Lore Goetschalckx · Lakshmi Narasimhan Govindarajan · Alekh Karkada Ashok · Aarit Ahuja
            · David Sheinberg · Thomas Serre
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72982">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72982-thumb.png?t=1702047637.8976552" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72982" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72982" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72982">
                    Abstract <i id="caret-72982" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72982">
            <div class="abstract-display">
                <p>The meteoric rise in the adoption of deep neural networks as computational models of vision has
                    inspired efforts to ``align” these models with humans. One dimension of interest for alignment
                    includes behavioral choices, but moving beyond characterizing choice patterns to capturing temporal
                    aspects of visual decision-making has been challenging. Here, we sketch a general-purpose
                    methodology to construct computational accounts of reaction times from a stimulus-computable,
                    task-optimized model. Specifically, we introduce a novel metric leveraging insights from subjective
                    logic theory summarizing evidence accumulation in recurrent vision models. We demonstrate that our
                    metric aligns with patterns of human reaction times for stimulus manipulations across four disparate
                    visual decision-making tasks spanning perceptual grouping, mental simulation, and scene
                    categorization. This work paves the way for exploring the temporal alignment of model and human
                    visual strategies in the context of various other cognitive tasks toward generating testable
                    hypotheses for neuroscience. Links to the code and data can be found on the project page:
                    https://serre-lab.github.io/rnn<em>rts</em>site/.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72878">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-99"></span>

        <script>
        add_bookmark_click(
            72878,
             1,
            'bookmark-number-99',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72878">The Pick-to-Learn Algorithm: Empowering Compression
                for Tight Generalization Bounds and Improved Post-training Performance</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Dario Paccagnan · Marco Campi · Simone Garatti</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72878">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72878-thumb.png?t=1702269051.6058128" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72878" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72878" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72878">
                    Abstract <i id="caret-72878" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72878">
            <div class="abstract-display">
                <p>Generalization bounds are valuable both for theory and applications. On the one hand, they shed light
                    on the mechanisms that underpin the learning processes; on the other, they certify how well a
                    learned model performs against unseen inputs. In this work we build upon a recent breakthrough in
                    compression theory to develop a new framework yielding tight generalization bounds of wide practical
                    applicability. The core idea is to embed any given learning algorithm into a suitably-constructed
                    meta-algorithm (here called Pick-to-Learn, P2L) in order to instill desirable compression
                    properties. When applied to the MNIST classification dataset and to a synthetic regression problem,
                    P2L not only attains generalization bounds that compare favorably with the state of the art
                    (test-set and PAC-Bayes bounds), but it also learns models with better post-training
                    performance.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72790">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-100"></span>

        <script>
        add_bookmark_click(
            72790,
             1,
            'bookmark-number-100',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72790">A Cross-Moment Approach for Causal Effect
                Estimation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yaroslav Kivva · Saber Salehkaleybar · Negar Kiyavash</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72790">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72790-thumb.png?t=1699614606.9435234" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72790" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72790" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72790">
                    Abstract <i id="caret-72790" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72790">
            <div class="abstract-display">
                <p>We consider the problem of estimating the causal effect of a treatment on an outcome in linear
                    structural causal models (SCM) with latent confounders when we have access to a single proxy
                    variable.Several methods (such as difference-in-difference (DiD) estimator or negative outcome
                    control) have been proposed in this setting in the literature. However, these approaches require
                    either restrictive assumptions on the data generating model or having access to at least two proxy
                    variables.We propose a method to estimate the causal effect using cross moments between the
                    treatment, the outcome, and the proxy variable. In particular, we show that the causal effect can be
                    identified with simple arithmetic operations on the cross moments if the latent confounder in linear
                    SCM is non-Gaussian.In this setting, DiD estimator provides an unbiased estimate only in the special
                    case where the latent confounder has exactly the same direct causal effects on the outcomes in the
                    pre-treatment and post-treatment phases. This translates to the common trend assumption in DiD,
                    which we effectively relax.Additionally, we provide an impossibility result that shows the causal
                    effect cannot be identified if the observational distribution over the treatment, the outcome, and
                    the proxy is jointly Gaussian. Our experiments on both synthetic …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72847">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-101"></span>

        <script>
        add_bookmark_click(
            72847,
             1,
            'bookmark-number-101',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72847">What Makes Data Suitable for a Locally Connected
                Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement.</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">‪Yotam Alexander‬‏ · Nimrod De La Vega · Noam Razin · Nadav Cohen</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72847">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72847-thumb.png?t=1696890443.2233346" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72847" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72847" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72847">
                    Abstract <i id="caret-72847" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72847">
            <div class="abstract-display">
                <p>The question of what makes a data distribution suitable for deep learning is a fundamental open
                    problem. Focusing on locally connected neural networks (a prevalent family of architectures that
                    includes convolutional and recurrent neural networks as well as local self-attention models), we
                    address this problem by adopting theoretical tools from quantum physics. Our main theoretical result
                    states that a certain locally connected neural network is capable of accurate prediction over a data
                    distribution if and only if the data distribution admits low quantum entanglement under certain
                    canonical partitions of features. As a practical application of this result, we derive a
                    preprocessing method for enhancing the suitability of a data distribution to locally connected
                    neural networks. Experiments with widespread models over various datasets demonstrate our findings.
                    We hope that our use of quantum entanglement will encourage further adoption of tools from physics
                    for formally reasoning about the relation between deep learning and real-world data.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71840">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-102"></span>

        <script>
        add_bookmark_click(
            71840,
             1,
            'bookmark-number-102',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71840">Distributionally Robust Skeleton Learning of
                Discrete Bayesian Networks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yeshu Li · Brian Ziebart</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71840">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71840-thumb.png?t=1700367702.170024" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71840" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71840" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71840">
                    Abstract <i id="caret-71840" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71840">
            <div class="abstract-display">
                <p>We consider the problem of learning the exact skeleton of general discrete Bayesian networks from
                    potentially corrupted data. Building on distributionally robust optimization and a regression
                    approach, we propose to optimize the most adverse risk over a family of distributions within bounded
                    Wasserstein distance or KL divergence to the empirical distribution. The worst-case risk accounts
                    for the effect of outliers. The proposed approach applies for general categorical random variables
                    without assuming faithfulness, an ordinal relationship or a specific form of conditional
                    distribution. We present efficient algorithms and show the proposed methods are closely related to
                    the standard regularized regression approach. Under mild assumptions, we derive non-asymptotic
                    guarantees for successful structure learning with logarithmic sample complexities for bounded-degree
                    graphs. Numerical study on synthetic and real datasets validates the effectiveness of our
                    method.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72471">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-103"></span>

        <script>
        add_bookmark_click(
            72471,
             1,
            'bookmark-number-103',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72471">Zero-shot causal learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Hamed Nilforoshan · Michael Moor · Yusuf Roohani · Yining Chen · Anja Šurina · Michihiro
            Yasunaga · Sara Oblak · Jure Leskovec
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72471">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72471" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72471" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72471">
                    Abstract <i id="caret-72471" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72471">
            <div class="abstract-display">
                <p>Predicting how different interventions will causally affect a specific individual is important in a
                    variety of domains such as personalized medicine, public policy, and online marketing. There are a
                    large number of methods to predict the effect of an existing intervention based on historical data
                    from individuals who received it. However, in many settings it is important to predict the effects
                    of novel interventions (e.g., a newly invented drug), which these methods do not address.Here, we
                    consider zero-shot causal learning: predicting the personalized effects of a novel intervention. We
                    propose CaML, a causal meta-learning framework which formulates the personalized prediction of each
                    intervention's effect as a task. CaML trains a single meta-model across thousands of tasks, each
                    constructed by sampling an intervention, its recipients, and its nonrecipients. By leveraging both
                    intervention information (e.g., a drug's attributes) and individual features (e.g., a patient's
                    history), CaML is able to predict the personalized effects of novel interventions that do not exist
                    at the time of training. Experimental results on real world datasets in large-scale medical claims
                    and cell-line perturbations demonstrate the effectiveness of our approach. Most strikingly, CaML's
                    zero-shot predictions outperform even strong baselines trained directly on data from the test
                    interventions.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72530">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-104"></span>

        <script>
        add_bookmark_click(
            72530,
             1,
            'bookmark-number-104',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72530">A Deep Instance Generative Framework for MILP
                Solvers Under Limited Data Availability</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zijie Geng · Xijun Li · Jie Wang · Xiao Li · Yongdong Zhang · Feng Wu</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72530">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72530" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72530" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72530">
                    Abstract <i id="caret-72530" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72530">
            <div class="abstract-display">
                <p>In the past few years, there has been an explosive surge in the use of machine learning (ML)
                    techniques to address combinatorial optimization (CO) problems, especially mixed-integer linear
                    programs (MILPs). Despite the achievements, the limited availability of real-world instances often
                    leads to sub-optimal decisions and biased solver assessments, which motivates a suite of synthetic
                    MILP instance generation techniques. However, existing methods either rely heavily on
                    expert-designed formulations or struggle to capture the rich features of real-world instances. To
                    tackle this problem, we propose G2MILP, <em>the first</em> deep generative framework for MILP
                    instances. Specifically, G2MILP represents MILP instances as bipartite graphs, and applies a masked
                    variational autoencoder to iteratively corrupt and replace parts of the original graphs to generate
                    new ones. The appealing feature of G2MILP is that it can learn to generate novel and realistic MILP
                    instances without prior expert-designed formulations, while preserving the structures and
                    computational hardness of real-world datasets, simultaneously. Thus the generated instances can
                    facilitate downstream tasks for enhancing MILP solvers under limited data availability. We design a
                    suite of benchmarks to evaluate the quality of the generated MILP instances. Experiments demonstrate
                    that our method can produce instances that closely resemble real-world datasets in terms of both …
                </p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73031">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-105"></span>

        <script>
        add_bookmark_click(
            73031,
             1,
            'bookmark-number-105',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73031">Should I Stop or Should I Go: Early Stopping with
                Heterogeneous Populations</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Hammaad Adam · Fan Yin · Huibin Hu · Neil Tenenholtz · Lorin Crawford · Lester Mackey ·
            Allison Koenecke
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73031">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/73031-thumb.png?t=1701986492.081954" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73031" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73031" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73031">
                    Abstract <i id="caret-73031" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73031">
            <div class="abstract-display">
                <p>Randomized experiments often need to be stopped prematurely due to the treatment having an unintended
                    harmful effect. Existing methods that determine when to stop an experiment early are typically
                    applied to the data in aggregate and do not account for treatment effect heterogeneity. In this
                    paper, we study the early stopping of experiments for harm on heterogeneous populations. We first
                    establish that current methods often fail to stop experiments when the treatment harms a minority
                    group of participants. We then use causal machine learning to develop CLASH, the first
                    broadly-applicable method for heterogeneous early stopping. We demonstrate CLASH's performance on
                    simulated and real data and show that it yields effective early stopping for both clinical trials
                    and A/B tests.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72817">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-106"></span>

        <script>
        add_bookmark_click(
            72817,
             1,
            'bookmark-number-106',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72817">A Spectral Theory of Neural Prediction and
                Alignment</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Abdulkadir Canatar · Jenelle Feather · Albert Wakhloo · SueYeon Chung</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72817">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72817" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72817" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72817">
                    Abstract <i id="caret-72817" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72817">
            <div class="abstract-display">
                <p>The representations of neural networks are often compared to those of biological systems by
                    performing regression between the neural network responses and those measured from biological
                    systems. Many different state-of-the-art deep neural networks yield similar neural predictions, but
                    it remains unclear how to differentiate among models that perform equally well at predicting neural
                    responses. To gain insight into this, we use a recent theoretical framework that relates the
                    generalization error from regression to the spectral properties of the model and the target. We
                    apply this theory to the case of regression between model activations and neural responses and
                    decompose the neural prediction error in terms of the model eigenspectra, alignment of model
                    eigenvectors and neural responses, and the training set size. Using this decomposition, we introduce
                    geometrical measures to interpret the neural prediction error. We test a large number of deep neural
                    networks that predict visual cortical activity and show that there are multiple types of geometries
                    that result in low neural prediction error as measured via regression. The work demonstrates that
                    carefully decomposing representational metrics can provide interpretability of how models are
                    capturing neural activity and points the way towards improved models of neural activity.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72499">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-107"></span>

        <script>
        add_bookmark_click(
            72499,
             1,
            'bookmark-number-107',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72499">Multi-Object Representation Learning via Feature
                Connectivity and Object-Centric Regularization</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Alex Foo · Wynne Hsu · Mong Li Lee</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72499">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72499-thumb.png?t=1701364468.4957433" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72499" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72499" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72499">
                    Abstract <i id="caret-72499" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72499">
            <div class="abstract-display">
                <p>Discovering object-centric representations from images has the potential to greatly improve the
                    robustness, sample efficiency and interpretability of machine learning algorithms. Current works on
                    multi-object images typically follow a generative approach that optimizes for input reconstruction
                    and fail to scale to real-world datasets despite significant increases in model capacity. We address
                    this limitation by proposing a novel method that leverages feature connectivity to cluster
                    neighboring pixels likely to belong to the same object. We further design two object-centric
                    regularization terms to refine object representations in the latent space, enabling our approach to
                    scale to complex real-world images. Experimental results on simulated, real-world, complex texture
                    and common object images demonstrate a substantial improvement in the quality of discovered objects
                    compared to state-of-the-art methods, as well as the sample efficiency and generalizability of our
                    approach. We also show that the discovered object-centric representations can accurately predict key
                    object properties in downstream tasks, highlighting the potential of our method to advance the field
                    of multi-object representation learning.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72680">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-108"></span>

        <script>
        add_bookmark_click(
            72680,
             1,
            'bookmark-number-108',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72680">HyTrel: Hypergraph-enhanced Tabular Data
                Representation Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Pei Chen · Soumajyoti Sarkar · Leonard Lausen · Balasubramaniam Srinivasan · Sheng Zha ·
            Ruihong Huang · George Karypis
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72680">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72680-thumb.png?t=1701742949.1532917" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72680" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72680" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72680">
                    Abstract <i id="caret-72680" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72680">
            <div class="abstract-display">
                <p>Language models pretrained on large collections of tabular data have demonstrated their effectiveness
                    in several downstream tasks.However, many of these models do not take into account the row/column
                    permutation invariances, hierarchical structure, etc. that exist in tabular data. To alleviate these
                    limitations, we propose HyTrel, a tabular language model, that captures the permutation invariances
                    and three more structural properties of tabular data by using hypergraphs--where the table cells
                    make up the nodes and the cells occurring jointly together in each row, column, and the entire table
                    are used to form three different types of hyperedges. We show thatHyTrel is maximally invariant
                    under certain conditions for tabular data, i.e., two tables obtain the same representations via
                    HyTreliff the two tables are identical up to permutation. Our empirical results demonstrate that
                    HyTrel consistently outperforms other competitive baselines on four downstream tasks with minimal
                    pretraining, illustrating the advantages of incorporating inductive biases associated with tabular
                    data into the representations. Finally, our qualitative analyses showcase that HyTrel can assimilate
                    the table structure to generate robust representations for the cells, rows, columns, and the entire
                    table.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72972">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-109"></span>

        <script>
        add_bookmark_click(
            72972,
             1,
            'bookmark-number-109',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72972">Learning Functional Transduction</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Mathieu Chalvidal · Thomas Serre · Rufin VanRullen</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72972">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72972" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72972" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72972">
                    Abstract <i id="caret-72972" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72972">
            <div class="abstract-display">
                <p>Research in statistical learning has polarized into two general approaches to perform regression
                    analysis: Transductive methods construct estimates directly based on exemplar data using generic
                    relational principles which might suffer from the curse of dimensionality. Conversely, inductive
                    methods can potentially fit highly complex functions at the cost of compute-intensive solution
                    searches. In this work, we leverage the theory of vector-valued Reproducing Kernel Banach Spaces
                    (RKBS) to propose a hybrid approach: We show that transductive regression systems can be
                    meta-learned with gradient descent to form efficient <em>in-context</em> neural approximators of
                    function defined over both finite and infinite-dimensional spaces (operator regression). Once
                    trained, our <em>Transducer</em> can almost instantaneously capture new functional relationships and
                    produce original image estimates, given a few pairs of input and output examples. We demonstrate the
                    benefit of our meta-learned transductive approach to model physical systems influenced by varying
                    external factors with little data at a fraction of the usual deep learning training costs for
                    partial differential equations and climate modeling applications.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72769">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-110"></span>

        <script>
        add_bookmark_click(
            72769,
             1,
            'bookmark-number-110',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72769">In-Context Learning Unlocked for Diffusion
                Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zhendong Wang · Yifan Jiang · Yadong Lu · yelong shen · Pengcheng He · Weizhu Chen ·
            Zhangyang "Atlas" Wang · Mingyuan Zhou
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72769">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72769-thumb.png?t=1701900736.6050673" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72769" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72769" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72769">
                    Abstract <i id="caret-72769" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72769">
            <div class="abstract-display">
                <p>We present Prompt Diffusion, a framework for enabling in-context learning in diffusion-based
                    generative models. Given a pair of task-specific example images, such as depth from/to image and
                    scribble from/to image, and a text guidance, our model automatically understands the underlying task
                    and performs the same task on a new query image following the text guidance. To achieve this, we
                    propose a vision-language prompt that can model a wide range of vision-language tasks and a
                    diffusion model that takes it as input. The diffusion model is trained jointly on six different
                    tasks using these prompts. The resulting Prompt Diffusion model becomes the first diffusion-based
                    vision-language foundation model capable of in-context learning. It demonstrates high-quality
                    in-context generation for the trained tasks and effectively generalizes to new, unseen vision tasks
                    using their respective prompts. Our model also shows compelling text-guided image editing results.
                    Our framework aims to facilitate research into in-context learning for computer vision. We share our
                    code and pre-trained models at https://github.com/Zhendong-Wang/Prompt-Diffusion.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72767">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-111"></span>

        <script>
        add_bookmark_click(
            72767,
             1,
            'bookmark-number-111',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72767">LinkerNet: Fragment Poses and Linker Co-Design with
                3D Equivariant Diffusion</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jiaqi Guan · Xingang Peng · PeiQi Jiang · Yunan Luo · Jian Peng · Jianzhu Ma</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72767">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72767" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72767" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72767">
                    Abstract <i id="caret-72767" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72767">
            <div class="abstract-display">
                <p>Targeted protein degradation techniques, such as PROteolysis TArgeting Chimeras (PROTACs), have
                    emerged as powerful tools for selectively removing disease-causing proteins. One challenging problem
                    in this field is designing a linker to connect different molecular fragments to form a stable
                    drug-candidate molecule. Existing models for linker design assume that the relative positions of the
                    fragments are known, which may not be the case in real scenarios. In this work, we address a more
                    general problem where the poses of the fragments are <em>unknown</em> in 3D space. We develop a 3D
                    equivariant diffusion model that jointly learns the generative process of both fragment poses and
                    the 3D structure of the linker. By viewing fragments as rigid bodies, we design a fragment pose
                    prediction module inspired by the Newton-Euler equations in rigid body mechanics. Empirical studies
                    on ZINC and PROTAC-DB datasets demonstrate that our model can generate chemically valid,
                    synthetically-accessible, and low-energy molecules under both unconstrained and constrained
                    generation settings.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72842">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-112"></span>

        <script>
        add_bookmark_click(
            72842,
             1,
            'bookmark-number-112',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72842">AlpacaFarm: A Simulation Framework for Methods that
                Learn from Human Feedback</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yann Dubois · Chen Xuechen Li · Rohan Taori · Tianyi Zhang · Ishaan Gulrajani · Jimmy Ba
            · Carlos Guestrin · Percy Liang · Tatsunori Hashimoto
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72842">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72842" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72842" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72842">
                    Abstract <i id="caret-72842" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72842">
            <div class="abstract-display">
                <p>Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to
                    follow user instructions well.Developing these LLMs involves a complex yet poorly understood
                    workflow requiring training with human feedback. Replicating and understanding this
                    instruction-following process faces three major challenges: the high cost of data collection, the
                    lack of trustworthy evaluation, and the absence of reference method implementations. We address
                    these bottlenecks with AlpacaFarm, a simulator that enables research and development for learning
                    from feedback at a low cost. First, we design LLM based simulator for human feedback that is 45x
                    cheaper than crowdworkers and displays high agreement with humans. Second, we identify an evaluation
                    dataset representative of real-world instructions and propose an automatic evaluation procedure.
                    Third, we contribute reference implementations for several methods (PPO, best-of-n, expert
                    iteration, among others) that learn from pairwise feedback. Finally, as an end-to-end validation of
                    AlpacaFarm, we train and evaluate eleven models on 10k pairs of human feedback and show that
                    rankings of models trained in AlpacaFarm match rankings of models trained on human data. As a
                    demonstration of the research possible in AlpacaFarm, we find that methods that use a reward model
                    can substantially improve over supervised fine-tuning and …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72811">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-113"></span>

        <script>
        add_bookmark_click(
            72811,
             1,
            'bookmark-number-113',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72811">PAC Learning Linear Thresholds from Label
                Proportions</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Anand Brahmbhatt · Rishi Saket · Aravindan Raghuveer</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72811">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72811" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72811" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72811">
                    Abstract <i id="caret-72811" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72811">
            <div class="abstract-display">
                Learning from label proportions (LLP) is a generalization of supervised learning in which the training
                data is available as sets or bags of feature-vectors (instances) along with the average instance-label
                of each bag. The goal is to train a good instance classifier. While most previous works on LLP have
                focused on training models on such training data, computational learnability of LLP was onlyrecently
                explored by Saket (2021, 2022) who showed worst case intractability of properly learning linear
                threshold functions (LTFs) from label proportions. However, their work did not rule out efficient
                algorithms for this problem for natural distributions.In this work we show that it is indeed possible to
                efficiently learn LTFs using LTFs when given access to random bags of some label proportion in which
                feature-vectors are, conditioned on their labels, independently sampled from a Gaussian distribution
                $N(µ, Σ)$. Our work shows that a certain matrix – formed using covariances of the differences of
                feature-vectors sampled from the bags with and without replacement – necessarily has its principal
                component, after a transformation, in the direction of the normal vector of the LTF. Our algorithm
                estimates the means and covariance matrices using subgaussian concentration bounds which we show can be
                applied …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72953">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-114"></span>

        <script>
        add_bookmark_click(
            72953,
             1,
            'bookmark-number-114',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72953">On the Connection between Pre-training Data
                Diversity and Fine-tuning Robustness</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Vivek Ramanujan · Thao Nguyen · Sewoong Oh · Ali Farhadi · Ludwig Schmidt</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72953">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72953" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72953" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72953">
                    Abstract <i id="caret-72953" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72953">
            <div class="abstract-display">
                <p>Pre-training has been widely adopted in deep learning to improve model performance, especially when
                    the training data for a target task is limited. In our work, we seek to understand the implications
                    of this training strategy on the generalization properties of downstream models. More specifically,
                    we ask the following question: how do properties of the pre-training distribution affect the
                    robustness of a fine-tuned model? The properties we explore include the label space, label
                    semantics, image diversity, data domains, and data quantity of the pre-training distribution. We
                    find that the primary factor influencing downstream effective robustness (Taori et al., 2020) is
                    data quantity, while other factors have limited significance. For example, reducing the number of
                    ImageNet pre-training classes by 4x while increasing the number of images per class by 4x (that is,
                    keeping total data quantity fixed) does not impact the robustness of fine-tuned models. We
                    demonstrate our findings on pre-training distributions drawn from various natural and synthetic data
                    sources, primarily using the iWildCam-WILDS distribution shift as a test for robustness.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72786">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-115"></span>

        <script>
        add_bookmark_click(
            72786,
             1,
            'bookmark-number-115',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72786">Compression with Bayesian Implicit Neural
                Representations</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zongyu Guo · Gergely Flamich · Jiajun He · Zhibo Chen · José Miguel Hernández-Lobato
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72786">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72786-thumb.png?t=1702246729.0169897" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72786" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72786" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72786">
                    Abstract <i id="caret-72786" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72786">
            <div class="abstract-display">
                Many common types of data can be represented as functions that map coordinates to signal values, such as
                pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by
                overfitting a compact neural network to its functional representation and then encoding the network
                weights. However, most current solutions for this are inefficient, as quantization to low-bit precision
                substantially degrades the reconstruction quality. To address this issue, we propose overfitting
                variational Bayesian neural networks to the data and compressing an approximate posterior weight sample
                using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct
                optimization of the rate-distortion performance by minimizing the $\beta$-ELBO, and target different
                rate-distortion trade-offs for a given network architecture by adjusting $\beta$. Moreover, we introduce
                an iterative algorithm for learning prior weight distributions and employ a progressive refinement
                process for the variational posterior that significantly enhances performance. Experiments show that our
                method achieves strong performance on image and audio compression while retaining simplicity.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72490">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-116"></span>

        <script>
        add_bookmark_click(
            72490,
             1,
            'bookmark-number-116',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72490">OKRidge: Scalable Optimal k-Sparse Ridge
                Regression</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jiachang Liu · Sam Rosen · Chudi Zhong · Cynthia Rudin</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72490">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72490-thumb.png?t=1701803262.7420402" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72490" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72490" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72490">
                    Abstract <i id="caret-72490" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72490">
            <div class="abstract-display">
                <p>We consider an important problem in scientific discovery, namely identifying sparse governing
                    equations for nonlinear dynamical systems. This involves solving sparse ridge regression problems to
                    provable optimality in order to determine which terms drive the underlying dynamics. We propose a
                    fast algorithm, OKRidge, for sparse ridge regression, using a novel lower bound calculation
                    involving, first, a saddle point formulation, and from there, either solving (i) a linear system or
                    (ii) using an ADMM-based approach, where the proximal operators can be efficiently evaluated by
                    solving another linear system and an isotonic regression problem. We also propose a method to
                    warm-start our solver, which leverages a beam search. Experimentally, our methods attain provable
                    optimality with run times that are orders of magnitude faster than those of the existing MIP
                    formulations solved by the commercial solver Gurobi.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73080">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-117"></span>

        <script>
        add_bookmark_click(
            73080,
             1,
            'bookmark-number-117',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73080">Complexity Matters: Rethinking the Latent Space for
                Generative Modeling</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Tianyang Hu · Fei Chen · Haonan Wang · Jiawei Li · Wenjia Wang · Jiacheng Sun · Zhenguo
            Li
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73080">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/73080-thumb.png?t=1702110456.45673" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73080" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73080" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73080">
                    Abstract <i id="caret-73080" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73080">
            <div class="abstract-display">
                <p>In generative modeling, numerous successful approaches leverage a low-dimensional latent space, e.g.,
                    Stable Diffusion models the latent space induced by an encoder and generates images through a paired
                    decoder. Although the selection of the latent space is empirically pivotal, determining the optimal
                    choice and the process of identifying it remain unclear. In this study, we aim to shed light on this
                    under-explored topic by rethinking the latent space from the perspective of model complexity. Our
                    investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN
                    training objective, we propose a novel "distance" between the latent and data distributions, whose
                    minimization coincides with that of the generator complexity. The minimizer of this distance is
                    characterized as the optimal data-dependent latent that most effectively capitalizes on the
                    generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder
                    network and propose a two-stage training strategy called Decoupled Autoencoder (DAE), where the
                    encoder is only updated in the first stage with an auxiliary decoder and then frozen in the second
                    stage while the actual decoder is being trained. DAE can improve the latent distribution and as a
                    result, improve the generative performance. Our theoretical analyses are corroborated by …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72890">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-118"></span>

        <script>
        add_bookmark_click(
            72890,
             1,
            'bookmark-number-118',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72890">QuantSR: Accurate Low-bit Quantization for
                Efficient Image Super-Resolution</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Haotong Qin · Yulun Zhang · Yifu Ding · Yifan liu · Xianglong Liu · Martin Danelljan ·
            Fisher Yu
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72890">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72890-thumb.png?t=1701439753.8267303" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72890" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72890" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72890">
                    Abstract <i id="caret-72890" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72890">
            <div class="abstract-display">
                <p>Low-bit quantization in image super-resolution (SR) has attracted copious attention in recent
                    research due to its ability to reduce parameters and operations significantly. However, many
                    quantized SR models suffer from accuracy degradation compared to their full-precision counterparts,
                    especially at ultra-low bit widths (2-4 bits), limiting their practical applications. To address
                    this issue, we propose a novel quantized image SR network, called QuantSR, which achieves accurate
                    and efficient SR processing under low-bit quantization. To overcome the representation homogeneity
                    caused by quantization in the network, we introduce the Redistribution-driven Learnable Quantizer
                    (RLQ). This is accomplished through an inference-agnostic efficient redistribution design, which
                    adds additional information in both forward and backward passes to improve the representation
                    ability of quantized networks. Furthermore, to achieve flexible inference and break the upper limit
                    of accuracy, we propose the Depth-dynamic Quantized Architecture (DQA). Our DQA allows for the
                    trade-off between efficiency and accuracy during inference through weight sharing. Our comprehensive
                    experiments show that QuantSR outperforms existing state-of-the-art quantized SR networks in terms
                    of accuracy while also providing more competitive computational efficiency. In addition, we
                    demonstrate the scheme's satisfactory architecture generality by providing QuantSR-C and QuantSR-T
                    for both convolution and Transformer versions, respectively. Our code and models are …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72419">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-119"></span>

        <script>
        add_bookmark_click(
            72419,
             1,
            'bookmark-number-119',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72419">PAPR: Proximity Attention Point Rendering</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yanshu Zhang · Shichong Peng · Alireza Moazeni · Ke Li</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72419">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72419" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72419" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72419">
                    Abstract <i id="caret-72419" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72419">
            <div class="abstract-display">
                <p>Learning accurate and parsimonious point cloud representations of scene surfaces from scratch remains
                    a challenge in 3D representation learning. Existing point-based methods often suffer from the
                    vanishing gradient problem or require a large number of points to accurately model scene geometry
                    and texture. To address these limitations, we propose Proximity Attention Point Rendering (PAPR), a
                    novel method that consists of a point-based scene representation and a differentiable renderer. Our
                    scene representation uses a point cloud where each point is characterized by its spatial position,
                    foreground score, and view-independent feature vector. The renderer selects the relevant points for
                    each ray and produces accurate colours using their associated features. PAPR effectively learns
                    point cloud positions to represent the correct scene geometry, even when the initialization
                    drastically differs from the target geometry. Notably, our method captures fine texture details
                    while using only a parsimonious set of points. We also demonstrate four practical applications of
                    our method: geometry editing, object manipulation, texture transfer, and exposure control. More
                    results and code are available on our project website at https://zvict.github.io/papr/.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72632">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-120"></span>

        <script>
        add_bookmark_click(
            72632,
             1,
            'bookmark-number-120',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72632">When Does Optimizing a Proper Loss Yield
                Calibration?</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jaroslaw Blasiok · Parikshit Gopalan · Lunjia Hu · Preetum Nakkiran</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72632">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72632-thumb.png?t=1701890674.1824358" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72632" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72632" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72632">
                    Abstract <i id="caret-72632" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72632">
            <div class="abstract-display">
                <p>Optimizing proper loss functions is popularly believed to yield predictors with good calibration
                    properties; the intuition being that for such losses, the global optimum is to predict the
                    ground-truth probabilities, which is indeed calibrated. However, typical machine learning models are
                    trained to approximately minimize loss over restricted families of predictors, that are unlikely to
                    contain the ground truth. Under what circumstances does optimizing proper loss over a restricted
                    family yield calibrated models? What precise calibration guarantees does it give? In this work, we
                    provide a rigorous answer to these questions. We replace the global optimality with a local
                    optimality condition stipulating that the (proper) loss of the predictor cannot be reduced much by
                    post-processing its predictions with a certain family of Lipschitz functions. We show that any
                    predictor with this local optimality satisfies smooth calibration as defined in [Kakade and Foster,
                    2008, Błasiok et al., 2023]. Local optimality is plausibly satisfied by well-trained DNNs, which
                    suggests an explanation for why they are calibrated from proper loss minimization alone. Finally, we
                    show that the connection between local optimality and calibration error goes both ways: nearly
                    calibrated predictors are also nearly locally optimal.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72444">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-121"></span>

        <script>
        add_bookmark_click(
            72444,
             1,
            'bookmark-number-121',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72444">Information Maximization Perspective of Orthogonal
                Matching Pursuit with Applications to Explainable AI</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Aditya Chattopadhyay · Ryan Pilgrim · Rene Vidal</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72444">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72444-thumb.png?t=1700191711.169092" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72444" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72444" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72444">
                    Abstract <i id="caret-72444" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72444">
            <div class="abstract-display">
                <p>Information Pursuit (IP) is a classical active testing algorithm for predicting an output by
                    sequentially and greedily querying the input in order of information gain. However, IP is
                    computationally intensive since it involves estimating mutual information in high-dimensional
                    spaces. This paper explores Orthogonal Matching Pursuit (OMP) as an alternative to IP for greedily
                    selecting the queries. OMP is a classical signal processing algorithm for sequentially encoding a
                    signal in terms of dictionary atoms chosen in order of correlation gain. In each iteration, OMP
                    selects the atom that is most correlated with the signal residual (the signal minus its
                    reconstruction thus far). Our first contribution is to establish a fundamental connection between IP
                    and OMP, where we prove that IP with random projections of dictionary atoms as queries ``almost''
                    reduces to OMP, with the difference being that IP selects atoms in order of normalized correlation
                    gain. We call this version IP-OMP and present simulations indicating that this difference does not
                    have any appreciable effect on the sparse code recovery rate of IP-OMP compared to that of OMP for
                    random Gaussian dictionaries. Inspired by this connection, our second contribution is to explore the
                    utility of IP-OMP for generating explainable predictions, an area …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72903">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-122"></span>

        <script>
        add_bookmark_click(
            72903,
             1,
            'bookmark-number-122',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72903">From Pixels to UI Actions: Learning to Follow
                Instructions via Graphical User Interfaces</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Peter Shaw · Mandar Joshi · James Cohan · Jonathan Berant · Panupong Pasupat · Hexiang
            Hu · Urvashi Khandelwal · Kenton Lee · Kristina N Toutanova
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72903">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72903-thumb.png?t=1701378944.9233437" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72903" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72903" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72903">
                    Abstract <i id="caret-72903" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72903">
            <div class="abstract-display">
                <p>Much of the previous work towards digital agents for graphical user interfaces (GUIs) has relied on
                    text-based representations (derived from HTML or other structured data sources), which are not
                    always readily available. These input representations have been often coupled with custom,
                    task-specific action spaces. This paper focuses on creating agents that interact with the digital
                    world using the same conceptual interface that humans commonly use — via pixel-based screenshots and
                    a generic action space corresponding to keyboard and mouse actions. Building upon recent progress in
                    pixel-based pretraining, we show, for the first time, that it is possible for such agents to
                    outperform human crowdworkers on the MiniWob++ benchmark of GUI-based instruction following
                    tasks.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72654">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-123"></span>

        <script>
        add_bookmark_click(
            72654,
             1,
            'bookmark-number-123',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72654">Bootstrapping Vision-Language Learning with
                Decoupled Language Pre-training</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yiren Jian · Chongyang Gao · Soroush Vosoughi</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72654">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72654-thumb.png?t=1702107313.492541" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72654" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72654" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72654">
                    Abstract <i id="caret-72654" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72654">
            <div class="abstract-display">
                <p>We present a novel methodology aimed at optimizing the application of frozen large language models
                    (LLMs) for resource-intensive vision-language (VL) pre-training. The current paradigm uses visual
                    features as prompts to guide language models, with a focus on determining the most relevant visual
                    features for corresponding text. Our approach diverges by concentrating on the language component,
                    specifically identifying the optimal prompts to align with visual features. We introduce the
                    Prompt-Transformer (P-Former), a model that predicts these ideal prompts, which is trained
                    exclusively on linguistic data, bypassing the need for image-text pairings. This strategy subtly
                    bifurcates the end-to-end VL training process into an additional, separate stage. Our experiments
                    reveal that our framework significantly enhances the performance of a robust image-to-text baseline
                    (BLIP-2), and effectively narrows the performance gap between models trained with either 4M or 129M
                    image-text pairs. Importantly, our framework is modality-agnostic and flexible in terms of
                    architectural design, as validated by its successful application in a video learning task using
                    varied base modules. The code will be made available at https://github.com/yiren-jian/BLIText.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72968">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-124"></span>

        <script>
        add_bookmark_click(
            72968,
             1,
            'bookmark-number-124',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72968">Characterizing the Optimal $0-1$ Loss for
                Multi-class Classification with a Test-time Attacker</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Sihui Dai · Wenxin Ding · Arjun Nitin Bhagoji · Daniel Cullina · Heather Zheng · Ben
            Zhao · Prateek Mittal
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72968">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72968-thumb.png?t=1702251033.417846" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72968" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72968" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72968">
                    Abstract <i id="caret-72968" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72968">
            <div class="abstract-display">
                Finding classifiers robust to adversarial examples is critical for their safedeployment. Determining the
                robustness of the best possible classifier under agiven threat model for a fixed data distribution and
                comparing it to thatachieved by state-of-the-art training methods is thus an important diagnostictool.
                In this paper, we find achievable information-theoretic lower bounds onrobust loss in the presence of a
                test-time attacker for *multi-classclassifiers on any discrete dataset*. We provide a general framework
                for findingthe optimal $0-1$ loss that revolves around the construction of a conflicthypergraph from the
                data and adversarial constraints. The prohibitive cost ofthis formulation in practice leads us to
                formulate other variants of the attacker-classifiergame that more efficiently determine the range of the
                optimal loss. Ourvaluation shows, for the first time, an analysis of the gap to optimalrobustness for
                classifiers in the multi-class setting on benchmark datasets.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72591">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-125"></span>

        <script>
        add_bookmark_click(
            72591,
             1,
            'bookmark-number-125',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72591">Hierarchical Decomposition of Prompt-Based
                Continual Learning: Rethinking Obscured Sub-optimality</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Liyuan Wang · Jingyi Xie · Xingxing Zhang · Mingyi Huang · Hang Su · Jun Zhu</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72591">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72591-thumb.png?t=1697183191.7925215" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72591" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72591" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72591">
                    Abstract <i id="caret-72591" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72591">
            <div class="abstract-display">
                <p>Prompt-based continual learning is an emerging direction in leveraging pre-trained knowledge for
                    downstream continual learning, and has almost reached the performance pinnacle under supervised
                    pre-training. However, our empirical research reveals that the current strategies fall short of
                    their full potential under the more realistic self-supervised pre-training, which is essential for
                    handling vast quantities of unlabeled data in practice. This is largely due to the difficulty of
                    task-specific knowledge being incorporated into instructed representations via prompt parameters and
                    predicted by uninstructed representations at test time. To overcome the exposed sub-optimality, we
                    conduct a theoretical analysis of the continual learning objective in the context of pre-training,
                    and decompose it into hierarchical components: within-task prediction, task-identity inference, and
                    task-adaptive prediction. Following these empirical and theoretical insights, we propose
                    Hierarchical Decomposition (HiDe-)Prompt, an innovative approach that explicitly optimizes the
                    hierarchical components with an ensemble of task-specific prompts and statistics of both
                    uninstructed and instructed representations, further with the coordination of a contrastive
                    regularization strategy. Our extensive experiments demonstrate the superior performance of
                    HiDe-Prompt and its robustness to pre-training paradigms in continual learning (e.g., up to 15.01%
                    and 9.61% lead on Split CIFAR-100 and Split ImageNet-R, respectively).</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72427">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-126"></span>

        <script>
        add_bookmark_click(
            72427,
             1,
            'bookmark-number-126',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72427">SPAE: Semantic Pyramid AutoEncoder for Multimodal
                Generation with Frozen LLMs</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Lijun Yu · Yong Cheng · Zhiruo Wang · Vivek Kumar · Wolfgang Macherey · Yanping Huang ·
            David Ross · Irfan Essa · Yonatan Bisk · Ming-Hsuan Yang · Kevin Murphy · Alexander Hauptmann · Lu Jiang
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72427">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72427-thumb.png?t=1702165857.277921" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72427" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72427" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72427">
                    Abstract <i id="caret-72427" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72427">
            <div class="abstract-display">
                <p>In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform
                    both understanding and generation tasks involving non-linguistic modalities such as images or
                    videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from
                    the LLM's vocabulary. The resulting tokens capture both the rich semantic meaning and the
                    fine-grained details needed for visual reconstruction, effectively translating the visual content
                    into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal
                    tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT
                    3.5 on a diverse set of image understanding and generation tasks.Our method marks the first
                    successful attempt to enable a frozen LLM to generate image content while surpassing
                    state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71403">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-127"></span>

        <script>
        add_bookmark_click(
            71403,
             1,
            'bookmark-number-127',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71403">DeWave: Discrete Encoding of EEG Waves for EEG to
                Text Translation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yiqun Duan · Charles Chau · Zhen Wang · Yu-Kai Wang · Chin-teng Lin</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71403">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71403-thumb.png?t=1701615526.1498234" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71403" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71403" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71403">
                    Abstract <i id="caret-71403" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71403">
            <div class="abstract-display">
                <p>The translation of brain dynamics into natural language is pivotal for brain-computer interfaces
                    (BCIs), a field that has seen substantial growth in recent years. With the swift advancement of
                    large language models, such as ChatGPT, the need to bridge the gap between the brain and languages
                    becomes increasingly pressing. Current methods, however, require eye-tracking fixations or event
                    markers to segment brain dynamics into word-level features, which can restrict the practical
                    application of these systems. These event markers may not be readily available or could be
                    challenging to acquire during real-time inference, and the sequence of eye fixations may not align
                    with the order of spoken words. To tackle these issues, we introduce a novel framework, DeWave, that
                    integrates discrete encoding sequences into open-vocabulary EEG-to-text translation tasks. DeWave
                    uses a quantized variational encoder to derive discrete codex encoding and align it with pre-trained
                    language models. This discrete codex representation brings forth two advantages: 1) it alleviates
                    the order mismatch between eye fixations and spoken words by introducing text-EEG contrastive
                    alignment training, and 2) it minimizes the interference caused by individual differences in EEG
                    waves through an invariant discrete codex. Our model surpasses the previous baseline (40.1 and 31.7)
                    by 3.06% and …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72674">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-128"></span>

        <script>
        add_bookmark_click(
            72674,
             1,
            'bookmark-number-128',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72674">One-step differentiation of iterative
                algorithms</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jerome Bolte · Edouard Pauwels · Samuel Vaiter</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72674">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72674" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72674" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72674">
                    Abstract <i id="caret-72674" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72674">
            <div class="abstract-display">
                <p>In appropriate frameworks, automatic differentiation is transparent to the user, at the cost of being
                    a significant computational burden when the number of operations is large. For iterative algorithms,
                    implicit differentiation alleviates this issue but requires custom implementation of Jacobian
                    evaluation. In this paper, we study one-step differentiation, also known as Jacobian-free
                    backpropagation, a method as easy as automatic differentiation and as performant as implicit
                    differentiation for fast algorithms (e.g. superlinear optimization methods). We provide a complete
                    theoretical approximation analysis with specific examples (Newton's method, gradient descent) along
                    with its consequences in bilevel optimization. Several numerical examples illustrate the
                    well-foundness of the one-step estimator.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72428">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-129"></span>

        <script>
        add_bookmark_click(
            72428,
             1,
            'bookmark-number-129',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72428">Fine-Grained Human Feedback Gives Better Rewards
                for Language Model Training</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zeqiu Wu · Yushi Hu · Weijia Shi · Nouha Dziri · Alane Suhr · Prithviraj Ammanabrolu ·
            Noah Smith · Mari Ostendorf · Hannaneh Hajishirzi
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72428">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72428" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72428" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72428">
                    Abstract <i id="caret-72428" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72428">
            <div class="abstract-display">
                <p>Language models (LMs) often exhibit undesirable text generation behaviors, including generating
                    false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF)---where human
                    preference judgments on LM outputs are transformed into a learning signal---has recently shown
                    promise in addressing these issues. However, such holistic feedback conveys limited information on
                    long text outputs; it does not indicate which aspects of the outputs influenced user preference;
                    e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback
                    (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We
                    introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions
                    that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a
                    sentence) is generated; and (2) incorporating multiple reward models associated with different
                    feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We
                    conduct experiments on detoxification and long-form question answering to illustrate how learning
                    with this reward function leads to improved performance, supported by both automatic and human
                    evaluation. Additionally, we show that LM behaviors can be customized using different combinations
                    of fine-grained reward models. We release all data, collected human feedback, and codes at
                    https://FineGrainedRLHF.github.io.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72638">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-130"></span>

        <script>
        add_bookmark_click(
            72638,
             1,
            'bookmark-number-130',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72638">Convex and Non-convex Optimization Under
                Generalized Smoothness</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Haochuan Li · Jian Qian · Yi Tian · Alexander Rakhlin · Ali Jadbabaie</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72638">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72638" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72638" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72638">
                    Abstract <i id="caret-72638" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72638">
            <div class="abstract-display">
                <p>Classical analysis of convex and non-convex optimization methods often requires the Lipschitz
                    continuity of the gradient, which limits the analysis to functions bounded by quadratics. Recent
                    work relaxed this requirement to a non-uniform smoothness condition with the Hessian norm bounded by
                    an affine function of the gradient norm, and proved convergence in the non-convex setting via
                    gradient clipping, assuming bounded noise. In this paper, we further generalize this non-uniform
                    smoothness condition and develop a simple, yet powerful analysis technique that bounds the gradients
                    along the trajectory, thereby leading to stronger results for both convex and non-convex
                    optimization problems. In particular, we obtain the classical convergence rates for (stochastic)
                    gradient descent and Nesterov's accelerated gradient method in the convex and/or non-convex setting
                    under this general smoothness condition. The new analysis approach does not require gradient
                    clipping and allows heavy-tailed noise with bounded variance in the stochastic setting.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72743">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-131"></span>

        <script>
        add_bookmark_click(
            72743,
             1,
            'bookmark-number-131',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72743">The Exact Sample Complexity Gain from Invariances
                for Kernel Regression</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Behrooz Tahmasebi · Stefanie Jegelka</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72743">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72743" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72743" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72743">
                    Abstract <i id="caret-72743" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72743">
            <div class="abstract-display">
                <p>In practice, encoding invariances into models improves sample complexity. In this work, we study this
                    phenomenon from a theoretical perspective. In particular, we provide minimax optimal rates for
                    kernel ridge regression on compact manifolds, with a target function that is invariant to a group
                    action on the manifold. Our results hold for any smooth compact Lie group action, even groups of
                    positive dimension. For a finite group, the gain effectively multiplies the number of samples by the
                    group size. For groups of positive dimension, the gain is observed by a reduction in the manifold's
                    dimension, in addition to a factor proportional to the volume of the quotient space. Our proof takes
                    the viewpoint of differential geometry, in contrast to the more common strategy of using invariant
                    polynomials. This new geometric viewpoint on learning with invariances may be of independent
                    interest.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72724">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-132"></span>

        <script>
        add_bookmark_click(
            72724,
             1,
            'bookmark-number-132',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72724">Hierarchical clustering with dot products recovers
                hidden tree structure</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Annie Gray · Alexander Modell · Patrick Rubin-Delanchy · Nick Whiteley</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72724">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72724" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72724" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72724">
                    Abstract <i id="caret-72724" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72724">
            <div class="abstract-display">
                <p>In this paper we offer a new perspective on the well established agglomerative clustering algorithm,
                    focusing on recovery of hierarchical structure. We recommend a simple variant of the standard
                    algorithm, in which clusters are merged by maximum average dot product and not, for example, by
                    minimum distance or within-cluster variance. We demonstrate that the tree output by this algorithm
                    provides a bona fide estimate of generative hierarchical structure in data, under a generic
                    probabilistic graphical model. The key technical innovations are to understand how hierarchical
                    information in this model translates into tree geometry which can be recovered from data, and to
                    characterise the benefits of simultaneously growing sample size and data dimension. We demonstrate
                    superior tree recovery performance with real data over existing approaches such as UPGMA, Ward's
                    method, and HDBSCAN.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72943">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-133"></span>

        <script>
        add_bookmark_click(
            72943,
             1,
            'bookmark-number-133',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72943">Efficient Online Clustering with Moving Costs</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Dimitrios Christou · Stratis Skoulakis · Volkan Cevher</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72943">Wed 13 Dec 12:15 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72943-thumb.png?t=1702081556.2949648" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72943" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72943" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72943">
                    Abstract <i id="caret-72943" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72943">
            <div class="abstract-display">
                In this work we consider an online learning problem, called Online $k$-Clustering with Moving Costs, at
                which a learner maintains a set of $k$ facilities over $T$ rounds so as to minimize the connection cost
                of an adversarially selected sequence of clients. The learner is informed on the positions of the
                clients at each round $t$ only after its facility-selection and can use this information to update its
                decision in the next round. However, updating the facility positions comes with an additional moving
                cost based on the moving distance of the facilities. We present the first $\mathcal{O}(\log n)$-regret
                polynomial-time online learning algorithm guaranteeing that the overall cost (connection $+$ moving) is
                at most $\mathcal{O}(\log n)$ times the time-averaged connection cost of the best fixed solution. Our
                work improves on the recent result of (Fotakis et al., 2021) establishing $\mathcal{O}(k)$-regret
                guarantees only on the connection cost.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72000">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-134"></span>

        <script>
        add_bookmark_click(
            72000,
             1,
            'bookmark-number-134',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72000">Blockwise Parallel Transformers for Large Context
                Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Hao Liu · Pieter Abbeel</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72000">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72000" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72000" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72000">
                    Abstract <i id="caret-72000" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72000">
            <div class="abstract-display">
                <p>Transformers have emerged as the cornerstone of state-of-the-art natural language processing models,
                    showcasing exceptional performance across a wide range of AI applications. However, the memory
                    demands posed by the self-attention mechanism and the large feedforward network in Transformers
                    limit their ability to handle long sequences, thereby creating challenges for tasks involving
                    multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise
                    Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward
                    network fusion to minimize memory costs. By processing longer input sequences while maintaining
                    memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up
                    to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling
                    and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory
                    requirements and improving performance.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72098">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-135"></span>

        <script>
        add_bookmark_click(
            72098,
             1,
            'bookmark-number-135',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72098">Skill-it! A data-driven skills framework for
                understanding and training language models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Mayee Chen · Nicholas Roberts · Kush Bhatia · Jue WANG · Ce Zhang · Frederic Sala ·
            Christopher Ré
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72098">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72098-thumb.png?t=1702269998.3994284" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72098" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72098" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72098">
                    Abstract <i id="caret-72098" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72098">
            <div class="abstract-display">
                <p>The quality of training data impacts the performance of pre-trained large language models (LMs).
                    Given a fixed budget of tokens, we study how to best select data that leads to good downstream model
                    performance across tasks. We develop a new framework based on a simple hypothesis: just as humans
                    acquire interdependent skills in a deliberate order, language models also follow a natural order
                    when learning a set of skills from their training data. If such an order exists, it can be utilized
                    for improved understanding of LMs and for data-efficient training. Using this intuition, our
                    framework formalizes the notion of a skill and of an ordered set of skills in terms of the
                    associated data. First, using both synthetic and real data, we demonstrate that these ordered skill
                    sets exist, and that their existence enables more advanced skills to be learned with less data when
                    we train on their prerequisite skills. Second, using our proposed framework, we introduce an online
                    data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and
                    fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and
                    an individual skill in the latter. On the LEGO synthetic in the …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71822">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-136"></span>

        <script>
        add_bookmark_click(
            71822,
             1,
            'bookmark-number-136',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71822">RePo: Resilient Model-Based Reinforcement Learning
                by Regularizing Posterior Predictability</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Chuning Zhu · Max Simchowitz · Siri Gadipudi · Abhishek Gupta</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71822">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71822-thumb.png?t=1702261735.3226874" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71822" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71822" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71822">
                    Abstract <i id="caret-71822" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71822">
            <div class="abstract-display">
                <p>Visual model-based RL methods typically encode image observations into low-dimensional
                    representations in a manner that does not eliminate redundant information. This leaves them
                    susceptible to spurious variations -- changes in task-irrelevant components such as background
                    distractors or lighting conditions. In this paper, we propose a visual model-based RL method that
                    learns a latent representation resilient to such spurious variations. Our training objective
                    encourages the representation to be maximally predictive of dynamics and reward, while constraining
                    the information flow from the observation to the latent representation. We demonstrate that this
                    objective significantly bolsters the resilience of visual model-based RL methods to visual
                    distractors, allowing them to operate in dynamic environments. We then show that while the learned
                    encoder is able to operate in dynamic environments, it is not invariant under significant
                    distribution shift. To address this, we propose a simple reward-free alignment procedure that
                    enables test time adaptation of the encoder. This allows for quick adaptation to widely differing
                    environments without having to relearn the dynamics and policy. Our effort is a step towards making
                    model-based RL a practical and useful tool for dynamic, diverse domains and we show its
                    effectiveness in simulation tasks with significant spurious variations.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71991">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-137"></span>

        <script>
        add_bookmark_click(
            71991,
             1,
            'bookmark-number-137',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71991">Generalizing Importance Weighting to A Universal
                Solver for Distribution Shift Problems</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Tongtong Fang · Nan Lu · Gang Niu · Masashi Sugiyama</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71991">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71991-thumb.png?t=1701914769.6889563" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71991" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71991" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71991">
                    Abstract <i id="caret-71991" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71991">
            <div class="abstract-display">
                <p>Distribution shift (DS) may have two levels: the distribution itself changes, and the support (i.e.,
                    the set where the probability density is non-zero) also changes. When considering the support change
                    between the training and test distributions, there can be four cases: (i) they exactly match; (ii)
                    the training support is wider (and thus covers the test support); (iii) the test support is wider;
                    (iv) they partially overlap. Existing methods are good at cases (i) and (ii), while cases (iii) and
                    (iv) are more common nowadays but still under-explored. In this paper, we generalize importance
                    weighting (IW), a golden solver for cases (i) and (ii), to a universal solver for all cases.
                    Specifically, we first investigate why IW might fail in cases (iii) and (iv); based on the findings,
                    we propose generalized IW (GIW) that could handle cases (iii) and (iv) and would reduce to IW in
                    cases (i) and (ii). In GIW, the test support is split into an in-training (IT) part and an
                    out-of-training (OOT) part, and the expected risk is decomposed into a weighted classification term
                    over the IT part and a standard classification term over the OOT part, which guarantees the risk
                    consistency of GIW. Then, the …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72277">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-138"></span>

        <script>
        add_bookmark_click(
            72277,
             1,
            'bookmark-number-138',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72277">A Privacy-Friendly Approach to Data Valuation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jiachen (Tianhao) Wang · Yuqing Zhu · Yu-Xiang Wang · Ruoxi Jia · Prateek Mittal</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72277">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72277-thumb.png?t=1701466693.9847958" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72277" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72277" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72277">
                    Abstract <i id="caret-72277" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72277">
            <div class="abstract-display">
                <p>Data valuation, a growing field that aims at quantifying the usefulness of individual data sources
                    for training machine learning (ML) models, faces notable yet often overlooked privacy challenges.
                    This paper studies these challenges with a focus on KNN-Shapley, one of the most practical data
                    valuation methods nowadays. We first emphasize the inherent privacy risks of KNN-Shapley, and
                    demonstrate the significant technical challenges in adapting KNN-Shapley to accommodate differential
                    privacy (DP). To overcome these challenges, we introduce TKNN-Shapley, a refined variant of
                    KNN-Shapley that is privacy-friendly, allowing for straightforward modifications to incorporate DP
                    guarantee (DP-TKNN-Shapley). We show that DP-TKNN-Shapley has several advantages and offers a
                    superior privacy-utility tradeoff compared to naively privatized KNN-Shapley. Moreover, even
                    non-private TKNN-Shapley matches KNN-Shapley's performance in discerning data quality. Overall, our
                    findings suggest that TKNN-Shapley is a promising alternative to KNN-Shapley, particularly for
                    real-world applications involving sensitive data.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71888">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-139"></span>

        <script>
        add_bookmark_click(
            71888,
             1,
            'bookmark-number-139',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71888">Mitigating the Popularity Bias of Graph
                Collaborative Filtering: A Dimensional Collapse Perspective</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yifei Zhang · Hao Zhu · yankai Chen · Zixing Song · Piotr Koniusz · Irwin King</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71888">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71888" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71888" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71888">
                    Abstract <i id="caret-71888" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71888">
            <div class="abstract-display">
                <p>Graph-based Collaborative Filtering (GCF) is widely used in personalized recommendation systems.
                    However, GCF suffers from a fundamental problem where features tend to occupy the embedding space
                    inefficiently (by spanning only a low-dimensional subspace). Such an effect is characterized in GCF
                    by the embedding space being dominated by a few of popular items with the user embeddings highly
                    concentrated around them. This enhances the so-called Matthew effect of the popularity bias where
                    popular items are highly recommend whereas remaining items are ignored. In this paper, we analyze
                    the above effect in GCF and reveal that the simplified graph convolution operation (typically used
                    in GCF) shrinks the singular space of the feature matrix. As typical approaches (i.e., optimizing
                    the uniformity term) fail to prevent the embedding space degradation, we propose a
                    decorrelation-enhanced GCF objective that promotes feature diversity by leveraging the so-called
                    principle of redundancy reduction in embeddings. However, unlike conventional methods that use the
                    Euclidean geometry to relax hard constraints for decorrelation, we exploit non-Euclidean geometry.
                    Such a choice helps maintain the range space of the matrix and obtain small condition number, which
                    prevents the embedding space degradation. Our method outperforms contrastive-based GCF models on
                    several benchmark datasets and improves the …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72299">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-140"></span>

        <script>
        add_bookmark_click(
            72299,
             1,
            'bookmark-number-140',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72299">Timewarp: Transferable Acceleration of Molecular
                Dynamics by Learning Time-Coarsened Dynamics</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Leon Klein · Andrew Foong · Tor Fjelde · Bruno Mlodozeniec · Marc Brockschmidt ·
            Sebastian Nowozin · Frank Noe · Ryota Tomioka
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72299">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72299-thumb.png?t=1700601942.0007596" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72299" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72299" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72299">
                    Abstract <i id="caret-72299" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72299">
            <div class="abstract-display">
                *Molecular dynamics* (MD) simulation is a widely used technique to simulate molecular systems, most
                commonly at the all-atom resolution where equations of motion are integrated with timesteps on the order
                of femtoseconds ($1\textrm{fs}=10^{-15}\textrm{s}$). MD is often used to compute equilibrium properties,
                which requires sampling from an equilibrium distribution such as the Boltzmann distribution. However,
                many important processes, such as binding and folding, occur over timescales of milliseconds or beyond,
                and cannot be efficiently sampled with conventional MD.Furthermore, new MD simulations need to be
                performed for each molecular system studied.We present *Timewarp*, an enhanced sampling method which
                uses a normalising flow as a proposal distribution in a Markov chain Monte Carlo method targeting the
                Boltzmann distribution. The flow is trained offline on MD trajectories and learns to make large steps in
                time, simulating the molecular dynamics of $10^{5} - 10^{6} \textrm{fs}$.Crucially, Timewarp is
                *transferable* between molecular systems: once trained, we show that it generalises to unseen small
                peptides (2-4 amino acids) at all-atom resolution, exploring their metastable states and providing
                wall-clock acceleration of sampling compared to standard MD.Our method constitutes an important step
                towards general, transferable algorithms for accelerating MD.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71875">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-141"></span>

        <script>
        add_bookmark_click(
            71875,
             1,
            'bookmark-number-141',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71875">Alternation makes the adversary weaker in
                two-player games</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Volkan Cevher · Ashok Cutkosky · Ali Kavis · Georgios Piliouras · Stratis Skoulakis ·
            Luca Viano
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71875">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71875-thumb.png?t=1702237614.4734826" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71875" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71875" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71875">
                    Abstract <i id="caret-71875" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71875">
            <div class="abstract-display">
                Motivated by alternating game-play in two-player games, we study an altenating variant of the
                \textit{Online Linear Optimization} (OLO). In alternating OLO, a \textit{learner} at each round $t \in
                [n]$ selects a vector $x^t$ and then an \textit{adversary} selects a cost-vector $c^t \in [-1,1]^n$. The
                learner then experiences cost $(c^t + c^{t-1})^\top x^t$ instead of $(c^t)^\top x^t$ as in standard OLO.
                We establish that under this small twist, the $\Omega(\sqrt{T})$ lower bound on the regret is no longer
                valid. More precisely, we present two online learning algorithms for alternating OLO that respectively
                admit $\mathcal{O}((\log n)^{4/3} T^{1/3})$ regret for the $n$-dimensional simplex and $\mathcal{O}(\rho
                \log T)$ regret for the ball of radius $\rho&gt;0$. Our results imply that in alternating game-play, an
                agent can always guarantee $\mathcal{\tilde{O}}((\log n)^{4/3} T^{1/3})$ regardless the strategies of
                the other agent while the regret bound improves to $\mathcal{O}(\log T)$ in case the agent admits only
                two actions.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72038">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-142"></span>

        <script>
        add_bookmark_click(
            72038,
             1,
            'bookmark-number-142',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72038">CODA: Generalizing to Open and Unseen Domains with
                Compaction and Disambiguation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Chaoqi Chen · Luyao Tang · Yue Huang · Xiaoguang Han · Yizhou Yu</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72038">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72038-thumb.png?t=1702236260.6982582" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72038" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72038" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72038">
                    Abstract <i id="caret-72038" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72038">
            <div class="abstract-display">
                <p>The generalization capability of machine learning systems degenerates notably when the test
                    distribution drifts from the training distribution. Recently, Domain Generalization (DG) has been
                    gaining momentum in enabling machine learning models to generalize to unseen domains. However, most
                    DG methods assume that training and test data share an identical label space, ignoring the potential
                    unseen categories in many real-world applications. In this paper, we delve into a more general but
                    difficult problem termed Open Test-Time DG (OTDG), where both domain shift and open class may occur
                    on the unseen test data. We propose Compaction and Disambiguation (CODA), a novel two-stage
                    framework for learning compact representations and adapting to open classes in the wild. To
                    meaningfully regularize the model's decision boundary, CODA introduces virtual unknown classes and
                    optimizes a new training objective to insert unknowns into the latent space by compacting the
                    embedding space of source known classes. To adapt target samples to the source model, we then
                    disambiguate the decision boundaries between known and unknown classes with a test-time training
                    objective, mitigating the adaptivity gap and catastrophic forgetting challenges. Experiments reveal
                    that CODA can significantly outperform the previous best method on standard DG datasets and
                    harmonize the classification accuracy between …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72237">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-143"></span>

        <script>
        add_bookmark_click(
            72237,
             1,
            'bookmark-number-143',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72237">Balancing memorization and generalization in RNNs
                for high performance brain-machine Interfaces</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Joseph Costello · Hisham Temmar · Luis Cubillos · Matthew Mender · Dylan Wallace · Matt
            Willsey · Parag Patil · Cynthia Chestek
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72237">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72237" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72237" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72237">
                    Abstract <i id="caret-72237" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72237">
            <div class="abstract-display">
                <p>Brain-machine interfaces (BMIs) can restore motor function to people with paralysis but are currently
                    limited by the accuracy of real-time decoding algorithms. Recurrent neural networks (RNNs) using
                    modern training techniques have shown promise in accurately predicting movements from neural signals
                    but have yet to be rigorously evaluated against other decoding algorithms in a closed-loop setting.
                    Here we compared RNNs to other neural network architectures in real-time, continuous decoding of
                    finger movements using intracortical signals from nonhuman primates. Across one and two finger
                    online tasks, LSTMs (a type of RNN) outperformed convolutional and transformer-based neural
                    networks, averaging 18% higher throughput than the convolution network. On simplified tasks with a
                    reduced movement set, RNN decoders were allowed to memorize movement patterns and matched
                    able-bodied control. Performance gradually dropped as the number of distinct movements increased but
                    did not go below fully continuous decoder performance. Finally, in a two-finger task where one
                    degree-of-freedom had poor input signals, we recovered functional control using RNNs trained to act
                    both like a movement classifier and continuous decoder. Our results suggest that RNNs can enable
                    functional real-time BMI control by learning and generating accurate movement patterns.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72294">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-144"></span>

        <script>
        add_bookmark_click(
            72294,
             1,
            'bookmark-number-144',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72294">Conditional Mutual Information for Disentangled
                Representations in Reinforcement Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Mhairi Dunion · Trevor McInroe · Kevin Sebastian Luck · Josiah Hanna · Stefano
            Albrecht
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72294">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72294-thumb.png?t=1698753659.1976643" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72294" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72294" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72294">
                    Abstract <i id="caret-72294" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72294">
            <div class="abstract-display">
                <p>Reinforcement Learning (RL) environments can produce training data with spurious correlations between
                    features due to the amount of training data or its limited feature coverage. This can lead to RL
                    agents encoding these misleading correlations in their latent representation, preventing the agent
                    from generalising if the correlation changes within the environment or when deployed in the real
                    world. Disentangled representations can improve robustness, but existing disentanglement techniques
                    that minimise mutual information between features require independent features, thus they cannot
                    disentangle correlated features. We propose an auxiliary task for RL algorithms that learns a
                    disentangled representation of high-dimensional observations with correlated features by minimising
                    the conditional mutual information between features in the representation. We demonstrate
                    experimentally, using continuous control tasks, that our approach improves generalisation under
                    correlation shifts, as well as improving the training performance of RL algorithms in the presence
                    of correlated features.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71994">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-145"></span>

        <script>
        add_bookmark_click(
            71994,
             1,
            'bookmark-number-145',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71994">Online Label Shift: Optimal Dynamic Regret meets
                Practical Algorithms</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Dheeraj Baby · Saurabh Garg · Tzu-Ching Yen · Sivaraman Balakrishnan · Zachary Lipton ·
            Yu-Xiang Wang
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71994">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71994" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71994" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71994">
                    Abstract <i id="caret-71994" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71994">
            <div class="abstract-display">
                This paper focuses on supervised and unsupervised online label shift,where the class marginals $Q(y)$
                variesbut the class-conditionals $Q(x|y)$ remain invariant. In the unsupervised setting, our goal is to
                adapt a learner, trained on some offline labeled data, to changing label distributions given unlabeled
                online data. In the supervised setting, we must both learn a classifier and adapt to the dynamically
                evolving class marginals given only labeled online data. We develop novel algorithms that reduce the
                adaptation problem to online regression and guarantee optimal dynamic regret without any prior knowledge
                of the extent of drift in the label distribution. Our solution is based on bootstrapping the estimates
                of *online regression oracles* that track the drifting proportions. Experiments across numerous
                simulated and real-world online label shift scenarios demonstrate the superior performance of our
                proposed approaches, often achieving 1-3% improvement in accuracy while being sample and computationally
                efficient. Code is publicly available at https://github.com/Anon-djiwh/OnlineLabelShift
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72380">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-146"></span>

        <script>
        add_bookmark_click(
            72380,
             1,
            'bookmark-number-146',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72380">DreamSim: Learning New Dimensions of Human Visual
                Similarity using Synthetic Data</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Stephanie Fu · Netanel Tamir · Shobhita Sundaram · Lucy Chai · Richard Zhang · Tali
            Dekel · Phillip Isola
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72380">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72380-thumb.png?t=1702204062.5481846" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72380" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72380" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72380">
                    Abstract <i id="caret-72380" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72380">
            <div class="abstract-display">
                <p>Current perceptual similarity metrics operate at the level of pixels and patches. These metrics
                    compare images in terms of their low-level colors and textures, but fail to capture mid-level
                    similarities and differences in image layout, object pose, and semantic content. In this paper, we
                    develop a perceptual metric that assesses images holistically. Our first step is to collect a new
                    dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to
                    this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we
                    use recent text-to-image models to create synthetic pairs that are perturbed along various
                    dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we
                    introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our
                    metric is affected by different visual attributes, and find that it focuses heavily on foreground
                    objects and semantic content while also being sensitive to color and layout. Notably, despite being
                    trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval
                    and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent
                    large vision models on …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72060">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-147"></span>

        <script>
        add_bookmark_click(
            72060,
             1,
            'bookmark-number-147',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72060">MMD-Fuse: Learning and Combining Kernels for
                Two-Sample Testing Without Data Splitting</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Felix Biggs · Antonin Schrab · Arthur Gretton</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72060">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72060-thumb.png?t=1702227915.8766148" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72060" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72060" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72060">
                    Abstract <i id="caret-72060" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72060">
            <div class="abstract-display">
                <p>We propose novel statistics which maximise the power of a two-sample test based on the Maximum Mean
                    Discrepancy (MMD), byadapting over the set of kernels used in defining it.For finite sets, this
                    reduces to combining (normalised) MMD values under each of these kernels via a weighted soft
                    maximum.Exponential concentration bounds are proved for our proposed statistics under the null and
                    alternative.We further show how these kernels can be chosen in a data-dependent but
                    permutation-independent way, in a well-calibrated test, avoiding data splitting.This technique
                    applies more broadly to general permutation-based MMD testing, and includes the use of deep kernels
                    with features learnt using unsupervised models such as auto-encoders.We highlight the applicability
                    of our MMD-Fuse tests on both synthetic low-dimensional and real-world high-dimensional data, and
                    compare its performance in terms of power against current state-of-the-art kernel tests.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72391">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-148"></span>

        <script>
        add_bookmark_click(
            72391,
             1,
            'bookmark-number-148',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72391">A One-Size-Fits-All Approach to Improving
                Randomness in Paper Assignment</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yixuan Xu · Steven Jecmen · Zimeng Song · Fei Fang</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72391">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72391-thumb.png?t=1700986832.115087" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72391" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72391" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72391">
                    Abstract <i id="caret-72391" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72391">
            <div class="abstract-display">
                <p>The assignment of papers to reviewers is a crucial part of the peer review processes of large
                    publication venues, where organizers (e.g., conference program chairs) rely on algorithms to perform
                    automated paper assignment. As such, a major challenge for the organizers of these processes is to
                    specify paper assignment algorithms that find appropriate assignments with respect to various
                    desiderata. Although the main objective when choosing a good paper assignment is to maximize the
                    expertise of each reviewer for their assigned papers, several other considerations make introducing
                    randomization into the paper assignment desirable: robustness to malicious behavior, the ability to
                    evaluate alternative paper assignments, reviewer diversity, and reviewer anonymity. However, it is
                    unclear in what way one should randomize the paper assignment in order to best satisfy all of these
                    considerations simultaneously. In this work, we present a practical, one-size-fits-all method for
                    randomized paper assignment intended to perform well across different motivations for randomness. We
                    show theoretically and experimentally that our method outperforms currently-deployed methods for
                    randomized paper assignment on several intuitive randomness metrics, demonstrating that the
                    randomized assignments produced by our method are general-purpose.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71710">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-149"></span>

        <script>
        add_bookmark_click(
            71710,
             1,
            'bookmark-number-149',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71710">An Optimal and Scalable Matrix Mechanism for Noisy
                Marginals under Convex Loss Functions</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yingtai Xiao · Guanlin He · Danfeng Zhang · Daniel Kifer</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71710">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71710" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71710" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71710">
                    Abstract <i id="caret-71710" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71710">
            <div class="abstract-display">
                <p>Noisy marginals are a common form of confidentiality-protecting data release and are useful for many
                    downstream tasks such as contingency table analysis, construction of Bayesian networks, and even
                    synthetic data generation. Privacy mechanisms that provide unbiased noisy answers to linear queries
                    (such as marginals) are known as matrix mechanisms.We propose ResidualPlanner, a matrix mechanism
                    for marginals with Gaussian noise that is both optimal and scalable. ResidualPlanner can optimize
                    for many loss functions that can be written as a convex function of marginal variances (prior work
                    was restricted to just one predefined objective function). ResidualPlanner can optimize the accuracy
                    of marginals in large scale settings in seconds, even when the previous state of the art (HDMM) runs
                    out of memory. It even runs on datasets with 100 attributes in a couple of minutes. Furthermore
                    ResidualPlanner can efficiently compute variance/covariance values for each marginal (prior methods
                    quickly run out of memory, even for relatively small datasets).</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71887">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-150"></span>

        <script>
        add_bookmark_click(
            71887,
             1,
            'bookmark-number-150',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71887">State Sequences Prediction via Fourier Transform
                for Representation Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Mingxuan Ye · Yufei Kuang · Jie Wang · Yang Rui · Wengang Zhou · Houqiang Li · Feng Wu
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71887">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71887-thumb.png?t=1701438983.54376" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71887" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71887" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71887">
                    Abstract <i id="caret-71887" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71887">
            <div class="abstract-display">
                <p>While deep reinforcement learning (RL) has been demonstrated effective in solving complex control
                    tasks, sample efficiency remains a key challenge due to the large amounts of data required for
                    remarkable performance. Existing research explores the application of representation learning for
                    data-efficient RL, e.g., learning predictive representations by predicting long-term future states.
                    However, many existing methods do not fully exploit the structural information inherent in
                    sequential state signals, which can potentially improve the quality of long-term decision-making but
                    is difficult to discern in the time domain. To tackle this problem, we propose State Sequences
                    Prediction via Fourier Transform (SPF), a novel method that exploits the frequency domain of state
                    sequences to extract the underlying patterns in time series data for learning expressive
                    representations efficiently. Specifically, we theoretically analyze the existence of structural
                    information in state sequences, which is closely related to policy performance and signal
                    regularity, and then propose to predict the Fourier transform of infinite-step future state
                    sequences to extract such information. One of the appealing features of SPF is that it is simple to
                    implement while not requiring storage of infinite-step future states as prediction targets.
                    Experiments demonstrate that the proposed method outperforms several state-of-the-art algorithms in
                    terms of …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70100">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-151"></span>

        <script>
        add_bookmark_click(
            70100,
             1,
            'bookmark-number-151',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70100">Faster Margin Maximization Rates for Generic
                Optimization Methods</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Guanghui Wang · Zihao Hu · Vidya Muthukumar · Jacob Abernethy</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70100">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70100" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70100" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70100">
                    Abstract <i id="caret-70100" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70100">
            <div class="abstract-display">
                First-order optimization methods tend to inherently favor certain solutions over others when minimizing
                a given training objective with multiple local optima. This phenomenon, known as \emph{implicit bias},
                plays a critical role in understanding the generalization capabilities of optimization algorithms.
                Recent research has revealed that gradient-descent-based methods exhibit an implicit bias for the
                $\ell_2$-maximal margin classifier in the context of separable binary classification. In contrast,
                generic optimization methods, such as mirror descent and steepest descent, have been shown to converge
                to maximal margin classifiers defined by alternative geometries. However, while gradient-descent-based
                algorithms demonstrate fast implicit bias rates, the implicit bias rates of generic optimization methods
                have been relatively slow. To address this limitation, in this paper, we present a series of
                state-of-the-art implicit bias rates for mirror descent and steepest descent algorithms. Our primary
                technique involves transforming a generic optimization algorithm into an online learning dynamic that
                solves a regularized bilinear game, providing a unified framework for analyzing the implicit bias of
                various optimization methods. The accelerated rates are derived leveraging the regret bounds of online
                learning algorithms within this game framework.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72349">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-152"></span>

        <script>
        add_bookmark_click(
            72349,
             1,
            'bookmark-number-152',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72349">Decentralized Randomly Distributed Multi-agent
                Multi-armed Bandit with Heterogeneous Rewards</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Mengfan Xu · Diego Klabjan</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72349">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72349-thumb.png?t=1702220263.2581565" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72349" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72349" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72349">
                    Abstract <i id="caret-72349" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72349">
            <div class="abstract-display">
                We study a decentralized multi-agent multi-armed bandit problem in which multiple clients are connected
                by time dependent random graphs provided by an environment. The reward distributions of each arm vary
                across clients and rewards are generated independently over time by an environment based on
                distributions that include both sub-exponential and sub-gaussian distributions. Each client pulls an arm
                and communicates with neighbors based on the graph provided by the environment. The goal is to minimize
                the overall regret of the entire system through collaborations. To this end, we introduce a novel
                algorithmic framework, which first provides robust simulation methods for generating random graphs using
                rapidly mixing markov chains or the random graph model, and then combines an averaging-based consensus
                approach with a newly proposed weighting technique and the upper confidence bound to deliver a UCB-type
                solution. Our algorithms account for the randomness in the graphs, removing the conventional doubly
                stochasticity assumption, and only require the knowledge of the number of clients at initialization. We
                derive optimal instance-dependent regret upper bounds of order $\log{T}$ in both sub-gaussian and
                sub-exponential environments, and a nearly optimal instance-free regret upper bound of order
                $\sqrt{T}\log T$ up to a $\log T$ factor. Importantly, our regret bounds …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71729">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-153"></span>

        <script>
        add_bookmark_click(
            71729,
             1,
            'bookmark-number-153',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71729">Private estimation algorithms for stochastic block
                models and mixture models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Hongjie Chen · Vincent Cohen-Addad · Tommaso d’Orsi · Alessandro Epasto · Jacob Imola ·
            David Steurer · Stefan Tiegel
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71729">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71729-thumb.png?t=1701856551.4718661" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71729" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71729" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71729">
                    Abstract <i id="caret-71729" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71729">
            <div class="abstract-display">
                We introduce general tools for designing efficient private estimation algorithms, in the
                high-dimensional settings, whose statistical guarantees almost match those of the best known non-private
                algorithms.To illustrate our techniques, we consider two problems: recovery of stochastic block models
                and learning mixtures of spherical Gaussians.For the former, we present the first efficient $(\epsilon,
                \delta)$-differentially private algorithm for both weak recovery and exact recovery. Previously known
                algorithms achieving comparable guarantees required quasi-polynomial time. For the latter, we design an
                $(\epsilon, \delta)$-differentially private algorithm that recovers the centers of the $k$-mixture when
                the minimum separation is at least $O(k^{1/t}\sqrt{t})$. For all choices of $t$, this algorithm requires
                sample complexity $n\geq k^{O(1)}d^{O(t)}$ and time complexity $(nd)^{O(t)}$. Prior work required either
                an additional additive $\Omega(\sqrt{\log n})$ term in the minimum separation or an explicit upper bound
                on the Euclidean norm of the centers.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71907">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-154"></span>

        <script>
        add_bookmark_click(
            71907,
             1,
            'bookmark-number-154',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71907">Restless Bandits with Average Reward: Breaking the
                Uniform Global Attractor Assumption</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yige Hong · Qiaomin Xie · Yudong Chen · Weina Wang</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71907">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71907-thumb.png?t=1702171704.0718977" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71907" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71907" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71907">
                    Abstract <i id="caret-71907" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71907">
            <div class="abstract-display">
                We study the infinite-horizon Restless Bandit problem with the average reward criterion, under both
                discrete-time and continuous-time settings.A fundamental goal is to design computationally efficient
                policies that achieve a diminishing optimality gap as the number of arms, $N$, grows large. Existing
                results on asymptotic optimality all rely on the uniform global attractor property (UGAP), a complex and
                challenging-to-verify assumption. In this paper, we propose a general, simulation-based framework,
                Follow-the-Virtual-Advice, that converts any single-armed policy into a policy for the original
                $N$-armed problem. This is done by simulating the single-armed policy on each arm and carefully steering
                the real state towards the simulated state. Our framework can be instantiated to produce a policy with
                an $O(1/\sqrt{N})$ optimality gap. In the discrete-time setting, our result holds under a simpler
                synchronization assumption, which covers some problem instances that violate UGAP. More notably, in the
                continuous-time setting, we do not require \emph{any} additional assumptions beyond the standard
                unichain condition. In both settings, our work is the first asymptotic optimality result that does not
                require UGAP.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71961">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-155"></span>

        <script>
        add_bookmark_click(
            71961,
             1,
            'bookmark-number-155',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71961">Sample Efficient Reinforcement Learning in Mixed
                Systems through Augmented Samples and Its Applications to Queueing Networks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Honghao Wei · Xin Liu · Weina Wang · Lei Ying</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71961">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71961-thumb.png?t=1699838715.626634" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71961" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71961" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71961">
                    Abstract <i id="caret-71961" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71961">
            <div class="abstract-display">
                This paper considers a class of reinforcement learning problems, which involve systems with two types of
                states: stochastic and pseudo-stochastic. In such systems, stochastic states follow a stochastic
                transition kernel while the transitions of pseudo-stochastic states are deterministic {\em given} the
                stochastic states/transitions. We refer to such systems as mixed systems, which are widely used in
                various applications, including Manufacturing systems, communication networks, and queueing networks. We
                propose a sample-efficient RL method that accelerates learning by generating augmented data samples. The
                proposed algorithm is data-driven (model-free), but it learns the policy from data samples from both
                real and augmented samples. This method significantly improves learning by reducing the sample
                complexity such that the dataset only needs to have sufficient coverage of the stochastic states. We
                analyze the sample complexity of the proposed method under Fitted Q Iteration (FQI) and demonstrate that
                the optimality gap decreases as $O\left(\sqrt{\frac{1}{n}}+\sqrt{\frac{1}{m}}\right),$ where $n$
                represents the number of real samples, and $m$ is the number of augmented samples per real sample. It is
                important to note that without augmented samples, the optimality gap is $O(1)$ due to the insufficient
                data coverage of the pseudo-stochastic states. Our experimental results on multiple queueing network
                applications confirm that …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71899">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-156"></span>

        <script>
        add_bookmark_click(
            71899,
             1,
            'bookmark-number-156',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71899">Protein Design with Guided Discrete Diffusion</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Nate Gruver · Samuel Stanton · Nathan Frey · Tim G. J. Rudner · Isidro Hotzel · Julien
            Lafrance-Vanasse · Arvind Rajpal · Kyunghyun Cho · Andrew Wilson
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71899">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71899" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71899" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71899">
                    Abstract <i id="caret-71899" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71899">
            <div class="abstract-display">
                <p>A popular approach to protein design is to combine a generative model with a discriminative model for
                    conditional sampling. The generative model samples plausible sequences while the discriminative
                    model guides a search for sequences with high fitness. Given its broad success in conditional
                    sampling, classifier-guided diffusion modeling is a promising foundation for protein design, leading
                    many to develop guided diffusion models for structure with inverse folding to recover sequences. In
                    this work, we propose diffusioN Optimized Sampling (NOS), a guidance method for discrete diffusion
                    models that follows gradients in the hidden states of the denoising network. NOS makes it possible
                    to perform design directly in sequence space, circumventing significant limitations of
                    structure-based methods, including scarce data and challenging inverse design. Moreover, we use NOS
                    to generalize LaMBO, a Bayesian optimization procedure for sequence design that facilitates multiple
                    objectives and edit-based constraints. The resulting method, LaMBO-2, enables discrete diffusions
                    and stronger performance with limited edits through a novel application of saliency maps. We apply
                    LaMBO-2 to a real-world protein design task, optimizing antibodies for higher expression yield and
                    binding affinity to several therapeutic targets under locality and developability constraints,
                    attaining a 99\% expression rate and 40\% binding rate in exploratory in …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72178">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-157"></span>

        <script>
        add_bookmark_click(
            72178,
             1,
            'bookmark-number-157',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72178">Tree-Based Diffusion Schrödinger Bridge with
                Applications to Wasserstein Barycenters</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Maxence Noble · Valentin De Bortoli · Arnaud Doucet · Alain Durmus</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72178">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72178-thumb.png?t=1701781851.4598355" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72178" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72178" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72178">
                    Abstract <i id="caret-72178" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72178">
            <div class="abstract-display">
                <p>Multi-marginal Optimal Transport (mOT), a generalization of OT, aims at minimizing the integral of a
                    cost function with respect to a distribution with some prescribed marginals. In this paper, we
                    consider an entropic version of mOT with a tree-structured quadratic cost, i.e., a function that can
                    be written as a sum of pairwise cost functions between the nodes of a tree. To address this problem,
                    we develop Tree-based Diffusion Schr\"odinger Bridge (TreeDSB), an extension of the Diffusion
                    Schr\"odinger Bridge (DSB) algorithm. TreeDSB corresponds to a dynamic and continuous state-space
                    counterpart of the multimarginal Sinkhorn algorithm. A notable use case of our methodology is to
                    compute Wasserstein barycenters which can be recast as the solution of a mOT problem on a
                    star-shaped tree. We demonstrate that our methodology can be applied in high-dimensional settings
                    such as image interpolation and Bayesian fusion.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72043">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-158"></span>

        <script>
        add_bookmark_click(
            72043,
             1,
            'bookmark-number-158',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72043">Attentive Transfer Entropy to Exploit Transient
                Emergence of Coupling Effect</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Xiaolei Ru · XINYA ZHANG · Zijia Liu · Jack Murdoch Moore · Gang Yan</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72043">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72043-thumb.png?t=1702054400.9628007" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72043" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72043" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72043">
                    Abstract <i id="caret-72043" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72043">
            <div class="abstract-display">
                <p>We consider the problem of reconstructing coupled networks (e.g., biological neural networks)
                    connecting large numbers of variables (e.g.,nerve cells), of which state evolution is governed by
                    dissipative dynamics consisting of strong self-drive (dominants the evolution) and weak
                    coupling-drive. The core difficulty is sparseness of coupling effect that emerges (the coupling
                    force is significant) only momentarily and otherwise remains quiescent in time series (e.g.,
                    neuronal activity sequence). Here we learn the idea from attention mechanism to guide the classifier
                    to make inference focusing on the critical regions of time series data where coupling effect may
                    manifest. Specifically, attention coefficients are assigned autonomously by artificial neural
                    networks trained to maximise the Attentive Transfer Entropy (ATEn), which is a novel generalization
                    of the iconic transfer entropy metric. Our results show that, without any prior knowledge of
                    dynamics, ATEn explicitly identifies areas where the strength of coupling-drive is distinctly
                    greater than zero. This innovation substantially improves reconstruction performance for both
                    synthetic and real directed coupling networks using data generated by neuronal models widely used in
                    neuroscience.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72132">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-159"></span>

        <script>
        add_bookmark_click(
            72132,
             1,
            'bookmark-number-159',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72132">Leveraging sparse and shared feature activations
                for disentangled representation learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Marco Fumero · Florian Wenzel · Luca Zancato · Alessandro Achille · Emanuele Rodolà ·
            Stefano Soatto · Bernhard Schölkopf · Francesco Locatello
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72132">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72132" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72132" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72132">
                    Abstract <i id="caret-72132" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72132">
            <div class="abstract-display">
                <p>Recovering the latent factors of variation of high dimensional data has so far focused on simple
                    synthetic settings. Mostly building on unsupervised and weakly-supervised objectives, prior work
                    missed out on the positive implications for representation learning on real world data. In this
                    work, we propose to leverage knowledge extracted from a diversified set of supervised tasks to learn
                    a common disentangled representation. Assuming each supervised task only depends on an unknown
                    subset of the factors of variation, we disentangle the feature space of a supervised multi-task
                    model, with features activating sparsely across different tasks and information being shared as
                    appropriate. Importantly, we never directly observe the factors of variations but establish that
                    access to multiple tasks is sufficient for identifiability under sufficiency and minimality
                    assumptions.We validate our approach on six real world distribution shift benchmarks, and different
                    data modalities (images, text), demonstrating how disentangled representations can be transferred to
                    real settings.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72390">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-160"></span>

        <script>
        add_bookmark_click(
            72390,
             1,
            'bookmark-number-160',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72390">MGDD: A Meta Generator for Fast Dataset
                Distillation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Songhua Liu · Xinchao Wang</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72390">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72390-thumb.png?t=1701960274.432292" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72390" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72390" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72390">
                    Abstract <i id="caret-72390" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72390">
            <div class="abstract-display">
                Existing dataset distillation (DD) techniques typically rely on iterative strategies to synthesize
                condensed datasets, where datasets before and after distillation are forward and backward through neural
                networks a massive number of times. Despite the promising results achieved, the time efficiency of prior
                approaches is still far from satisfactory. Moreover, when different sizes of synthetic datasets are
                required, they have to repeat the iterative training procedures, which is highly cumbersome and lacks
                flexibility. In this paper, different from the time-consuming forward-backward passes, we introduce a
                generative fashion for dataset distillation with significantly improved efficiency. Specifically,
                synthetic samples are produced by a generator network conditioned on the initialization of DD, while
                synthetic labels are obtained by solving a least-squares problem in a feature space. Our theoretical
                analysis reveals that the errors of synthetic datasets solved in the original space and then processed
                by any conditional generators are upper-bounded. To find a satisfactory generator efficiently, we
                propose a meta-learning algorithm, where a meta generator is trained on a large dataset so that only a
                few steps are required to adapt to a target dataset. The meta generator is termed as MGDD in our
                approach. Once adapted, it can handle arbitrary sizes of synthetic …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71736">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-161"></span>

        <script>
        add_bookmark_click(
            71736,
             1,
            'bookmark-number-161',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71736">A Spectral Algorithm for List-Decodable Covariance
                Estimation in Relative Frobenius Norm</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Ilias Diakonikolas · Daniel Kane · Jasper Lee · Ankit Pensia · Thanasis Pittas</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71736">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71736-thumb.png?t=1702002546.627076" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71736" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71736" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71736">
                    Abstract <i id="caret-71736" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71736">
            <div class="abstract-display">
                We study the problem of list-decodable Gaussian covariance estimation. Given a multiset $T$ of $n$
                points in $\mathbb{R}^d$ such that an unknown $\alpha&lt;1/2$ fraction of points in $T$ are i.i.d.
                samples from an unknown Gaussian $\mathcal{N}(\mu, \Sigma)$, the goal is to output a list of
                $O(1/\alpha)$ hypotheses at least one of which is close to $\Sigma$ in relative Frobenius norm. Our main
                result is a $\mathrm{poly}(d,1/\alpha)$ sample and time algorithm for this task that guarantees relative
                Frobenius norm error of $\mathrm{poly}(1/\alpha)$. Importantly, our algorithm relies purely on spectral
                techniques. As a corollary, we obtain an efficient spectral algorithm for robust partial clustering of
                Gaussian mixture models (GMMs) --- a key ingredient in the recent work of [BakDJKKV22] on robustly
                learning arbitrary GMMs. Combined with the other components of [BakDJKKV22], our new method yields the
                first Sum-of-Squares-free algorithm for robustly learning GMMs, resolving an open problem proposed by
                Vempala and Kothari. At the technical level, we develop a novel multi-filtering method for
                list-decodable covariance estimation that may be useful in other settings.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71831">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-162"></span>

        <script>
        add_bookmark_click(
            71831,
             1,
            'bookmark-number-162',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71831">Evaluating the Moral Beliefs Encoded in LLMs</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Nino Scherrer · Claudia Shi · Amir Feder · David Blei</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71831">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71831-thumb.png?t=1702078723.6036072" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71831" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71831" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71831">
                    Abstract <i id="caret-71831" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71831">
            <div class="abstract-display">
                <p>This paper presents a case study on the design, administration, post-processing, and evaluation of
                    surveys on large language models (LLMs). It comprises two components:(1) A statistical method for
                    eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that
                    quantify the probability of an LLM "making a choice", the associated uncertainty, and the
                    consistency of that choice.(2) We apply this method to study what moral beliefs are encoded in
                    different LLMs, especially in ambiguous cases where the right choice is not obvious.We design a
                    large-scale survey comprising 680 high-ambiguity moral scenarios (e.g., "Should I tell a white
                    lie?") and 687 low-ambiguity moral scenarios (e.g., "Should I stop for a pedestrian on the road?").
                    Each scenario includes a description, two possible actions, and auxiliary labels indicating violated
                    rules (e.g., "do not kill"). We administer the survey to 28 open- and closed-source LLMs.We find
                    that (a) in unambiguous scenarios, most models ``choose" actions that align with commonsense. In
                    ambiguous cases, most models express uncertainty.(b) Some models are uncertain about choosing the
                    commonsense action because their responses are sensitive to the question-wording.(c) Some models
                    reflect clear preferences in ambiguous scenarios. Specifically, closed-source models tend to agree
                    with each other.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72058">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-163"></span>

        <script>
        add_bookmark_click(
            72058,
             1,
            'bookmark-number-163',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72058">Randomized Sparse Neural Galerkin Schemes for
                Solving Evolution Equations with Deep Networks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jules Berman · Benjamin Peherstorfer</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72058">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72058-thumb.png?t=1702074743.2103667" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72058" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72058" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72058">
                    Abstract <i id="caret-72058" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72058">
            <div class="abstract-display">
                <p>Training neural networks sequentially in time to approximate solution fields of time-dependent
                    partial differential equations can be beneficial for preserving causality and other physics
                    properties; however, the sequential-in-time training is numerically challenging because training
                    errors quickly accumulate and amplify over time. This work introduces Neural Galerkin schemes that
                    update randomized sparse subsets of network parameters at each time step. The randomization avoids
                    overfitting locally in time and so helps prevent the error from accumulating quickly over the
                    sequential-in-time training, which is motivated by dropout that addresses a similar issue of
                    overfitting due to neuron co-adaptation. The sparsity of the update reduces the computational costs
                    of training without losing expressiveness because many of the network parameters are redundant
                    locally at each time step. In numerical experiments with a wide range of evolution equations, the
                    proposed scheme with randomized sparse updates is up to two orders of magnitude more accurate at a
                    fixed computational budget and up to two orders of magnitude faster at a fixed accuracy than schemes
                    with dense updates.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72158">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-164"></span>

        <script>
        add_bookmark_click(
            72158,
             1,
            'bookmark-number-164',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72158">Alignment with human representations supports
                robust few-shot learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Ilia Sucholutsky · Tom Griffiths</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72158">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72158-thumb.png?t=1701746967.7744782" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72158" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72158" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72158">
                    Abstract <i id="caret-72158" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72158">
            <div class="abstract-display">
                <p>Should we care whether AI systems have representations of the world that are similar to those of
                    humans? We provide an information-theoretic analysis that suggests that there should be a U-shaped
                    relationship between the degree of representational alignment with humans and performance on
                    few-shot learning tasks. We confirm this prediction empirically, finding such a relationship in an
                    analysis of the performance of 491 computer vision models. We also show that highly-aligned models
                    are more robust to both natural adversarial attacks and domain shifts. Our results suggest that
                    human-alignment is often a sufficient, but not necessary, condition for models to make effective use
                    of limited data, be robust, and generalize well.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72145">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-165"></span>

        <script>
        add_bookmark_click(
            72145,
             1,
            'bookmark-number-165',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72145">Future-Dependent Value-Based Off-Policy Evaluation
                in POMDPs</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Masatoshi Uehara · Haruka Kiyohara · Andrew Bennett · Victor Chernozhukov · Nan Jiang ·
            Nathan Kallus · Chengchun Shi · Wen Sun
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72145">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72145" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72145" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72145">
                    Abstract <i id="caret-72145" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72145">
            <div class="abstract-display">
                <p>We study off-policy evaluation (OPE) for partially observable MDPs (POMDPs) with general function
                    approximation. Existing methods such as sequential importance sampling estimators and fitted-Q
                    evaluation suffer from the curse of horizon in POMDPs. To circumvent this problem, we develop a
                    novel model-free OPE method by introducing future-dependent value functions that take future proxies
                    as inputs. Future-dependent value functions play similar roles as classical value functions in
                    fully-observable MDPs. We derive a new off-policy Bellman equation for future-dependent value
                    functions as conditional moment equations that use history proxies as instrumental variables. We
                    further propose a minimax learning method to learn future-dependent value functions using the new
                    Bellman equation. We obtain the PAC result, which implies our OPE estimator is close to the true
                    policy value as long as futures and histories contain sufficient information about latent states,
                    and the Bellman completeness. Our code is available at
                    https://github.com/aiueola/neurips2023-future-dependent-ope</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72284">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-166"></span>

        <script>
        add_bookmark_click(
            72284,
             1,
            'bookmark-number-166',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72284">Kronecker-Factored Approximate Curvature for Modern
                Neural Network Architectures</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Runa Eschenhagen · Alexander Immer · Richard Turner · Frank Schneider · Philipp Hennig
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72284">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72284" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72284" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72284">
                    Abstract <i id="caret-72284" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72284">
            <div class="abstract-display">
                The core components of many modern neural network architectures, such as transformers, convolutional, or
                graph neural networks, can be expressed as linear layers with *weight-sharing*. Kronecker-Factored
                Approximate Curvature (K-FAC), a second-order optimisation method, has shown promise to speed up neural
                network training and thereby reduce computational costs. However, there is currently no framework to
                apply it to generic architectures, specifically ones with linear weight-sharing layers. In this work, we
                identify two different settings of linear weight-sharing layers which motivate two flavours of K-FAC --
                *expand* and *reduce*. We show that they are exact for deep linear networks with weight-sharing in their
                respective setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we leverage to
                speed up automatic hyperparameter selection via optimising the marginal likelihood for a Wide ResNet.
                Finally, we observe little difference between these two K-FAC variations when using them to train both a
                graph neural network and a vision transformer. However, both variations are able to reach a fixed
                validation metric target in $50$-$75$\% of the number of steps of a first-order reference run, which
                translates into a comparable improvement in wall-clock time. This highlights the potential of applying
                K-FAC to modern neural network architectures.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72130">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-167"></span>

        <script>
        add_bookmark_click(
            72130,
             1,
            'bookmark-number-167',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72130">QuACK: Accelerating Gradient-Based Quantum
                Optimization with Koopman Operator Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Di Luo · Jiayu Shen · Rumen Dangovski · Marin Soljacic</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72130">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72130" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72130" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72130">
                    Abstract <i id="caret-72130" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72130">
            <div class="abstract-display">
                <p>Quantum optimization, a key application of quantum computing, has traditionally been stymied by the
                    linearly increasing complexity of gradient calculations with an increasing number of parameters.
                    This work bridges the gap between Koopman operator theory, which has found utility in applications
                    because it allows for a linear representation of nonlinear dynamical systems, and natural gradient
                    methods in quantum optimization, leading to a significant acceleration of gradient-based quantum
                    optimization. We present Quantum-circuit Alternating Controlled Koopman learning (QuACK), a novel
                    framework that leverages an alternating algorithm for efficient prediction of gradient dynamics on
                    quantum computers. We demonstrate QuACK's remarkable ability to accelerate gradient-based
                    optimization across a range of applications in quantum optimization and machine learning. In fact,
                    our empirical studies, spanning quantum chemistry, quantum condensed matter, quantum machine
                    learning, and noisy environments, have shown accelerations of more than 200x speedup in the
                    overparameterized regime, 10x speedup in the smooth regime, and 3x speedup in the non-smooth regime.
                    With QuACK, we offer a robust advancement that harnesses the advantage of gradient-based quantum
                    optimization for practical benefits.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72293">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-168"></span>

        <script>
        add_bookmark_click(
            72293,
             1,
            'bookmark-number-168',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72293">Honesty Is the Best Policy: Defining and Mitigating
                AI Deception</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Francis Ward · Francesca Toni · Francesco Belardinelli · Tom Everitt</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72293">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72293" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72293" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72293">
                    Abstract <i id="caret-72293" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72293">
            <div class="abstract-display">
                <p>Deceptive agents are a challenge for the safety, trustworthiness, and cooperation of AI systems. We
                    focus on the problem that agents might deceive in order to achieve their goals (for instance, in our
                    experiments with language models, the goal of being evaluated as truthful).There are a number of
                    existing definitions of deception in the literature on game theory and symbolic AI, but there is no
                    overarching theory of deception for learning agents in games. We introduce a formaldefinition of
                    deception in structural causal games, grounded in the philosophyliterature, and applicable to
                    real-world machine learning systems.Several examples and results illustrate that our formal
                    definition aligns with the philosophical and commonsense meaning of deception.Our main technical
                    result is to provide graphical criteria for deception. We show, experimentally, that these results
                    can be used to mitigate deception in reinforcement learning agents and language models.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72055">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-169"></span>

        <script>
        add_bookmark_click(
            72055,
             1,
            'bookmark-number-169',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72055">DIFUSCO: Graph-based Diffusion Solvers for
                Combinatorial Optimization</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zhiqing Sun · Yiming Yang</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72055">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72055-thumb.png?t=1701988567.2879682" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72055" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72055" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72055">
                    Abstract <i id="caret-72055" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72055">
            <div class="abstract-display">
                <p>Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving
                    various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper
                    broadens the current scope of neural solvers for NPC problems by introducing a new graph-based
                    diffusion framework, namely DIFUSCO. It formulates NPC problems into a discrete {0, 1}-vector space
                    and uses graph-based denoising diffusion models to generate high-quality solutions. Specifically, we
                    explore diffusion models with Gaussian and Bernoulli noise, respectively, and also introduce an
                    effective inference schedule to improve the generation quality. We evaluate our methods on two
                    well-studied combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal
                    Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous
                    state-of-the-art neural solvers, improving the performance gap between ground-truth and neural
                    solvers from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19% to 2.58% on
                    TSP-10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on
                    the challenging SATLIB benchmark. Our code is available at <a
                            href="https://github.com/Edward-Sun/DIFUSCO">this url</a>.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72328">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-170"></span>

        <script>
        add_bookmark_click(
            72328,
             1,
            'bookmark-number-170',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72328">Auditing Fairness by Betting</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Ben Chugg · Santiago Cortes-Gomez · Bryan Wilder · Aaditya Ramdas</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72328">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72328" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72328" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72328">
                    Abstract <i id="caret-72328" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72328">
            <div class="abstract-display">
                <p>We provide practical, efficient, and nonparametric methods for auditing the fairness of deployed
                    classification and regression models. Whereas previous work relies on a fixed-sample size, our
                    methods are sequential and allow for the continuous monitoring of incoming data, making them highly
                    amenable to tracking the fairness of real-world systems. We also allow the data to be collected by a
                    probabilistic policy as opposed to sampled uniformly from the population. This enables auditing to
                    be conducted on data gathered for another purpose. Moreover, this policy may change over time and
                    different policies may be used on different subpopulations. Finally, our methods can handle
                    distribution shift resulting from either changes to the model or changes in the underlying
                    population. Our approach is based on recent progress in anytime-valid inference and game-theoretic
                    statistics---the ``testing by betting'' framework in particular. These connections ensure that our
                    methods are interpretable, fast, and easy to implement. We demonstrate the efficacy of our approach
                    on three benchmark fairness datasets.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72015">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-171"></span>

        <script>
        add_bookmark_click(
            72015,
             1,
            'bookmark-number-171',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72015">SE(3) Equivariant Augmented Coupling Flows</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Laurence Midgley · Vincent Stimper · Javier Antorán · Emile Mathieu · Bernhard Schölkopf
            · José Miguel Hernández-Lobato
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72015">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72015-thumb.png?t=1701268686.0790231" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72015" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72015" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72015">
                    Abstract <i id="caret-72015" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72015">
            <div class="abstract-display">
                <p>Coupling normalizing flows allow for fast sampling and density evaluation, making them the tool of
                    choice for probabilistic modeling of physical systems. However, the standard coupling architecture
                    precludes endowing flows that operate on the Cartesian coordinates of atoms with the SE(3) and
                    permutation invariances of physical systems. This work proposes a coupling flow that preserves SE(3)
                    and permutation equivariance by performing coordinate splits along additional augmented dimensions.
                    At each layer, the flow maps atoms' positions into learned SE(3) invariant bases, where we apply
                    standard flow transformations, such as monotonic rational-quadratic splines, before returning to the
                    original basis. Crucially, our flow preserves fast sampling and density evaluation, and may be used
                    to produce unbiased estimates of expectations with respect to the target distribution via importance
                    sampling. When trained on the DW4, LJ13, and QM9-positional datasets, our flow is competitive with
                    equivariant continuous normalizing flows, while allowing sampling more than an order of magnitude
                    faster. Moreover, to the best of our knowledge, we are the first to learn the full Boltzmann
                    distribution of alanine dipeptide by only modeling the Cartesian positions of its atoms. Lastly, we
                    demonstrate that our flow can be trained to approximately sample from the Boltzmann distribution of
                    the …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71812">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-172"></span>

        <script>
        add_bookmark_click(
            71812,
             1,
            'bookmark-number-172',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71812">Scale Alone Does not Improve Mechanistic
                Interpretability in Vision Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Roland S. Zimmermann · Thomas Klein · Wieland Brendel</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71812">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71812-thumb.png?t=1701687771.2176948" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71812" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71812" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71812">
                    Abstract <i id="caret-71812" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71812">
            <div class="abstract-display">
                <p>In light of the recent widespread adoption of AI systems, understanding the internal information
                    processing of neural networks has become increasingly critical. Most recently, machine vision has
                    seen remarkable progress by scaling neural networks to unprecedented levels in dataset and model
                    size. We here ask whether this extraordinary increase in scale also positively impacts the field of
                    mechanistic interpretability. In other words, has our understanding of the inner workings of scaled
                    neural networks improved as well? We use a psychophysical paradigm to quantify one form of
                    mechanistic interpretability for a diverse suite of nine models and find no scaling effect for
                    interpretability - neither for model nor dataset size. Specifically, none of the investigated
                    state-of-the-art models are easier to interpret than the GoogLeNet model from almost a decade ago.
                    Latest-generation vision models appear even less interpretable than older architectures, hinting at
                    a regression rather than improvement, with modern models sacrificing interpretability for accuracy.
                    These results highlight the need for models explicitly designed to be mechanistically interpretable
                    and the need for more helpful interpretability methods to increase our understanding of networks at
                    an atomic level. We release a dataset containing more than 130'000 human responses from our
                    psychophysical evaluation of 767 …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71939">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-173"></span>

        <script>
        add_bookmark_click(
            71939,
             1,
            'bookmark-number-173',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71939">From Tempered to Benign Overfitting in ReLU Neural
                Networks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Guy Kornowski · Gilad Yehudai · Ohad Shamir</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71939">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71939-thumb.png?t=1701075206.7784219" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71939" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71939" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71939">
                    Abstract <i id="caret-71939" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71939">
            <div class="abstract-display">
                <p>Overparameterized neural networks (NNs) are observed to generalize well even when trained to
                    perfectly fit noisy data. This phenomenon motivated a large body of work on "benign overfitting",
                    where interpolating predictors achieve near-optimal performance. Recently, it was conjectured and
                    empirically observed that the behavior of NNs is often better described as "tempered overfitting",
                    where the performance is non-optimal yet also non-trivial, and degrades as a function of the noise
                    level. However, a theoretical justification of this claim for non-linear NNs has been lacking so
                    far. In this work, we provide several results that aim at bridging these complementing views. We
                    study a simple classification setting with 2-layer ReLU NNs, and prove that under various
                    assumptions, the type of overfitting transitions from tempered in the extreme case of
                    one-dimensional data, to benign in high dimensions. Thus, we show that the input dimension has a
                    crucial role on the overfitting profile in this setting, which we also validate empirically for
                    intermediate dimensions. Overall, our results shed light on the intricate connections between the
                    dimension, sample size, architecture and training algorithm on the one hand, and the type of
                    resulting overfitting on the other hand.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72258">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-174"></span>

        <script>
        add_bookmark_click(
            72258,
             1,
            'bookmark-number-174',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72258">Episodic Multi-Task Learning with Heterogeneous
                Neural Processes</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jiayi Shen · Xiantong Zhen · Qi Wang · Marcel Worring</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72258">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72258-thumb.png?t=1697447403.2631981" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72258" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72258" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72258">
                    Abstract <i id="caret-72258" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72258">
            <div class="abstract-display">
                <p>This paper focuses on the data-insufficiency problem in multi-task learning within an episodic
                    training setup. Specifically, we explore the potential of heterogeneous information across tasks and
                    meta-knowledge among episodes to effectively tackle each task with limited data. Existing
                    meta-learning methods often fail to take advantage of crucial heterogeneous information in a single
                    episode, while multi-task learning models neglect reusing experience from earlier episodes. To
                    address the problem of insufficient data, we develop Heterogeneous Neural Processes (HNPs) for the
                    episodic multi-task setup. Within the framework of hierarchical Bayes, HNPs effectively capitalize
                    on prior experiences as meta-knowledge and capture task-relatedness among heterogeneous tasks,
                    mitigating data-insufficiency. Meanwhile, transformer-structured inference modules are designed to
                    enable efficient inferences toward meta-knowledge and task-relatedness. In this way, HNPs can learn
                    more powerful functional priors for adapting to novel heterogeneous tasks in each meta-test episode.
                    Experimental results show the superior performance of the proposed HNPs over typical baselines, and
                    ablation studies verify the effectiveness of the designed inference modules.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72272">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-175"></span>

        <script>
        add_bookmark_click(
            72272,
             1,
            'bookmark-number-175',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72272">Squared Neural Families: A New Class of Tractable
                Density Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Russell Tsuchida · Cheng Soon Ong · Dino Sejdinovic</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72272">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72272-thumb.png?t=1699840985.8477323" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72272" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72272" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72272">
                    Abstract <i id="caret-72272" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72272">
            <div class="abstract-display">
                <p>Flexible models for probability distributions are an essential ingredient in many machine learning
                    tasks. We develop and investigate a new class of probability distributions, which we call a Squared
                    Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with
                    respect to a base measure. Following the reasoning similar to the well established connections
                    between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit closed
                    form normalising constants in many cases of interest, thereby resulting in flexible yet fully
                    tractable density models. SNEFYs strictly generalise classical exponential families, are closed
                    under conditioning, and have tractable marginal distributions. Their utility is illustrated on a
                    variety of density estimation, conditional density estimation, and density estimation with missing
                    data tasks.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72172">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-176"></span>

        <script>
        add_bookmark_click(
            72172,
             1,
            'bookmark-number-176',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72172">On quantum backpropagation, information reuse, and
                cheating measurement collapse</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Amira Abbas · Robbie King · Hsin-Yuan Huang · William J. Huggins · Ramis Movassagh · Dar
            Gilboa · Jarrod McClean
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72172">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72172" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72172" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72172">
                    Abstract <i id="caret-72172" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72172">
            <div class="abstract-display">
                <p>The success of modern deep learning hinges on the ability to train neural networks at scale. Through
                    clever reuse of intermediate information, backpropagation facilitates training through gradient
                    computation at a total cost roughly proportional to running the function, rather than incurring an
                    additional factor proportional to the number of parameters -- which can now be in the trillions.
                    Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum
                    information as in backpropagation. But recent developments in shadow tomography, which assumes
                    access to multiple copies of a quantum state, have challenged that notion. Here, we investigate
                    whether parameterized quantum models can train as efficiently as classical neural networks. We show
                    that achieving backpropagation scaling is impossible without access to multiple copies of a state.
                    With this added ability, we introduce an algorithm with foundations in shadow tomography that
                    matches backpropagation scaling in quantum resources while reducing classical auxiliary
                    computational costs to open problems in shadow tomography. These results highlight the nuance of
                    reusing quantum information for practical purposes and clarify the unique difficulties in training
                    large quantum models, which could alter the course of quantum machine learning.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72136">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-177"></span>

        <script>
        add_bookmark_click(
            72136,
             1,
            'bookmark-number-177',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72136">Evaluating and Inducing Personality in Pre-trained
                Language Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Guangyuan Jiang · Manjie Xu · Song-Chun Zhu · Wenjuan Han · Chi Zhang · Yixin Zhu</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72136">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72136-thumb.png?t=1701403065.9372535" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72136" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72136" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72136">
                    Abstract <i id="caret-72136" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72136">
            <div class="abstract-display">
                Standardized and quantified evaluation of machine behaviors is a crux of understanding LLMs. In this
                study, we draw inspiration from psychometric studies by leveraging human personality theory as a tool
                for studying machine behaviors. Originating as a philosophical quest for human behaviors, the study of
                personality delves into how individuals differ in thinking, feeling, and behaving. Toward building and
                understanding human-like social machines, we are motivated to ask: Can we assess machine behaviors by
                leveraging human psychometric tests in a **principled** and **quantitative** manner? If so, can we
                induce a specific personality in LLMs? To answer these questions, we introduce the Machine Personality
                Inventory (MPI) tool for studying machine behaviors; MPI follows standardizedpersonality tests, built
                upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By
                systematically evaluating LLMs with MPI, we provide the first piece of evidence demonstrating the
                efficacy of MPI in studying LLMs behaviors. We further devise a Personality Prompting (P$^2$) method to
                induce LLMs with specific personalities in a **controllable** way, capable of producing diverse and
                verifiable behaviors. We hope this work sheds light on future studies by adopting personality as the
                essential indicator for various downstream tasks, and could further motivate research …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73054">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-178"></span>

        <script>
        add_bookmark_click(
            73054,
             1,
            'bookmark-number-178',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73054">Score-based Generative Models with Lévy
                Processes</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">EUN BI YOON · Keehun Park · Sungwoong Kim · Sungbin Lim</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73054">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/73054-thumb.png?t=1697528205.60491" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73054" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73054" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73054">
                    Abstract <i id="caret-73054" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73054">
            <div class="abstract-display">
                Investigating the optimal stochastic process beyond Gaussian for noise injection in a score-based
                generative model remains an open question. Brownian motion is a light-tailed process with continuous
                paths, which leads to a slow convergence rate for the Number of Function Evaluation (NFE). Recent
                studies have shown that diffusion models suffer from mode-collapse issues on imbalanced data.In order to
                overcome the limitations of Brownian motion, we introduce a novel score-based generative model referred
                to as Lévy-Itō Model (LIM). This model utilizes isotropic $\alpha$-stable Lévy processes. We first
                derive an exact reverse-time stochastic differential equation driven by the Lévy process and develop the
                corresponding fractional denoising score matching. The proposed generative model takes advantage of the
                heavy-tailed properties of the Lévy process. Our experimental results show LIM allows for faster and
                more diverse sampling while maintaining high fidelity compared to existing diffusion models across
                various image datasets such as CIFAR10, CelebA, and imbalanced dataset CIFAR10LT. Comparing our results
                to those of DDPM with 3.21 Fréchet Inception Distance (FID) and 0.6437 Recall on the CelebA dataset, we
                achieve 1.58 FID and 0.7006 Recall using the same architecture. LIM shows the best performance in NFE
                500 with $2\times$ faster total wall-clock time than the …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72210">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-179"></span>

        <script>
        add_bookmark_click(
            72210,
             1,
            'bookmark-number-179',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72210">4D Panoptic Scene Graph Generation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jingkang Yang · Jun CEN · WENXUAN PENG · Shuai Liu · Fangzhou Hong · Xiangtai Li ·
            Kaiyang Zhou · Qifeng Chen · Ziwei Liu
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72210">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72210" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72210" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72210">
                    Abstract <i id="caret-72210" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72210">
            <div class="abstract-display">
                <p>We are living in a three-dimensional space while moving forward through a fourth dimension: time. To
                    allow artificial intelligence to develop a comprehensive understanding of such a 4D environment, we
                    introduce <strong>4D Panoptic Scene Graph (PSG-4D)</strong>, a new representation that bridges the
                    raw visual data perceived in a dynamic 4D world and high-level visual understanding. Specifically,
                    PSG-4D abstracts rich 4D sensory data into nodes, which represent entities with precise location and
                    status information, and edges, which capture the temporal relations. To facilitate research in this
                    new area, we build a richly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of
                    1M frames, each of which is labeled with 4D panoptic segmentation masks as well as fine-grained,
                    dynamic scene graphs. To solve PSG-4D, we propose PSG4DFormer, a Transformer-based model that can
                    predict panoptic segmentation masks, track masks along the time axis, and generate the corresponding
                    scene graphs via a relation component. Extensive experiments on the new dataset show that our method
                    can serve as a strong baseline for future research on PSG-4D. In the end, we provide a real-world
                    application example to demonstrate how we can achieve dynamic scene understanding by integrating a
                    large language model into …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72127">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-180"></span>

        <script>
        add_bookmark_click(
            72127,
             1,
            'bookmark-number-180',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72127">EmbodiedGPT: Vision-Language Pre-Training via
                Embodied Chain of Thought</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yao Mu · Qinglong Zhang · Mengkang Hu · Wenhai Wang · Mingyu Ding · Jun Jin · Bin Wang ·
            Jifeng Dai · Yu Qiao · Ping Luo
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72127">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72127-thumb.png?t=1701680815.8085413" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72127" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72127" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72127">
                    Abstract <i id="caret-72127" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72127">
            <div class="abstract-display">
                <p>Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for
                    robots to accomplish long-horizon tasks in physical environments.In this work, we introduce
                    EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents
                    with multi-modal understanding and execution capabilities. To achieve this, we have made the
                    following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset
                    consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality
                    language instructions. Specifically, we generate a sequence of sub-goals with the "Chain of
                    Thoughts" mode for effective embodied planning.(ii) We introduce an efficient training approach to
                    EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the
                    EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features
                    from LLM-generated planning queries to form a closed loop between high-level planning and low-level
                    control.Extensive experiments show the effectiveness of EmbodiedGPT on embodied tasks, including
                    embodied planning, embodied control, visual captioning, and visual question answering.Notably,
                    EmbodiedGPT significantly enhances the success rate of the embodied control task by extracting more
                    effective features. It has achieved a remarkable 1.6 times increase in success rate on the Franka
                    …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72318">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-181"></span>

        <script>
        add_bookmark_click(
            72318,
             1,
            'bookmark-number-181',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72318">SlotDiffusion: Object-Centric Generative Modeling
                with Diffusion Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Ziyi Wu · Jingyu Hu · Wuyue Lu · Igor Gilitschenski · Animesh Garg</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72318">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72318-thumb.png?t=1700857276.9563384" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72318" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72318" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72318">
                    Abstract <i id="caret-72318" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72318">
            <div class="abstract-display">
                <p>Object-centric learning aims to represent visual data with a set of object entities (a.k.a. slots),
                    providing structured representations that enable systematic generalization.Leveraging advanced
                    architectures like Transformers, recent approaches have made significant progress in unsupervised
                    object discovery.In addition, slot-based representations hold great potential for generative
                    modeling, such as controllable image generation and object manipulation in image editing.However,
                    current slot-based methods often produce blurry images and distorted objects, exhibiting poor
                    generative modeling capabilities.In this paper, we focus on improving slot-to-image decoding, a
                    crucial aspect for high-quality visual generation.We introduce SlotDiffusion -- an object-centric
                    Latent Diffusion Model (LDM) designed for both image and video data.Thanks to the powerful modeling
                    capacity of LDMs, SlotDiffusion surpasses previous slot models in unsupervised object segmentation
                    and visual generation across six datasets.Furthermore, our learned object features can be utilized
                    by existing object-centric dynamics models, improving video prediction quality and downstream
                    temporal reasoning tasks.Finally, we demonstrate the scalability of SlotDiffusion to unconstrained
                    real-world datasets such as PASCAL VOC and COCO, when integrated with self-supervised pre-trained
                    image encoders.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72220">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-182"></span>

        <script>
        add_bookmark_click(
            72220,
             1,
            'bookmark-number-182',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72220">Transition-constant Normalization for Image
                Enhancement</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jie Huang · man zhou · Jinghao Zhang · Gang Yang · Mingde Yao · Chongyi Li · Zhiwei
            Xiong · Feng Zhao
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72220">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72220-thumb.png?t=1699755839.0814335" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72220" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72220" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72220">
                    Abstract <i id="caret-72220" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72220">
            <div class="abstract-display">
                <p>Normalization techniques that capture image style by statistical representation have become a popular
                    component in deep neural networks.Although image enhancement can be considered as a form of style
                    transformation, there has been little exploration of how normalization affect the enhancement
                    performance. To fully leverage the potential of normalization, we present a novel
                    Transition-Constant Normalization (TCN) for various image enhancement tasks.Specifically, it
                    consists of two streams of normalization operations arranged under an invertible constraint, along
                    with a feature sub-sampling operation that satisfies the normalization constraint.TCN enjoys several
                    merits, including being parameter-free, plug-and-play, and incurring no additional computational
                    costs.We provide various formats to utilize TCN for image enhancement, including seamless
                    integration with enhancement networks, incorporation into encoder-decoder architectures for
                    downsampling, and implementation of efficient architectures.Through extensive experiments on
                    multiple image enhancement tasks, like low-light enhancement, exposure correction, SDR2HDR
                    translation, and image dehazing, our TCN consistently demonstrates performance improvements.Besides,
                    it showcases extensive ability in other tasks including pan-sharpening and medical segmentation.The
                    code is available at \textit{\textcolor{blue}{https://github.com/huangkevinj/TCNorm}}.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71967">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-183"></span>

        <script>
        add_bookmark_click(
            71967,
             1,
            'bookmark-number-183',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71967">Dynamic Tensor Decomposition via Neural
                Diffusion-Reaction Processes</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zheng Wang · Shikai Fang · Shibo Li · Shandian Zhe</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71967">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71967-thumb.png?t=1699740313.5149221" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71967" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71967" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71967">
                    Abstract <i id="caret-71967" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71967">
            <div class="abstract-display">
                <p>Tensor decomposition is an important tool for multiway data analysis. In practice, the data is often
                    sparse yet associated with rich temporal information. Existing methods, however, often under-use the
                    time information and ignore the structural knowledge within the sparsely observed tensor entries. To
                    overcome these limitations and to better capture the underlying temporal structure, we propose
                    Dynamic EMbedIngs fOr dynamic Tensor dEcomposition (DEMOTE). We develop a neural diffusion-reaction
                    process to estimate dynamic embeddings for the entities in each tensor mode. Specifically, based on
                    the observed tensor entries, we build a multi-partite graph to encode the correlation between the
                    entities. We construct a graph diffusion process to co-evolve the embedding trajectories of the
                    correlated entities and use a neural network to construct a reaction process for each individual
                    entity. In this way, our model can capture both the commonalities and personalities during the
                    evolution of the embeddings for different entities. We then use a neural network to model the entry
                    value as a nonlinear function of the embedding trajectories. For model estimation, we combine ODE
                    solvers to develop a stochastic mini-batch learning algorithm. We propose a stratified sampling
                    method to balance the cost of processing each mini-batch so as to improve …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72030">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-184"></span>

        <script>
        add_bookmark_click(
            72030,
             1,
            'bookmark-number-184',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72030">Invariant Learning via Probability of Sufficient
                and Necessary Causes</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Mengyue Yang · Yonggang Zhang · Zhen Fang · Yali Du · Furui Liu · Jean-Francois Ton ·
            Jianhong Wang · Jun Wang
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72030">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72030-thumb.png?t=1701740123.1459298" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72030" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72030" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72030">
                    Abstract <i id="caret-72030" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72030">
            <div class="abstract-display">
                <p>Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where
                    testing distribution typically unknown and different from the training. Recent methods derived from
                    causality have shown great potential in achieving OOD generalization. However, existing methods
                    mainly focus on the invariance property of causes, while largely overlooking the property of
                    sufficiency and necessity conditions. Namely, a necessary but insufficient cause (feature) is
                    invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient
                    yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a
                    new domain. To capture the information of sufficient and necessary causes, we employ a classical
                    concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability
                    of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we
                    propose PNS risk and formulate an algorithm to learn representation with a high PNS value. We
                    theoretically analyze and prove the generalizability of the PNS risk. Experiments on both synthetic
                    and real-world benchmarks demonstrate the effectiveness of the proposed method. The detailed
                    implementation can be found at the GitHub repository: https://github.com/ymy4323460/CaSN.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71885">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-185"></span>

        <script>
        add_bookmark_click(
            71885,
             1,
            'bookmark-number-185',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71885">A Holistic Approach to Unifying Automatic Concept
                Extraction and Concept Importance Estimation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Thomas FEL · Victor Boutin · Louis Béthune · Remi Cadene · Mazda Moayeri · Léo Andéol ·
            Mathieu Chalvidal · Thomas Serre
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71885">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71885" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71885" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71885">
                    Abstract <i id="caret-71885" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71885">
            <div class="abstract-display">
                <p>In recent years, concept-based approaches have emerged as some of the most promising explainability
                    methods to help us interpret the decisions of Artificial Neural Networks (ANNs). These methods seek
                    to discover intelligible visual ``concepts'' buried within the complex patterns of ANN activations
                    in two key steps: (1) concept extraction followed by (2) importance estimation. While these two
                    steps are shared across methods, they all differ in their specific implementations. Here, we
                    introduce a unifying theoretical framework that recast the first step -- concept extraction problem
                    -- as a special case of <strong>dictionary learning</strong>, and we formalize the second step --
                    concept importance estimation -- as a more general form of <strong>attribution method</strong>.This
                    framework offers several advantages as it allows us: (i) to propose new evaluation metrics for
                    comparing different concept extraction approaches; (ii) to leverage modern attribution methods and
                    evaluation metrics to extend and systematically evaluate state-of-the-art concept-based approaches
                    and importance estimation techniques; (iii) to derive theoretical guarantees regarding the
                    optimality of such methods. We further leverage our framework to try to tackle a crucial question in
                    explainability: how to <em>efficiently</em> identify clusters of data points that are classified
                    based on a similar shared strategy.To illustrate these findings …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72262">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-186"></span>

        <script>
        add_bookmark_click(
            72262,
             1,
            'bookmark-number-186',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72262">Streaming PCA for Markovian Data</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Syamantak Kumar · Purnamrita Sarkar</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72262">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72262-thumb.png?t=1701885730.605845" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72262" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72262" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72262">
                    Abstract <i id="caret-72262" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72262">
            <div class="abstract-display">
                Since its inception in 1982, Oja's algorithm has become an established method for streaming principle
                component analysis (PCA). We study the problem of streaming PCA, where the data-points are sampled from
                an irreducible, aperiodic, and reversible Markov chain starting in stationarity. Our goal is to estimate
                the top eigenvector of the unknown covariance matrix of the stationary distribution. This setting has
                implications in scenarios where data can solely be sampled from a Markov Chain Monte Carlo (MCMC) type
                algorithm, and the objective is to perform inference on parameters of the stationary distribution. Most
                convergence guarantees for Oja's algorithm in the literature assume that the data-points are sampled
                IID. For data streams with Markovian dependence, one typically downsamples the data to get a "nearly"
                independent data stream. In this paper, we obtain the first near-optimal rate for Oja's algorithm on the
                entire data, where we remove the logarithmic dependence on the sample size, $n$, resulting from throwing
                data away in downsampling strategies.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71688">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-187"></span>

        <script>
        add_bookmark_click(
            71688,
             1,
            'bookmark-number-187',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71688">PDE-Refiner: Achieving Accurate Long Rollouts with
                Neural PDE Solvers</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Phillip Lippe · Bas Veeling · Paris Perdikaris · Richard Turner · Johannes
            Brandstetter
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71688">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71688-thumb.png?t=1701702800.4249387" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71688" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71688" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71688">
                    Abstract <i id="caret-71688" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71688">
            <div class="abstract-display">
                <p>Time-dependent partial differential equations (PDEs) are ubiquitous in science and engineering.
                    Recently, mostly due to the high computational cost of traditional solution techniques, deep neural
                    network based surrogates have gained increased interest. The practical utility of such neural PDE
                    solvers relies on their ability to provide accurate, stable predictions over long time horizons,
                    which is a notoriously hard problem. In this work, we present a large-scale analysis of common
                    temporal rollout strategies, identifying the neglect of non-dominant spatial frequency information,
                    often associated with high frequencies in PDE solutions, as the primary pitfall limiting stable,
                    accurate rollout performance. Based on these insights, we draw inspiration from recent advances in
                    diffusion models to introduce PDE-Refiner; a novel model class that enables more accurate modeling
                    of all frequency components via a multistep refinement process. We validate PDE-Refiner on
                    challenging benchmarks of complex fluid dynamics, demonstrating stable and accurate rollouts that
                    consistently outperform state-of-the-art models, including neural, numerical, and hybrid
                    neural-numerical architectures. We further demonstrate that PDE-Refiner greatly enhances data
                    efficiency, since the denoising objective implicitly induces a novel form of spectral data
                    augmentation. Finally, PDE-Refiner's connection to diffusion models enables an accurate and
                    efficient assessment of the model's predictive uncertainty, allowing us to …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72356">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-188"></span>

        <script>
        add_bookmark_click(
            72356,
             1,
            'bookmark-number-188',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72356">How to Scale Your EMA</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Dan Busbridge · Jason Ramapuram · Pierre Ablin · Tatiana Likhomanenko · Eeshan Gunesh
            Dhekane · Xavier Suau Cuadros · Russell Webb
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72356">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72356-thumb.png?t=1701272239.7458715" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72356" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72356" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72356">
                    Abstract <i id="caret-72356" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72356">
            <div class="abstract-display">
                Preserving training dynamics across batch sizes is an important tool for practical machine learning as
                it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by
                a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly
                with the batch size. Another important machine learning tool is the model EMA, a functional copy of a
                target model, whose parameters move towards those of its target model according to an Exponential Moving
                Average (EMA) at a rate parameterized by a momentum hyperparameter. This model EMA can improve the
                robustness and generalization of supervised learning, stabilize pseudo-labeling, and provide a learning
                signal for Self-Supervised Learning (SSL). Prior works have not considered the optimization of the model
                EMA when performing scaling, leading to different training dynamics across batch sizes and lower model
                performance. In this work, we provide a scaling rule for optimization in the presence of a model EMA and
                demonstrate the rule's validity across a range of architectures, optimizers, and data modalities. We
                also show the rule's validity where the model EMA contributes to the optimization of the target model,
                enabling us to train EMA-based pseudo-labeling and SSL methods at small …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72046">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-189"></span>

        <script>
        add_bookmark_click(
            72046,
             1,
            'bookmark-number-189',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72046">Towards Symmetry-Aware Generation of Periodic
                Materials</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Youzhi Luo · Chengkai Liu · Shuiwang Ji</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72046">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72046-thumb.png?t=1701747406.1103938" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72046" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72046" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72046">
                    Abstract <i id="caret-72046" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72046">
            <div class="abstract-display">
                <p>We consider the problem of generating periodic materials with deep models. While symmetry-aware
                    molecule generation has been studied extensively, periodic materials possess different symmetries,
                    which have not been completely captured by existing methods.In this work, we propose SyMat, a novel
                    material generation approach that can capture physical symmetries of periodic material structures.
                    SyMat generates atom types and lattices of materials through generating atom type sets, lattice
                    lengths and lattice angles with a variational auto-encoder model. In addition, SyMat employs a
                    score-based diffusion model to generate atom coordinates of materials, in which a novel
                    symmetry-aware probabilistic model is used in the coordinate diffusion process. We show that SyMat
                    is theoretically invariant to all symmetry transformations on materials and demonstrate that SyMat
                    achieves promising performance on random generation and property optimization tasks. Our code is
                    publicly available as part of the AIRS library (https://github.com/divelab/AIRS).</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72298">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-190"></span>

        <script>
        add_bookmark_click(
            72298,
             1,
            'bookmark-number-190',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72298">Equivariant Neural Operator Learning with Graphon
                Convolution</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Chaoran Cheng · Jian Peng</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72298">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72298-thumb.png?t=1699334639.2100453" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72298" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72298" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72298">
                    Abstract <i id="caret-72298" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72298">
            <div class="abstract-display">
                <p>We propose a general architecture that combines the coefficient learning scheme with a residual
                    operator layer for learning mappings between continuous functions in the 3D Euclidean space. Our
                    proposed model is guaranteed to achieve SE(3)-equivariance by design. From the graph spectrum view,
                    our method can be interpreted as convolution on graphons (dense graphs with infinitely many nodes),
                    which we term InfGCN. By leveraging both the continuous graphon structure and the discrete graph
                    structure of the input data, our model can effectively capture the geometric information while
                    preserving equivariance. Through extensive experiments on large-scale electron density datasets, we
                    observed that our model significantly outperformed the current state-of-the-art architectures.
                    Multiple ablation studies were also carried out to demonstrate the effectiveness of the proposed
                    architecture.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70059">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-191"></span>

        <script>
        add_bookmark_click(
            70059,
             1,
            'bookmark-number-191',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70059">Critical Initialization of Wide and Deep Neural
                Networks using Partial Jacobians: General Theory and Applications</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Darshil Doshi · Tianyu He · Andrey Gromov</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70059">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70059-thumb.png?t=1701900326.387539" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70059" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70059" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70059">
                    Abstract <i id="caret-70059" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70059">
            <div class="abstract-display">
                Deep neural networks are notorious for defying theoretical treatment. However, when the number of
                parameters in each layer tends to infinity, the network function is a Gaussian process (GP) and
                quantitatively predictive description is possible. Gaussian approximation allows one to formulate
                criteria for selecting hyperparameters, such as variances of weights and biases, as well as the learning
                rate. These criteria rely on the notion of criticality defined for deep neural networks. In this work we
                describe a new practical way to diagnose criticality. We introduce *partial Jacobians* of a network,
                defined as derivatives of preactivations in layer $l$ with respect to preactivations in layer $l_0\leq
                l$. We derive recurrence relations for the norms of partial Jacobians and utilize these relations to
                analyze criticality of deep fully connected neural networks with LayerNorm and/or residual connections.
                We derive and implement a simple and cheap numerical test that allows one to select optimal
                initialization for a broad class of deep neural networks; containing fully connected, convolutional and
                normalization layers. Using these tools we show quantitatively that proper stacking of the LayerNorm
                (applied to preactivations) and residual connections leads to an architecture that is critical for any
                initialization. Finally, we apply our methods to …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71995">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-192"></span>

        <script>
        add_bookmark_click(
            71995,
             1,
            'bookmark-number-192',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71995">VoxDet: Voxel Learning for Novel Instance
                Detection</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Bowen Li · Jiashun Wang · Yaoyu Hu · Chen Wang · Sebastian Scherer</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71995">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71995-thumb.png?t=1701895735.568034" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71995" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71995" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71995">
                    Abstract <i id="caret-71995" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71995">
            <div class="abstract-display">
                Detecting unseen instances based on multi-view templates is a challenging problem due to its open-world
                nature. Traditional methodologies, which primarily rely on $2 \mathrm{D}$ representations and matching
                techniques, are often inadequate in handling pose variations and occlusions. To solve this, we introduce
                VoxDet, a pioneer 3D geometry-aware framework that fully utilizes the strong 3D voxel representation and
                reliable voxel matching mechanism. VoxDet first ingeniously proposes template voxel aggregation (TVA)
                module, effectively transforming multi-view 2D images into 3D voxel features. By leveraging associated
                camera poses, these features are aggregated into a compact 3D template voxel. In novel instance
                detection, this voxel representation demonstrates heightened resilience to occlusion and pose
                variations. We also discover that a $3 \mathrm{D}$ reconstruction objective helps to pre-train the 2D-3D
                mapping in TVA. Second, to quickly align with the template voxel, VoxDet incorporates a Query Voxel
                Matching (QVM) module. The 2D queries are first converted into their voxel representation with the
                learned 2D-3D mapping. We find that since the 3D voxel representations encode the geometry, we can first
                estimate the relative rotation and then compare the aligned voxels, leading to improved accuracy and
                efficiency. In addition to method, we also introduce the first instance detection benchmark, RoboTools,
                …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72142">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-193"></span>

        <script>
        add_bookmark_click(
            72142,
             1,
            'bookmark-number-193',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72142">Provably Bounding Neural Network Preimages</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Suhas Kotha · Christopher Brix · J. Zico Kolter · Krishnamurthy Dvijotham · Huan Zhang
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72142">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72142" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72142" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72142">
                    Abstract <i id="caret-72142" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72142">
            <div class="abstract-display">
                Most work on the formal verification of neural networks has focused on bounding the set of outputs that
                correspond to a given set of inputs (for example, bounded perturbations of a nominal input). However,
                many use cases of neural network verification require solving the inverse problem, or over-approximating
                the set of inputs that lead to certain outputs. We present the INVPROP algorithm for verifying
                properties over the preimage of a linearly constrained output set, which can be combined with
                branch-and-bound to increase precision. Contrary to other approaches, our efficient algorithm is
                GPU-accelerated and does not require a linear programming solver. We demonstrate our algorithm for
                identifying safe control regions for a dynamical system via backward reachability analysis, verifying
                adversarial robustness, and detecting out-of-distribution inputs to a neural network. Our results show
                that in certain settings, we find over-approximations over $2500\times$ tighter than prior work while
                being $2.5\times$ faster. By strengthening robustness verification with output constraints, we
                consistently verify more properties than the previous state-of-the-art on multiple benchmarks, including
                a large model with 167k neurons in VNN-COMP 2023. Our algorithm has been incorporated into the
                $\alpha,\beta$-CROWN verifier, available at https://abcrown.org.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71783">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-194"></span>

        <script>
        add_bookmark_click(
            71783,
             1,
            'bookmark-number-194',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71783">Hierarchically Gated Recurrent Neural Network for
                Sequence Modeling</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zhen Qin · Songlin Yang · Yiran Zhong</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71783">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71783-thumb.png?t=1701938122.570914" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71783" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71783" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71783">
                    Abstract <i id="caret-71783" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71783">
            <div class="abstract-display">
                <p>Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training
                    and long-term dependency modeling.Recently, there has been a renewed interest in using linear RNNs
                    for efficient sequence modeling.These linear RNNs often employ gating mechanisms in the output of
                    the linear recurrence layer while ignoring the significance of using forget gates within the
                    recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent
                    Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The
                    lower bound increases monotonically when moving up layers. This allows the upper layers to model
                    long-term dependencies and the lower layers to model more local, short-term dependencies.
                    Experiments on language modeling, image classification, and long-range arena benchmarks showcase the
                    efficiency and effectiveness of our proposed model. The source code is available at
                    https://github.com/OpenNLPLab/HGRN.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72017">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-195"></span>

        <script>
        add_bookmark_click(
            72017,
             1,
            'bookmark-number-195',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72017">MeCo: Zero-Shot NAS with One Data and Single
                Forward Pass via Minimum Eigenvalue of Correlation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Tangyu Jiang · Haodi Wang · Rongfang Bie</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72017">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72017-thumb.png?t=1701509254.6813083" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72017" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72017" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72017">
                    Abstract <i id="caret-72017" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72017">
            <div class="abstract-display">
                Neural Architecture Search (NAS) is a promising paradigm in automatic architecture engineering.
                Zero-shot NAS can evaluate the network without training via some specific metrics called zero-cost
                proxies. Though effective, the existing zero-cost proxies either invoke at least one backpropagation or
                depend highly on the data and labels. To alleviate the above issues, in this paper, we first reveal how
                the Pearson correlation matrix of the feature maps impacts the convergence rate and the generalization
                capacity of an over-parameterized neural network. Enlightened by the theoretical analysis, we propose a
                novel zero-cost proxy called $\mathsf{MeCo}$, which requires only one random data for a single forward
                pass. We further propose an optimization approach $\mathsf{MeCo_{opt}}$ to improve the performance of
                our method. We design comprehensive experiments and extensively evaluate $\mathsf{MeCo}$ on multiple
                popular benchmarks. $\mathsf{MeCo}$ achieves the highest correlation with the ground truth (e.g., 0.89
                on NATS-Bench-TSS with CIFAR-10) among all the state-of-the-art proxies, which is also fully independent
                of the data and labels. Moreover, we integrate $\mathsf{MeCo}$ with the existing generation method to
                comprise a complete NAS. The experimental results illustrate that $\mathsf{MeCo}$-based NAS can select
                the architecture with the highest accuracy and a low search cost. For instance, the best network
                searched …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71847">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-196"></span>

        <script>
        add_bookmark_click(
            71847,
             1,
            'bookmark-number-196',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71847">Lexinvariant Language Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Qian Huang · Eric Zelikman · Sarah Chen · Yuhuai Wu · Gregory Valiant · Percy Liang
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71847">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71847-thumb.png?t=1701715591.3065493" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71847" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71847" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71847">
                    Abstract <i id="caret-71847" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71847">
            <div class="abstract-display">
                <p>Token embeddings, a mapping from discrete lexical symbols to continuous vectors, are at the heart of
                    any language model (LM). However, lexical symbol meanings can also be determined and even redefined
                    by their structural role in a long context. In this paper, we ask: is it possible for a language
                    model to be performant without \emph{any} fixed token embeddings? Such a language model would have
                    to rely entirely on the co-occurence and repetition of tokens in the context rather than the
                    \textit{a priori} identity of any token. To answer this, we study \textit{lexinvariant}language
                    models that are invariant to lexical symbols and therefore do not need fixed token embeddings in
                    practice. First, we prove that we can construct a lexinvariant LM to converge to the true language
                    model at a uniform rate that is polynomial in terms of the context length, with a constant factor
                    that is sublinear in the vocabulary size. Second, to build a lexinvariant LM, we simply encode
                    tokens using random Gaussian vectors, such that each token maps to the same representation within
                    each sequence but different representations across sequences. Empirically, we demonstrate that it
                    can indeed attain perplexity comparable to that of a standard language model, given …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72368">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-197"></span>

        <script>
        add_bookmark_click(
            72368,
             1,
            'bookmark-number-197',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72368">ZoomTrack: Target-aware Non-uniform Resizing for
                Efficient Visual Tracking</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yutong Kou · Jin Gao · Bing Li · Gang Wang · Weiming Hu · Yizheng Wang · Liang Li</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72368">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72368-thumb.png?t=1699866495.4993877" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72368" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72368" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72368">
                    Abstract <i id="caret-72368" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72368">
            <div class="abstract-display">
                <p>Recently, the transformer has enabled the speed-oriented trackers to approach state-of-the-art (SOTA)
                    performance with high-speed thanks to the smaller input size or the lighter feature extraction
                    backbone, though they still substantially lag behind their corresponding performance-oriented
                    versions. In this paper, we demonstrate that it is possible to narrow or even close this gap while
                    achieving high tracking speed based on the smaller input size. To this end, we non-uniformly resize
                    the cropped image to have a smaller input size while the resolution of the area where the target is
                    more likely to appear is higher and vice versa. This enables us to solve the dilemma of attending to
                    a larger visual field while retaining more raw information for the target despite a smaller input
                    size. Our formulation for the non-uniform resizing can be efficiently solved through quadratic
                    programming (QP) and naturally integrated into most of the crop-based local trackers. Comprehensive
                    experiments on five challenging datasets based on two kinds of transformer trackers, \ie, OSTrack
                    and TransT, demonstrate consistent improvements over them. In particular, applying our method to the
                    speed-oriented version of OSTrack even outperforms its performance-oriented counterpart by 0.6\% AUC
                    on TNL2K, while running 50\% faster and saving over 55\% …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71700">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-198"></span>

        <script>
        add_bookmark_click(
            71700,
             1,
            'bookmark-number-198',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71700">Adaptive Data Analysis in a Balanced Adversarial
                Model</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Kobbi Nissim · Uri Stemmer · Eliad Tsfadia</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71700">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71700-thumb.png?t=1701486785.1136189" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71700" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71700" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71700">
                    Abstract <i id="caret-71700" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71700">
            <div class="abstract-display">
                In adaptive data analysis, a mechanism gets $n$ i.i.d. samples from an unknown distribution $\cal{D}$,
                andis required to provide accurate estimations to a sequence of adaptively chosen statistical queries
                with respect to $\cal{D}$.Hardt and Ullman (FOCS 2014) and Steinke and Ullman (COLT 2015) showed that in
                general, it is computationally hard to answer more than $\Theta(n^2)$ adaptive queries, assuming the
                existence of one-way functions. However, these negative results strongly rely on an adversarial model
                that significantly advantages the adversarial analyst over the mechanism, as the analyst, who chooses
                the adaptive queries, also chooses the underlying distribution $\cal{D}$. This imbalance raises
                questions with respect to the applicability of the obtained hardness results -- an analyst who has
                complete knowledge of the underlying distribution $\cal{D}$ would have little need, if at all, to issue
                statistical queries to a mechanism which only holds a finite number of samples from $\cal{D}$.We
                consider more restricted adversaries, called \emph{balanced}, where each such adversary consists of two
                separated algorithms: The \emph{sampler} who is the entity that chooses the distribution and provides
                the samples to the mechanism, and the \emph{analyst} who chooses the adaptive queries, but has no prior
                knowledge of the underlying distribution (and hence has no …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71956">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-199"></span>

        <script>
        add_bookmark_click(
            71956,
             1,
            'bookmark-number-199',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71956">Diffusion Models and Semi-Supervised Learners
                Benefit Mutually with Few Labels</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zebin You · Yong Zhong · Fan Bao · Jiacheng Sun · Chongxuan LI · Jun Zhu</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71956">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71956-thumb.png?t=1701430811.2080247" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71956" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71956" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71956">
                    Abstract <i id="caret-71956" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71956">
            <div class="abstract-display">
                In an effort to further advance semi-supervised generative and classification tasks, we propose a simple
                yet effective training strategy called *dual pseudo training* (DPT), built upon strong semi-supervised
                learners and diffusion models. DPT operates in three stages: training a classifier on partially labeled
                data to predict pseudo-labels; training a conditional generative model using these pseudo-labels to
                generate pseudo images; and retraining the classifier with a mix of real and pseudo images. Empirically,
                DPT consistently achieves SOTA performance of semi-supervised generation and classification across
                various settings. In particular, with one or two labels per class, DPT achieves a Fréchet Inception
                Distance (FID) score of 3.08 or 2.52 on ImageNet $256\times256$. Besides, DPT outperforms competitive
                semi-supervised baselines substantially on ImageNet classification tasks, *achieving top-1 accuracies of
                59.0 (+2.8), 69.5 (+3.0), and 74.4 (+2.0)* with one, two, or five labels per class, respectively.
                Notably, our results demonstrate that diffusion can generate realistic images with only a few labels
                (e.g., $&lt;0.1$%) and generative augmentation remains viable for semi-supervised classification. Our
                code is available at *https://github.com/ML-GSAI/DPT*.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72386">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-200"></span>

        <script>
        add_bookmark_click(
            72386,
             1,
            'bookmark-number-200',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72386">Provably Fast Finite Particle Variants of SVGD via
                Virtual Particle Stochastic Approximation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Aniket Das · Dheeraj Nagaraj</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72386">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72386" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72386" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72386">
                    Abstract <i id="caret-72386" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72386">
            <div class="abstract-display">
                Stein Variational Gradient Descent (SVGD) is a popular particle-based variational inference algorithm
                with impressive empirical performance across various domains. Although the population (i.e,
                infinite-particle) limit dynamics of SVGD is well characterized, its behavior in the finite-particle
                regime is far less understood. To this end, our work introduces the notion of *virtual particles* to
                develop novel stochastic approximations of population-limit SVGD dynamics in the space of probability
                measures, that are exactly realizable using finite particles. As a result, we design two computationally
                efficient variants of SVGD, namely VP-SVGD and GB-SVGD, with provably fast finite-particle convergence
                rates. Our algorithms can be viewed as specific random-batch approximations of SVGD, which are
                computationally more efficient than ordinary SVGD. We show that the $n$ particles output by VP-SVGD and
                GB-SVGD, run for $T$ steps with batch-size $K$, are at-least as good as i.i.d samples from a
                distribution whose Kernel Stein Discrepancy to the target is at most $O(\tfrac{d^{1/3}}{(KT)^{1/6}})$
                under standard assumptions. Our results also hold under a mild growth condition on the potential
                function, which is much weaker than the isoperimetric (e.g. Poincare Inequality) or
                information-transport conditions (e.g. Talagrand's Inequality $\mathsf{T}_1$) generally considered in
                prior works. As a corollary, we analyze the convergence of the …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72077">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-201"></span>

        <script>
        add_bookmark_click(
            72077,
             1,
            'bookmark-number-201',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72077">Kernel Quadrature with Randomly Pivoted
                Cholesky</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Ethan Epperly · Elvira Moreno</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72077">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72077-thumb.png?t=1701217835.616582" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72077" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72077" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72077">
                    Abstract <i id="caret-72077" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72077">
            <div class="abstract-display">
                <p>This paper presents new quadrature rules for functions in a reproducing kernel Hilbert space using
                    nodes drawn by a sampling algorithm known as randomly pivoted Cholesky. The resulting computational
                    procedure compares favorably to previous kernel quadrature methods, which either achieve low
                    accuracy or require solving a computationally challenging sampling problem. Theoretical and
                    numerical results show that randomly pivoted Cholesky is fast and achieves comparable quadrature
                    error rates to more computationally expensive quadrature schemes based on continuous volume
                    sampling, thinning, and recombination. Randomly pivoted Cholesky is easily adapted to complicated
                    geometries with arbitrary kernels, unlocking new potential for kernel quadrature.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71693">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-202"></span>

        <script>
        add_bookmark_click(
            71693,
             1,
            'bookmark-number-202',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71693">Bifurcations and loss jumps in RNN training</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Lukas Eisenmann · Zahra Monfared · Niclas Göring · Daniel Durstewitz</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71693">Wed 13 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71693-thumb.png?t=1701421390.7740183" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71693" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71693" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71693">
                    Abstract <i id="caret-71693" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71693">
            <div class="abstract-display">
                Recurrent neural networks (RNNs) are popular machine learning tools for modeling and forecasting
                sequential data and for inferring dynamical systems (DS) from observed time series. Concepts from DS
                theory (DST) have variously been used to further our understanding of both, how trained RNNs solve
                complex tasks, and the training process itself. Bifurcations are particularly important phenomena in DS,
                including RNNs, that refer to topological (qualitative) changes in a system's dynamical behavior as one
                or more of its parameters are varied. Knowing the bifurcation structure of an RNN will thus allow to
                deduce many of its computational and dynamical properties, like its sensitivity to parameter variations
                or its behavior during training. In particular, bifurcations may account for sudden loss jumps observed
                in RNN training that could severely impede the training process. Here we first mathematically prove for
                a particular class of ReLU-based RNNs that certain bifurcations are indeed associated with loss
                gradients tending toward infinity or zero. We then introduce a novel heuristic algorithm for detecting
                all fixed points and $k$-cycles in ReLU-based RNNs and their existence and stability regions, hence
                bifurcation manifolds in parameter space. In contrast to previous numerical algorithms for finding fixed
                points and common continuation methods, our …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71015">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-203"></span>

        <script>
        add_bookmark_click(
            71015,
             1,
            'bookmark-number-203',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71015">Epistemic Neural Networks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Ian Osband · Zheng Wen · Seyed Mohammad Asghari · Vikranth Dwaracherla · MORTEZA
            IBRAHIMI · Xiuyuan Lu · Benjamin Van Roy
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71015">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71015" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71015" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71015">
                    Abstract <i id="caret-71015" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71015">
            <div class="abstract-display">
                <p>Intelligence relies on an agent's knowledge of what it does not know.This capability can be assessed
                    based on the quality of joint predictions of labels across multiple inputs.In principle,
                    ensemble-based approaches can produce effective joint predictions, but the computational costs of
                    large ensembles become prohibitive.We introduce the epinet: an architecture that can supplement any
                    conventional neural network, including large pretrained models, and can be trained with modest
                    incremental computation to estimate uncertainty.With an epinet, conventional neural networks
                    outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude
                    less computation.The epinet does not fit the traditional framework of Bayesian neural networks.To
                    accommodate development of approaches beyond BNNs, such as the epinet, we introduce the epistemic
                    neural network (ENN) as a general interface for models that produce joint predictions.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71468">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-204"></span>

        <script>
        add_bookmark_click(
            71468,
             1,
            'bookmark-number-204',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71468">Auditing for Human Expertise</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Rohan Alur · Loren Laine · Darrick Li · Manish Raghavan · Devavrat Shah · Dennis Shung
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71468">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71468-thumb.png?t=1702254247.5892696" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71468" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71468" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71468">
                    Abstract <i id="caret-71468" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71468">
            <div class="abstract-display">
                <p>High-stakes prediction tasks (e.g., patient diagnosis) are often handled by trained human experts. A
                    common source of concern about automation in these settings is that experts may exercise intuition
                    that is difficult to model and/or have access to information (e.g., conversations with a patient)
                    that is simply unavailable to a would-be algorithm. This raises a natural question whether human
                    experts add value which could not be captured by an algorithmic predictor.We develop a statistical
                    framework under which we can pose this question as a natural hypothesis test. Indeed, as our
                    framework highlights, detecting human expertise is more subtle than simply comparing the accuracy of
                    expert predictions to those made by a particular learning algorithm. Instead, we propose a simple
                    procedure which tests whether expert predictions are statistically independent from the outcomes of
                    interest after conditioning on the available inputs (‘features’). A rejection of our test thus
                    suggests that human experts may add value to any algorithm trained on the available data, and has
                    direct implications for whether human-AI ‘complementarity’ is achievable in a given prediction
                    task.We highlight the utility of our procedure using admissions data collected from the emergency
                    department of a large academic hospital system, where we show that …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71577">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-205"></span>

        <script>
        add_bookmark_click(
            71577,
             1,
            'bookmark-number-205',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71577">Implicit Variational Inference for High-Dimensional
                Posteriors</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Anshuk Uppal · Kristoffer Stensbo-Smidt · Wouter Boomsma · Jes Frellsen</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71577">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71577-thumb.png?t=1702255022.9368236" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71577" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71577" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71577">
                    Abstract <i id="caret-71577" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71577">
            <div class="abstract-display">
                <p>In variational inference, the benefits of Bayesian models rely on accurately capturing the true
                    posterior distribution. We propose using neural samplers that specify implicit distributions, which
                    are well-suited for approximating complex multimodal and correlated posteriors in high-dimensional
                    spaces. Our approach introduces novel bounds for approximate inference using implicit distributions
                    by locally linearising the neural sampler. This is distinct from existing methods that rely on
                    additional discriminator networks and unstable adversarial objectives. Furthermore, we present a new
                    sampler architecture that, for the first time, enables implicit distributions over tens of millions
                    of latent variables, addressing computational concerns by using differentiable numerical
                    approximations. We empirically show that our method is capable of recovering correlations across
                    layers in large Bayesian neural networks, a property that is crucial for a network's performance but
                    notoriously challenging to achieve. To the best of our knowledge, no other method has been shown to
                    accomplish this task for such large models. Through experiments in downstream tasks, we demonstrate
                    that our expressive posteriors outperform state-of-the-art uncertainty quantification methods,
                    validating the effectiveness of our training algorithm and the quality of the learned implicit
                    approximation.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71132">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-206"></span>

        <script>
        add_bookmark_click(
            71132,
             1,
            'bookmark-number-206',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71132">Paxion: Patching Action Knowledge in Video-Language
                Foundation Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zhenhailong Wang · Ansel Blume · Sha Li · Genglin Liu · Jaemin Cho · Zineng Tang · Mohit
            Bansal · Heng Ji
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71132">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71132-thumb.png?t=1698163953.4924502" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71132" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71132" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71132">
                    Abstract <i id="caret-71132" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71132">
            <div class="abstract-display">
                <p>Action knowledge involves the understanding of textual, visual, and temporal aspects of actions. We
                    introduce the <strong>Action Dynamics Benchmark (ActionBench)</strong> containing two carefully
                    designed probing tasks: Action Antonym and Video Reversal, which targets multimodal alignment
                    capabilities and temporal understanding skills of the model, respectively. Despite recent
                    video-language models’ (VidLM) impressive performance on various benchmark tasks, our diagnostic
                    tasks reveal their surprising deficiency (near-random performance) in action knowledge, suggesting
                    that current models rely on object recognition abilities as a shortcut for action understanding. To
                    remedy this, we propose a novel framework, <strong>Paxion</strong>, along with a new <strong>Discriminative
                        Video Dynamics Modeling (DVDM)</strong> objective. The Paxion framework utilizes a <strong>Knowledge
                        Patcher</strong> network to encode new action knowledge and a <strong>Knowledge Fuser</strong>
                    component to integrate the Patcher into frozen VidLMs without compromising their existing
                    capabilities. Due to limitations of the widely-used Video-Text Contrastive (VTC) loss for learning
                    action knowledge, we introduce the DVDM objective to train the Knowledge Patcher. DVDM forces the
                    model to encode the correlation between the action text and the correct ordering of video frames.
                    Our extensive analyses show that Paxion and DVDM together effectively fill the gap in action
                    knowledge understanding (~50% → 80%), while maintaining or improving performance …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71238">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-207"></span>

        <script>
        add_bookmark_click(
            71238,
             1,
            'bookmark-number-207',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71238">Normalizing flow neural networks by JKO scheme</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Chen Xu · Xiuyuan Cheng · Yao Xie</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71238">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71238" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71238" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71238">
                    Abstract <i id="caret-71238" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71238">
            <div class="abstract-display">
                <p>Normalizing flow is a class of deep generative models for efficient sampling and likelihood
                    estimation, which achieves attractive performance, particularly in high dimensions. The flow is
                    often implemented using a sequence of invertible residual blocks. Existing works adopt special
                    network architectures and regularization of flow trajectories. In this paper, we develop a neural
                    ODE flow network called JKO-iFlow, inspired by the Jordan-Kinderleherer-Otto (JKO) scheme, which
                    unfolds the discrete-time dynamic of the Wasserstein gradient flow. The proposed method stacks
                    residual blocks one after another, allowing efficient block-wise training of the residual blocks,
                    avoiding sampling SDE trajectories and score matching or variational learning, thus reducing the
                    memory load and difficulty in end-to-end training. We also develop adaptive time reparameterization
                    of the flow network with a progressive refinement of the induced trajectory in probability space to
                    improve the model accuracy further. Experiments with synthetic and real data show that the proposed
                    JKO-iFlow network achieves competitive performance compared with existing flow and diffusion models
                    at a significantly reduced computational and memory cost.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71546">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-208"></span>

        <script>
        add_bookmark_click(
            71546,
             1,
            'bookmark-number-208',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71546">L-CAD: Language-based Colorization with Any-level
                Descriptions using Diffusion Priors</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">zheng chang · Shuchen Weng · Peixuan Zhang · Yu Li · Si Li · Boxin Shi</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71546">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71546-thumb.png?t=1699601862.235215" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71546" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71546" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71546">
                    Abstract <i id="caret-71546" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71546">
            <div class="abstract-display">
                <p>Language-based colorization produces plausible and visually pleasing colors under the guidance of
                    user-friendly natural language descriptions. Previous methods implicitly assume that users provide
                    comprehensive color descriptions for most of the objects in the image, which leads to suboptimal
                    performance. In this paper, we propose a unified model to perform language-based colorization with
                    any-level descriptions. We leverage the pretrained cross-modality generative model for its robust
                    language understanding and rich color priors to handle the inherent ambiguity of any-level
                    descriptions. We further design modules to align with input conditions to preserve local spatial
                    structures and prevent the ghosting effect. With the proposed novel sampling strategy, our model
                    achieves instance-aware colorization in diverse and complex scenarios. Extensive experimental
                    results demonstrate our advantages of effectively handling any-level descriptions and outperforming
                    both language-based and automatic colorization methods. The code and pretrained modelsare available
                    at: https://github.com/changzheng123/L-CAD.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70991">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-209"></span>

        <script>
        add_bookmark_click(
            70991,
             1,
            'bookmark-number-209',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70991">Hypernetwork-based Meta-Learning for Low-Rank
                Physics-Informed Neural Networks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Woojin Cho · Kookjin Lee · Donsub Rim · Noseong Park</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70991">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70991-thumb.png?t=1702055981.0119371" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70991" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70991" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70991">
                    Abstract <i id="caret-70991" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70991">
            <div class="abstract-display">
                <p>In various engineering and applied science applications, repetitive numerical simulations of partial
                    differential equations (PDEs) for varying input parameters are often required (e.g., aircraft shape
                    optimization over many design parameters) and solvers are required to perform rapid execution. In
                    this study, we suggest a path that potentially opens up a possibility for physics-informed neural
                    networks (PINNs), emerging deep-learning-based solvers, to be considered as one such solver.
                    Although PINNs have pioneered a proper integration of deep-learning and scientific computing, they
                    require repetitive time-consuming training of neural networks, which is not suitable for many-query
                    scenarios. To address this issue, we propose a lightweight low-rank PINNs containing only hundreds
                    of model parameters and an associated hypernetwork-based meta-learning algorithm, which allows
                    efficient approximation of solutions of PDEs for varying ranges of PDE input parameters. Moreover,
                    we show that the proposed method is effective in overcoming a challenging issue, known as "failure
                    modes" of PINNs.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71505">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-210"></span>

        <script>
        add_bookmark_click(
            71505,
             1,
            'bookmark-number-210',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71505">Expressive Sign Equivariant Networks for Spectral
                Geometric Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Derek Lim · Joshua Robinson · Stefanie Jegelka · Haggai Maron</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71505">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71505" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71505" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71505">
                    Abstract <i id="caret-71505" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71505">
            <div class="abstract-display">
                <p>Recent work has shown the utility of developing machine learning models that respect the structure
                    and symmetries of eigenvectors. These works promote sign invariance, since for any eigenvector v the
                    negation -v is also an eigenvector. However, we show that sign invariance is theoretically limited
                    for tasks such as building orthogonally equivariant models and learning node positional encodings
                    for link prediction in graphs. In this work, we demonstrate the benefits of sign equivariance for
                    these tasks. To obtain these benefits, we develop novel sign equivariant neural network
                    architectures. Our models are based on a new analytic characterization of sign equivariant
                    polynomials and thus inherit provable expressiveness properties. Controlled synthetic experiments
                    show that our networks can achieve the theoretically predicted benefits of sign equivariant
                    models.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71589">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-211"></span>

        <script>
        add_bookmark_click(
            71589,
             1,
            'bookmark-number-211',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71589">ARTree: A Deep Autoregressive Model for
                Phylogenetic Inference</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Tianyu Xie · Cheng Zhang</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71589">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71589-thumb.png?t=1699440381.066554" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71589" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71589" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71589">
                    Abstract <i id="caret-71589" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71589">
            <div class="abstract-display">
                <p>Designing flexible probabilistic models over tree topologies is important for developing efficient
                    phylogenetic inference methods. To do that, previous works often leverage the similarity of tree
                    topologies via hand-engineered heuristic features which would require domain expertise and may
                    suffer from limited approximation capability. In this paper, we propose a deep autoregressive model
                    for phylogenetic inference based on graph neural networks (GNNs), called ARTree. By decomposing a
                    tree topology into a sequence of leaf node addition operations and modeling the involved conditional
                    distributions based on learnable topological features via GNNs, ARTree can provide a rich family of
                    distributions over tree topologies that have simple sampling algorithms, without using heuristic
                    features. We demonstrate the effectiveness and efficiency of our method on a benchmark of
                    challenging real data tree topology density estimation and variational Bayesian phylogenetic
                    inference problems.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71213">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-212"></span>

        <script>
        add_bookmark_click(
            71213,
             1,
            'bookmark-number-212',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71213">Thought Cloning: Learning to Think while Acting by
                Imitating Human Thinking</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Shengran Hu · Jeff Clune</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71213">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71213-thumb.png?t=1702078367.8465376" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71213" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71213" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71213">
                    Abstract <i id="caret-71213" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71213">
            <div class="abstract-display">
                Language is often considered a key aspect of human thinking, providing us with exceptional abilities to
                generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL)
                agents are far from human-level performance in any of these abilities. We hypothesize one reason for
                such cognitive deficiencies is that they lack the benefits of thinking in language and that we can
                improve AI agents by training them to $\textit{think like humans do}$. We introduce a novel Imitation
                Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human
                demonstrators, $\textit{but also the thoughts humans have as they perform these behaviors}$. While we
                expect Thought Cloning to truly shine at scale on internet-sized datasets (e.g. online videos with
                transcripts), here we conduct experiments in a domain where the thinking and action data are
                synthetically generated. Results reveal that Thought Cloning learns much faster than Behavioral Cloning
                and its performance advantage grows the further out of distribution test tasks are, highlighting its
                ability to better handle novel situations. Thought Cloning also provides important benefits for AI
                Safety and Interpretability, and makes it easier to debug and improve AI. Because we can observe the
                agent’s thoughts, we …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71257">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-213"></span>

        <script>
        add_bookmark_click(
            71257,
             1,
            'bookmark-number-213',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71257">Puzzlefusion: Unleashing the Power of Diffusion
                Models for Spatial Puzzle Solving</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Sepidehsadat (Sepid) Hossieni · Mohammad Amin Shabani · Saghar Irandoust · Yasutaka
            Furukawa
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71257">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71257" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71257" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71257">
                    Abstract <i id="caret-71257" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71257">
            <div class="abstract-display">
                <p>This paper presents an end-to-end neural architecture based on Diffusion Models for spatial puzzle
                    solving, particularly jigsaw puzzle and room arrangement tasks.In the latter task, for instance, the
                    proposed system ``PuzzleFusion'' takes a set of room layouts as polygonal curves in the top-down
                    view and aligns the room layout pieces by estimating their 2D translations and rotations, akin to
                    solving the jigsaw puzzle of room layouts. A surprising discovery of the paper is that the simple
                    use of a Diffusion Model effectively solves these challenging spatial puzzle tasks as a conditional
                    generation process. To enable learning of an end-to-end neural system, the paper introduces new
                    datasets with ground-truth arrangements: 1) 2D Voronoi Jigsaw Dataset, a synthetic one where pieces
                    are generated by voronoi diagram of 2D pointset; and 2) MagicPlan Dataset, a real one from a
                    production pipeline by MagicPlan, where pieces are room layouts constructed by augmented reality App
                    by real-estate consumers.The qualitative and quantitative evaluations demonstrate that the proposed
                    approach outperforms the competing methods by significant margins in all three spatial puzzle tasks.
                    We have provided code and data in https://sepidsh.github.io/puzzlefusion.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71276">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-214"></span>

        <script>
        add_bookmark_click(
            71276,
             1,
            'bookmark-number-214',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71276">STEVE-1: A Generative Model for Text-to-Behavior in
                Minecraft</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Shalev Lifshitz · Keiran Paster · Harris Chan · Jimmy Ba · Sheila McIlraith</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71276">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71276-thumb.png?t=1700012490.3890536" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71276" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71276" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71276">
                    Abstract <i id="caret-71276" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71276">
            <div class="abstract-display">
                <p>Constructing AI models that respond to text instructions is challenging, especially for sequential
                    decision-making tasks. This work introduces an instruction-tuned Video Pretraining (VPT) model for
                    Minecraft called STEVE-1, demonstrating that the unCLIP approach, utilized in DALL•E 2, is also
                    effective for creating instruction-following sequential decision-making agents. STEVE-1 is trained
                    in two steps: adapting the pretrained VPT model to follow commands in MineCLIP's latent space, then
                    training a prior to predict latent codes from text. This allows us to finetune VPT through
                    self-supervised behavioral cloning and hindsight relabeling, bypassing the need for costly human
                    text annotations. By leveraging pretrained models like VPT and MineCLIP and employing best practices
                    from text-conditioned image generation, STEVE-1 costs just $60 to train and can follow short-horizon
                    open-ended text and visual instructions in Minecraft. STEVE-1 sets a new bar for open-ended
                    instruction following in Minecraft with low-level controls (mouse and keyboard) and raw pixel
                    inputs, far outperforming previous baselines and robustly completing 12 of 13 tasks in our
                    early-game evaluation suite. We provide experimental evidence highlighting key factors for
                    downstream performance, including pretraining, classifier-free guidance, and data scaling. All
                    resources, including our model weights, training scripts, and evaluation tools are made available
                    for further research.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71099">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-215"></span>

        <script>
        add_bookmark_click(
            71099,
             1,
            'bookmark-number-215',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71099">HIQL: Offline Goal-Conditioned RL with Latent
                States as Actions</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Seohong Park · Dibya Ghosh · Benjamin Eysenbach · Sergey Levine</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71099">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71099-thumb.png?t=1698475338.7148008" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71099" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71099" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71099">
                    Abstract <i id="caret-71099" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71099">
            <div class="abstract-display">
                <p>Unsupervised pre-training has recently become the bedrock for computer vision and natural language
                    processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous
                    self-supervised approach for making use of large quantities of unlabeled (reward-free) data.
                    However, building effective algorithms for goal-conditioned RL that can learn directly from diverse
                    offline data is challenging, because it is hard to accurately estimate the exact value function for
                    faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant
                    goals entails first passing through closer subgoals. This structure can be very useful, as assessing
                    the quality of actions for nearby goals is typically easier than for more distant goals. Based on
                    this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one
                    action-free value function, we learn two policies that allow us to exploit this structure: a
                    high-level policy that treats states as actions and predicts (a latent representation of) a subgoal
                    and a low-level policy that predicts the action for reaching this subgoal. Through analysis and
                    didactic examples, we show how this hierarchical decomposition makes our method robust to noise in
                    the estimated value function. We then apply our method to offline goal-reaching benchmarks, showing
                    that our …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71271">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-216"></span>

        <script>
        add_bookmark_click(
            71271,
             1,
            'bookmark-number-216',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71271">Deep Fractional Fourier Transform</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Hu Yu · Jie Huang · Lingzhi LI · man zhou · Feng Zhao</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71271">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71271-thumb.png?t=1699698130.4282463" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71271" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71271" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71271">
                    Abstract <i id="caret-71271" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71271">
            <div class="abstract-display">
                <p>Existing deep learning-based computer vision methods usually operate in the spatial and frequency
                    domains, which are two orthogonal \textbf{individual} perspectives for image processing.In this
                    paper, we introduce a new spatial-frequency analysis tool, Fractional Fourier Transform (FRFT), to
                    provide comprehensive \textbf{unified} spatial-frequency perspectives.The FRFT is a unified
                    continuous spatial-frequency transform that simultaneously reflects an image's spatial and frequency
                    representations, making it optimal for processing non-stationary image signals.We explore the
                    properties of the FRFT for image processing and present a fast implementation of the 2D FRFT, which
                    facilitates its widespread use.Based on these explorations, we introduce a simple yet effective
                    operator, Multi-order FRactional Fourier Convolution (MFRFC), which exhibits the remarkable merits
                    of processing images from more perspectives in the spatial-frequency plane. Our proposed MFRFC is a
                    general and basic operator that can be easily integrated into various tasks for performance
                    improvement.We experimentally evaluate the MFRFC on various computer vision tasks, including object
                    detection, image classification, guided super-resolution, denoising, dehazing, deraining, and
                    low-light enhancement. Our proposed MFRFC consistently outperforms baseline methods by significant
                    margins across all tasks.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71128">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-217"></span>

        <script>
        add_bookmark_click(
            71128,
             1,
            'bookmark-number-217',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71128">Learning Universal Policies via Text-Guided Video
                Generation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yilun Du · Sherry Yang · Bo Dai · Hanjun Dai · Ofir Nachum · Josh Tenenbaum · Dale
            Schuurmans · Pieter Abbeel
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71128">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71128" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71128" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71128">
                    Abstract <i id="caret-71128" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71128">
            <div class="abstract-display">
                <p>A goal of artificial intelligence is to construct an agent that can solve a wide variety of tasks.
                    Recent progress in text-guided image synthesis has yielded models with an impressive ability to
                    generate complex novel images, exhibiting combinatorial generalization across domains. Motivated by
                    this success, we investigate whether such tools can be used to construct more general-purpose
                    agents. Specifically, we cast the sequential decision making problem as a text-conditioned video
                    generation problem, where, given a text-encoded specification of a desired goal, a planner
                    synthesizes a set of future frames depicting its planned actions in the future, after which control
                    actions are extracted from the generated video. By leveraging text as the underlying goal
                    specification, we are able to naturally and combinatorially generalize to novel goals. The proposed
                    policy-as-video formulation can further represent environments with different state and action
                    spaces in a unified space of images, which, for example, enables learning and generalization across
                    a variety of robot manipulation tasks. Finally, by leveraging pretrained language embeddings and
                    widely available videos from the internet, the approach enables knowledge transfer through
                    predicting highly realistic video plans for real robots.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71544">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-218"></span>

        <script>
        add_bookmark_click(
            71544,
             1,
            'bookmark-number-218',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71544">4M: Massively Multimodal Masked Modeling</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">David Mizrahi · Roman Bachmann · Oguzhan Kar · Teresa Yeo · Mingfei Gao · Afshin Dehghan
            · Amir Zamir
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71544">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71544-thumb.png?t=1699980544.7839813" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71544" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71544" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71544">
                    Abstract <i id="caret-71544" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71544">
            <div class="abstract-display">
                <p>Current machine learning models for vision are often highly specialized and limited to a single
                    modality and task. In contrast, recent large language models exhibit a wide range of capabilities,
                    hinting at a possibility for similarly versatile models in computer vision.In this paper, we take a
                    step in this direction and propose a multimodal training scheme called 4M. It consists of training a
                    single unified Transformer encoder-decoder using a masked modeling objective across a wide range of
                    input/output modalities – including text, images, geometric, and semantic modalities, as well as
                    neural network feature maps. 4M achieves scalability by unifying the representation space of all
                    modalities through mapping them into discrete tokens and performing multimodal masked modeling on a
                    small randomized subset of tokens.4M leads to models that exhibit several key capabilities: (1) they
                    can perform a diverse set of vision tasks out of the box, (2) they excel when fine-tuned for unseen
                    downstream tasks or new input modalities, and (3) they can function as a generative model that can
                    be conditioned on arbitrary modalities, enabling a wide variety of expressive multimodal editing
                    capabilities with remarkable flexibility.Through experimental analyses, we demonstrate the potential
                    of 4M for training versatile and scalable foundation …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71345">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-219"></span>

        <script>
        add_bookmark_click(
            71345,
             1,
            'bookmark-number-219',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71345">Hierarchical Integration Diffusion Model for
                Realistic Image Deblurring</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zheng Chen · Yulun Zhang · Ding Liu · bin xia · Jinjin Gu · Linghe Kong · Xin Yuan</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71345">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71345-thumb.png?t=1697791042.687296" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71345" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71345" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71345">
                    Abstract <i id="caret-71345" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71345">
            <div class="abstract-display">
                <p>Diffusion models (DMs) have recently been introduced in image deblurring and exhibited promising
                    performance, particularly in terms of details reconstruction. However, the diffusion model requires
                    a large number of inference iterations to recover the clean image from pure Gaussian noise, which
                    consumes massive computational resources. Moreover, the distribution synthesized by the diffusion
                    model is often misaligned with the target results, leading to restrictions in distortion-based
                    metrics. To address the above issues, we propose the Hierarchical Integration Diffusion Model
                    (HI-Diff), for realistic image deblurring. Specifically, we perform the DM in a highly compacted
                    latent space to generate the prior feature for the deblurring process. The deblurring process is
                    implemented by a regression-based method to obtain better distortion accuracy. Meanwhile, the highly
                    compact latent space ensures the efficiency of the DM. Furthermore, we design the hierarchical
                    integration module to fuse the prior into the regression-based model from multiple scales, enabling
                    better generalization in complex blurry scenarios. Comprehensive experiments on synthetic and
                    real-world blur datasets demonstrate that our HI-Diff outperforms state-of-the-art methods. Code and
                    trained models are available at https://github.com/zhengchen1999/HI-Diff.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71667">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-220"></span>

        <script>
        add_bookmark_click(
            71667,
             1,
            'bookmark-number-220',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71667">Relax, it doesn’t matter how you get there: A new
                self-supervised approach for multi-timescale behavior analysis</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Mehdi Azabou · Michael Mendelson · Nauman Ahad · Maks Sorokin · Shantanu Thakoor ·
            Carolina Urzay · Eva Dyer
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71667">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71667-thumb.png?t=1702163804.8340917" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71667" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71667" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71667">
                    Abstract <i id="caret-71667" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71667">
            <div class="abstract-display">
                <p>Unconstrained and natural behavior consists of dynamics that are complex and unpredictable,
                    especially when trying to predict what will happen multiple steps into the future. While some
                    success has been found in building representations of animal behavior under constrained or
                    simplified task-based conditions, many of these models cannot be applied to free and naturalistic
                    settings where behavior becomes increasingly hard to model. In this work, we develop a multi-task
                    representation learning model for animal behavior that combines two novel components: (i) an
                    action-prediction objective that aims to predict the distribution of actions over future timesteps,
                    and (ii) a multi-scale architecture that builds separate latent spaces to accommodate short- and
                    long-term dynamics. After demonstrating the ability of the method to build representations of both
                    local and global dynamics in robots in varying environments and terrains, we apply our method to the
                    MABe 2022 Multi-Agent Behavior challenge, where our model ranks first overall on both mice and fly
                    benchmarks. In all of these cases, we show that our model can build representations that capture the
                    many different factors that drive behavior and solve a wide range of downstream tasks.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71657">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-221"></span>

        <script>
        add_bookmark_click(
            71657,
             1,
            'bookmark-number-221',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71657">Promises and Pitfalls of Threshold-based
                Auto-labeling</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Harit Vishwakarma · Heguang Lin · Frederic Sala · Ramya Korlakai Vinayak</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71657">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71657" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71657" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71657">
                    Abstract <i id="caret-71657" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71657">
            <div class="abstract-display">
                <p>Creating large-scale high-quality labeled datasets is a major bottleneck in supervised machine
                    learning workflows. Threshold-based auto-labeling (TBAL), where validation data obtained from humans
                    is used to find a confidence threshold above which the data is machine-labeled, reduces reliance on
                    manual annotation. TBAL is emerging as a widely-used solution in practice. Given the long shelf-life
                    and diverse usage of the resulting datasets, understanding when the data obtained by such
                    auto-labeling systems can be relied on is crucial. This is the first work to analyze TBAL systems
                    and derive sample complexity bounds on the amount of human-labeled validation data required for
                    guaranteeing the quality of machine-labeled data. Our results provide two crucial insights. First,
                    reasonable chunks of unlabeled data can be automatically and accurately labeled by seemingly bad
                    models. Second, a hidden downside of TBAL systems is potentially prohibitive validation data usage.
                    Together, these insights describe the promise and pitfalls of using such systems. We validate our
                    theoretical guarantees with extensive experiments on synthetic and real datasets.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71136">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-222"></span>

        <script>
        add_bookmark_click(
            71136,
             1,
            'bookmark-number-222',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71136">Anonymous and Copy-Robust Delegations for Liquid
                Democracy</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Markus Utke · Ulrike Schmidt-Kraepelin</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71136">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71136-thumb.png?t=1701685108.7958677" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71136" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71136" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71136">
                    Abstract <i id="caret-71136" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71136">
            <div class="abstract-display">
                <p>Liquid democracy with ranked delegations is a novel voting scheme that unites the practicability of
                    representative democracy with the idealistic appeal of direct democracy: Every voter decides between
                    casting their vote on a question at hand or delegating their voting weight to some other, trusted
                    agent. Delegations are transitive, and since voters may end up in a delegation cycle, they are
                    encouraged to indicate not only a single delegate, but a set of potential delegates and a ranking
                    among them. Based on the delegation preferences of all voters, a delegation rule selects one
                    representative per voter. Previous work has revealed a trade-off between two properties of
                    delegation rules called anonymity and copy-robustness. To overcome this issue we study two
                    fractional delegation rules: Mixed Borda branching, which generalizes a rule satisfying
                    copy-robustness, and the random walk rule, which satisfies anonymity. Using the Markov chain tree
                    theorem, we show that the two rules are in fact equivalent, and simultaneously satisfy generalized
                    versions of the two properties. Combining the same theorem with Fulkerson's algorithm, we develop a
                    polynomial-time algorithm for computing the outcome of the studied delegation rule. This algorithm
                    is of independent interest, having applications in semi-supervised learning and graph theory.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71147">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-223"></span>

        <script>
        add_bookmark_click(
            71147,
             1,
            'bookmark-number-223',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71147">Online Control for Meta-optimization</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Xinyi Chen · Elad Hazan</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71147">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71147-thumb.png?t=1701715820.2132661" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71147" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71147" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71147">
                    Abstract <i id="caret-71147" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71147">
            <div class="abstract-display">
                <p>Choosing the optimal hyperparameters, including learning rate and momentum, for specific optimization
                    instances is a significant yet non-convex challenge. This makes conventional iterative techniques
                    such as hypergradient descent \cite{baydin2017online} insufficient in obtaining global optimality
                    guarantees.We consider the more general task of meta-optimization -- online learning of the best
                    optimization algorithm given problem instances, and introduce a novel approach based on control
                    theory. We show how meta-optimization can be formulated as an optimal control problem, departing
                    from existing literature that use stability-based methods to study optimization. Our approach
                    leverages convex relaxation techniques in the recently-proposed nonstochastic control framework to
                    overcome the challenge of nonconvexity, and obtains regret guarantees vs. the best offline solution.
                    This guarantees that in meta-optimization, we can learn a method that attains convergence comparable
                    to that of the best optimization method in hindsight from a class of methods.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71203">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-224"></span>

        <script>
        add_bookmark_click(
            71203,
             1,
            'bookmark-number-224',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71203">CS4ML: A general framework for active learning with
                arbitrary data based on Christoffel functions</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Juan M. Cardenas · Ben Adcock · Nick Dexter</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71203">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71203-thumb.png?t=1701726405.739612" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71203" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71203" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71203">
                    Abstract <i id="caret-71203" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71203">
            <div class="abstract-display">
                <p>We introduce a general framework for active learning in regression problems. Our framework extends
                    the standard setup by allowing for general types of data, rather than merely pointwise samples of
                    the target function. This generalization covers many cases of practical interest, such as data
                    acquired in transform domains (e.g., Fourier data), vector-valued data (e.g., gradient-augmented
                    data), data acquired along continuous curves, and, multimodal data (i.e., combinations of different
                    types of measurements). Our framework considers random sampling according to a finite number of
                    sampling measures and arbitrary nonlinear approximation spaces (model classes). We introduce the
                    concept of \textit{generalized Christoffel functions} and show how these can be used to optimize the
                    sampling measures. We prove that this leads to near-optimal sample complexity in various important
                    cases. This paper focuses on applications in scientific computing, where active learning is often
                    desirable, since it is usually expensive to generate data. We demonstrate the efficacy of our
                    framework for gradient-augmented learning with polynomials, Magnetic Resonance Imaging (MRI) using
                    generative models and adaptive sampling for solving PDEs using Physics-Informed Neural Networks
                    (PINNs).</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70990">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-225"></span>

        <script>
        add_bookmark_click(
            70990,
             1,
            'bookmark-number-225',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70990">Private (Stochastic) Non-Convex Optimization
                Revisited: Second-Order Stationary Points and Excess Risks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Daogao Liu · Arun Ganesh · Sewoong Oh · Abhradeep Guha Thakurta</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70990">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70990" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70990" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70990">
                    Abstract <i id="caret-70990" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70990">
            <div class="abstract-display">
                <p>We reconsider the challenge of non-convex optimization under differential privacy constraint.
                    Building upon the previous variance-reduced algorithm SpiderBoost, we propose a novel framework that
                    employs two types of gradient oracles: one that estimates the gradient at a single point and a more
                    cost-effective option that calculates the gradient difference between two points. Our framework can
                    ensure continuous accuracy of gradient estimations and subsequently enhances the rates of
                    identifying second-order stationary points.Additionally, we consider a more challenging task by
                    attempting to locate the global minima of a non-convex objective via the exponential mechanism
                    without almost any assumptions. Our preliminary results suggest that the regularized exponential
                    mechanism can effectively emulate previous empirical and population risk bounds, negating the need
                    for smoothness assumptions for algorithms with polynomial running time. Furthermore, with running
                    time factors excluded, the exponential mechanism demonstrates promising population risk bound
                    performance, and we provide a nearly matching lower bound.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71154">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-226"></span>

        <script>
        add_bookmark_click(
            71154,
             1,
            'bookmark-number-226',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71154">Uncovering the Hidden Dynamics of Video
                Self-supervised Learning under Distribution Shifts</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Pritam Sarkar · Ahmad Beirami · Ali Etemad</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71154">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71154-thumb.png?t=1701739073.1759608" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71154" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71154" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71154">
                    Abstract <i id="caret-71154" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71154">
            <div class="abstract-display">
                <p>Video self-supervised learning (VSSL) has made significant progress in recent years. However, the
                    exact behavior and dynamics of these models under different forms of distribution shift are not yet
                    known. In this paper, we comprehensively study the behavior of six popular self-supervised methods
                    (v-SimCLR, v-MoCo, v-BYOL, v-SimSiam, v-DINO, v-MAE) in response to various forms of natural
                    distribution shift, i.e., (i) context shift, (ii) viewpoint shift, (iii) actor shift, (iv) source
                    shift, (v) generalizability to unknown classes (zero-shot), and (vi) open-set recognition. To
                    perform this extensive study, we carefully craft a test bed consisting of 17 in-distribution and
                    out-of-distribution benchmark pairs using available public datasets and a series of evaluation
                    protocols to stress-test the different methods under the intended shifts. Our study uncovers a
                    series of intriguing findings and interesting behaviors of VSSL methods. For instance, we observe
                    that while video models generally struggle with context shifts, v-MAE and supervised learning
                    exhibit more robustness. Moreover, our study shows that v-MAE is a strong temporal learner, whereas
                    contrastive methods, v-SimCLR and v-MoCo, exhibit strong performances against viewpoint shifts. When
                    studying the notion of open-set recognition, we notice a trade-off between closed-set and open-set
                    recognition performance if the pretrained VSSL encoders are used …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71582">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-227"></span>

        <script>
        add_bookmark_click(
            71582,
             1,
            'bookmark-number-227',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71582">Topological Parallax: A Geometric Specification for
                Deep Perception Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Abraham Smith · Michael Catanzaro · Gabrielle Angeloro · Nirav Patel · Paul Bendich
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71582">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71582-thumb.png?t=1701377756.5779572" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71582" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71582" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71582">
                    Abstract <i id="caret-71582" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71582">
            <div class="abstract-display">
                <p>For safety and robustness of AI systems, we introduce <em>topological parallax</em> as a theoretical
                    and computational tool that compares a trained model to a reference dataset to determine whether
                    they have similar multiscale geometric structure. Our proofs and examples show that this geometric
                    similarity between dataset and model is essential to trustworthy interpolation and perturbation, and
                    we conjecture that this new concept will add value to the current debate regarding the unclear
                    relationship between "overfitting"' and "generalization'' in applications of deep-learning. In
                    typical deep-learning applications, an explicit geometric description of the model isimpossible, but
                    parallax can estimate topological features (components, cycles, voids, etc.)in the model by
                    examining the effect on the Rips complex of geodesic distortions using the reference dataset.Thus,
                    parallax indicates whether the model shares similar multiscale geometric features with the
                    dataset.Parallax presents theoretically via topological data analysis [TDA] as a bi-filtered
                    persistence module,and the key properties of this module are stable under perturbation of the
                    reference dataset.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71523">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-228"></span>

        <script>
        add_bookmark_click(
            71523,
             1,
            'bookmark-number-228',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71523">A Unified Generalization Analysis of Re-Weighting
                and Logit-Adjustment for Imbalanced Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zitai Wang · Qianqian Xu · Zhiyong Yang · Yuan He · Xiaochun Cao · Qingming Huang</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71523">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71523-thumb.png?t=1699860977.5445325" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71523" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71523" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71523">
                    Abstract <i id="caret-71523" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71523">
            <div class="abstract-display">
                <p>Real-world datasets are typically imbalanced in the sense that only a few classes have numerous
                    samples, while many classes are associated with only a few samples. As a result, a naive ERM
                    learning process will be biased towards the majority classes, making it difficult to generalize to
                    the minority classes. To address this issue, one simple but effective approach is to modify the loss
                    function to emphasize the learning on minority classes, such as re-weighting the losses or adjusting
                    the logits via class-dependent terms. However, existing generalization analysis of such losses is
                    still coarse-grained and fragmented, failing to explain some empirical results. To bridge this gap
                    between theory and practice, we propose a novel technique named data-dependent contraction to
                    capture how these modified losses handle different classes. On top of this technique, a fine-grained
                    generalization bound is established for imbalanced learning, which helps reveal the mystery of
                    re-weighting and logit-adjustment in a unified manner. Furthermore, a principled learning algorithm
                    is developed based on the theoretical insights. Finally, the empirical results on benchmark datasets
                    not only validate the theoretical results but also demonstrate the effectiveness of the proposed
                    method.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71481">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-229"></span>

        <script>
        add_bookmark_click(
            71481,
             1,
            'bookmark-number-229',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71481">Effective Human-AI Teams via Learned Natural
                Language Rules and Onboarding</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Hussein Mozannar · Jimin Lee · Dennis Wei · Prasanna Sattigeri · Subhro Das · David
            Sontag
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71481">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71481" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71481" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71481">
                    Abstract <i id="caret-71481" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71481">
            <div class="abstract-display">
                <p>People are relying on AI agents to assist them with various tasks. The human must know when to rely
                    on the agent, collaborate with the agent, or ignore its suggestions. In this work, we propose to
                    learn rules grounded in data regions and described in natural language that illustrate how the human
                    should collaborate with the AI. Our novel region discovery algorithm finds local regions in the data
                    as neighborhoods in an embedding space that corrects the human prior. Each region is then described
                    using an iterative and contrastive procedure where a large language model describes the region. We
                    then teach these rules to the human via an onboarding stage. Through user studies on object
                    detection and question-answering tasks, we show that our method can lead to more accurate human-AI
                    teams. We also evaluate our region discovery and description algorithms separately.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71561">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-230"></span>

        <script>
        add_bookmark_click(
            71561,
             1,
            'bookmark-number-230',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71561">Neural Injective Functions for Multisets, Measures
                and Graphs via a Finite Witness Theorem</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Tal Amir · Steven Gortler · Ilai Avni · Ravina Ravina · Nadav Dym</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71561">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71561-thumb.png?t=1699874820.104098" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71561" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71561" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71561">
                    Abstract <i id="caret-71561" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71561">
            <div class="abstract-display">
                Injective multiset functions have a key role in the theoretical study of machine learning on multisets
                and graphs. Yet, there remains a gap between the provably injective multiset functions considered in
                theory, which typically rely on polynomial moments, and the multiset functions used in practice, which
                rely on $\textit{neural moments}$ — whose injectivity on multisets has not been studied to date.In this
                paper, we bridge this gap by showing that moments of neural networks do define injective multiset
                functions, provided that an analytic non-polynomial activation is used. The number of moments required
                by our theory is optimal essentially up to a multiplicative factor of two. To prove this result, we
                state and prove a $\textit{finite witness theorem}$, which is of independent interest. As a corollary to
                our main theorem, we derive new approximation results for functions on multisets and measures, and new
                separation results for graph neural networks. We also provide two negative results: (1) moments of
                piecewise-linear neural networks cannot be injective multiset functions; and (2) even when moment-based
                multiset functions are injective, they can never be bi-Lipschitz.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71416">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-231"></span>

        <script>
        add_bookmark_click(
            71416,
             1,
            'bookmark-number-231',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71416">Rank-N-Contrast: Learning Continuous
                Representations for Regression</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Kaiwen Zha · Peng Cao · Jeany Son · Yuzhe Yang · Dina Katabi</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71416">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71416-thumb.png?t=1698697633.7740283" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71416" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71416" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71416">
                    Abstract <i id="caret-71416" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71416">
            <div class="abstract-display">
                <p>Deep regression models typically learn in an end-to-end fashion without explicitly emphasizing a
                    regression-aware representation. Consequently, the learned representations exhibit fragmentation and
                    fail to capture the continuous nature of sample orders, inducing suboptimal results across a wide
                    range of regression tasks. To fill the gap, we propose Rank-N-Contrast (RNC), a framework that
                    learns continuous representations for regression by contrasting samples against each other based on
                    their rankings in the target space. We demonstrate, theoretically and empirically, that RNC
                    guarantees the desired order of learned representations in accordance with the target orders,
                    enjoying not only better performance but also significantly improved robustness, efficiency, and
                    generalization. Extensive experiments using five real-world regression datasets that span computer
                    vision, human-computer interaction, and healthcare verify that RNC achieves state-of-the-art
                    performance, highlighting its intriguing properties including better data efficiency, robustness to
                    spurious targets and data corruptions, and generalization to distribution shifts.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71226">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-232"></span>

        <script>
        add_bookmark_click(
            71226,
             1,
            'bookmark-number-232',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71226">SimFBO: Towards Simple, Flexible and
                Communication-efficient Federated Bilevel Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yifan Yang · Peiyao Xiao · Kaiyi Ji</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71226">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71226" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71226" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71226">
                    Abstract <i id="caret-71226" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71226">
            <div class="abstract-display">
                <p>Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge
                    computing due to the emerging nested optimization structure in meta-learning, fine-tuning,
                    hyperparameter tuning, etc. However, existing FBO algorithms often involve complicated computations
                    and require multiple sub-loops per iteration, each of which contains a number of communication
                    rounds. In this paper, we propose a simple and flexible FBO framework named SimFBO, which is easy to
                    implement without sub-loops, and includes a generalized server-side aggregation and update for
                    improving communication efficiency. We further propose System-level heterogeneity robust FBO
                    (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneous local computation. We
                    show that SimFBO and ShroFBO provably achieve a linear convergence speedup with partial client
                    participation and client sampling without replacement, as well as improved sample and communication
                    complexities. Experiments demonstrate the effectiveness of the proposed methods over existing FBO
                    algorithms.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71302">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-233"></span>

        <script>
        add_bookmark_click(
            71302,
             1,
            'bookmark-number-233',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71302">Which Models have Perceptually-Aligned Gradients?
                An Explanation via Off-Manifold Robustness</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Suraj Srinivas · Sebastian Bordt · Himabindu Lakkaraju</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71302">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71302" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71302" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71302">
                    Abstract <i id="caret-71302" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71302">
            <div class="abstract-display">
                <p>One of the remarkable properties of robust computer vision models is that their input-gradients are
                    often aligned with human perception, referred to in the literature as perceptually-aligned gradients
                    (PAGs). Despite only being trained for classification, PAGs cause robust models to have rudimentary
                    generative capabilities, including image generation, denoising, and in-painting. However, the
                    underlying mechanisms behind these phenomena remain unknown. In this work, we provide a first
                    explanation of PAGs via \emph{off-manifold robustness}, which states that models must be more robust
                    off- the data manifold than they are on-manifold. We first demonstrate theoretically that
                    off-manifold robustness leads input gradients to lie approximately on the data manifold, explaining
                    their perceptual alignment. We then show that Bayes optimal models satisfy off-manifold robustness,
                    and confirm the same empirically for robust models trained via gradient norm regularization,
                    randomized smoothing, and adversarial training with projected gradient descent. Quantifying the
                    perceptual alignment of model gradients via their similarity with the gradients of generative
                    models, we show that off-manifold robustness correlates well with perceptual alignment. Finally,
                    based on the levels of on- and off-manifold robustness, we identify three different regimes of
                    robustness that affect both perceptual alignment and model accuracy: weak robustness, bayes-aligned
                    robustness, and excessive robustness. Code …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71491">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-234"></span>

        <script>
        add_bookmark_click(
            71491,
             1,
            'bookmark-number-234',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71491">Approximate Heavy Tails in Offline (Multi-Pass)
                Stochastic Gradient Descent</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Kruno Lehman · Alain Durmus · Umut Simsekli</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71491">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71491" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71491" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71491">
                    Abstract <i id="caret-71491" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71491">
            <div class="abstract-display">
                <p>A recent line of empirical studies has demonstrated that SGD might exhibit a heavy-tailed behavior in
                    practical settings, and the heaviness of the tails might correlate with the overall performance. In
                    this paper, we investigate the emergence of such heavy tails. Previous works on this problem only
                    considered, up to our knowledge, online (also called single-pass) SGD, in which the emergence of
                    heavy tails in theoretical findings is contingent upon access to an infinite amount of data. Hence,
                    the underlying mechanism generating the reported heavy-tailed behavior in practical settings, where
                    the amount of training data is finite, is still not well-understood. Our contribution aims to fill
                    this gap. In particular, we show that the stationary distribution of offline (also called
                    multi-pass) SGD exhibits ‘approximate’ power-law tails and the approximation error is controlled by
                    how fast the empirical distribution of the training data converges to the true underlying data
                    distribution in the Wasserstein metric. Our main takeaway is that, as the number of data points
                    increases, offline SGD will behave increasingly ‘power-law-like’. To achieve this result, we first
                    prove nonasymptotic Wasserstein convergence bounds for offline SGD to online SGD as the number of
                    data points increases, which can be interesting on …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71200">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-235"></span>

        <script>
        add_bookmark_click(
            71200,
             1,
            'bookmark-number-235',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71200">Inference-Time Intervention: Eliciting Truthful
                Answers from a Language Model</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Kenneth Li · Oam Patel · Fernanda Viégas · Hanspeter Pfister · Martin Wattenberg</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71200">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71200-thumb.png?t=1697336788.999372" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71200" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71200" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71200">
                    Abstract <i id="caret-71200" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71200">
            <div class="abstract-display">
                We introduce Inference-Time Intervention (ITI), a technique designed to enhance the "truthfulness" of
                large language models (LLMs). ITI operates by shifting model activations during inference, following a
                learned set of directions across a limited number of attention heads. This intervention significantly
                improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA
                called Alpaca, ITI improves its truthfulness from $32.5\%$ to $65.1\%$. We identify a tradeoff between
                truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI
                is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while
                approaches like RLHF require extensive annotations, ITI locates truthful directions using only few
                hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood
                of something being true, even as they produce falsehoods on the surface.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71188">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-236"></span>

        <script>
        add_bookmark_click(
            71188,
             1,
            'bookmark-number-236',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71188">Tree Variational Autoencoders</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Laura Manduchi · Moritz Vandenhirtz · Alain Ryser · Julia Vogt</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71188">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71188-thumb.png?t=1701853806.627718" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71188" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71188" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71188">
                    Abstract <i id="caret-71188" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71188">
            <div class="abstract-display">
                <p>We propose Tree Variational Autoencoder (TreeVAE), a new generative hierarchical clustering model
                    that learns a flexible tree-based posterior distribution over latent variables. TreeVAE
                    hierarchically divides samples according to their intrinsic characteristics, shedding light on
                    hidden structures in the data. It adapts its architecture to discover the optimal tree for encoding
                    dependencies between latent variables. The proposed tree-based generative architecture enables
                    lightweight conditional inference and improves generative performance by utilizing specialized leaf
                    decoders. We show that TreeVAE uncovers underlying clusters in the data and finds meaningful
                    hierarchical relations between the different groups on a variety of datasets, including real-world
                    imaging data. We present empirically that TreeVAE provides a more competitive log-likelihood lower
                    bound than the sequential counterparts. Finally, due to its generative nature, TreeVAE is able to
                    generate new samples from the discovered clusters via conditional sampling.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70218">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-237"></span>

        <script>
        add_bookmark_click(
            70218,
             1,
            'bookmark-number-237',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70218">Safety Verification of Decision-Tree Policies in
                Continuous Time</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Christian Schilling · Anna Lukina · Emir Demirović · Kim Larsen</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70218">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70218-thumb.png?t=1699542887.975714" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70218" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70218" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70218">
                    Abstract <i id="caret-70218" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70218">
            <div class="abstract-display">
                <p>Decision trees have gained popularity as interpretable surrogate models for learning-based control
                    policies. However, providing safety guarantees for systems controlled by decision trees is an open
                    challenge. We show that the problem is undecidable even for systems with the simplest dynamics, and
                    PSPACE-complete for finite-horizon properties. The latter can be verified for discrete-time systems
                    via bounded model checking. However, for continuous-time systems, such an approach requires
                    discretization, thereby weakening the guarantees for the original system. This paper presents the
                    first algorithm to directly verify decision-tree controlled system in continuous time. The key
                    aspect of our method is exploiting the decision-tree structure to propagate a set-based
                    approximation through the decision nodes. We demonstrate the effectiveness of our approach by
                    verifying safety of several decision trees distilled to imitate neural-network policies for
                    nonlinear systems.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71061">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-238"></span>

        <script>
        add_bookmark_click(
            71061,
             1,
            'bookmark-number-238',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71061">On the Learnability of Multilabel Ranking</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Vinod Raman · UNIQUE SUBEDI · Ambuj Tewari</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71061">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71061" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71061" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71061">
                    Abstract <i id="caret-71061" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71061">
            <div class="abstract-display">
                <p>Multilabel ranking is a central task in machine learning. However, the most fundamental question of
                    learnability in a multilabel ranking setting with relevance-score feedback remains unanswered. In
                    this work, we characterize the learnability of multilabel ranking problems in both batch and online
                    settings for a large family of ranking losses. Along the way, we give two equivalence classes of
                    ranking losses based on learnability that capture most losses used in practice.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71087">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-239"></span>

        <script>
        add_bookmark_click(
            71087,
             1,
            'bookmark-number-239',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71087">Bayesian Extensive-Rank Matrix Factorization with
                Rotational Invariant Priors</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Farzad Pourkamali · Nicolas Macris</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71087">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71087-thumb.png?t=1699377876.3087428" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71087" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71087" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71087">
                    Abstract <i id="caret-71087" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71087">
            <div class="abstract-display">
                <p>We consider a statistical model for matrix factorization in a regime where the rank of the two hidden
                    matrix factors grows linearly with their dimension and their product is corrupted by additive noise.
                    Despite various approaches, statistical and algorithmic limits of such problems have remained
                    elusive. We study a Bayesian setting with the assumptions that (a) one of the matrix factors is
                    symmetric, (b) both factors as well as the additive noise have rotational invariant priors, (c) the
                    priors are known to the statistician. We derive analytical formulas for Rotation Invariant
                    Estimators to reconstruct the two matrix factors, and conjecture that these are optimal in the
                    large-dimension limit, in the sense that they minimize the average mean-square-error. We provide
                    numerical checks which confirm the optimality conjecture when confronted to Oracle Estimators which
                    are optimal by definition, but involve the ground-truth. Our derivation relies on a combination of
                    tools, namely random matrix theory transforms, spherical integral formulas, and the replica method
                    from statistical mechanics.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71135">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-240"></span>

        <script>
        add_bookmark_click(
            71135,
             1,
            'bookmark-number-240',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71135">Contrastive Lift: 3D Object Instance Segmentation
                by Slow-Fast Contrastive Fusion</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yash Bhalgat · Iro Laina · João Henriques · Andrea Vedaldi · Andrew Zisserman</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71135">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71135-thumb.png?t=1701533591.2102373" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71135" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71135" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71135">
                    Abstract <i id="caret-71135" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71135">
            <div class="abstract-display">
                <p>Instance segmentation in 3D is a challenging task due to the lack of large-scale annotated datasets.
                    In this paper, we show that this task can be addressed effectively by leveraging instead 2D
                    pre-trained models for instance segmentation. We propose a novel approach to lift 2D segments to 3D
                    and fuse them by means of a neural field representation, which encourages multi-view consistency
                    across frames. The core of our approach is a slow-fast clustering objective function, which is
                    scalable and well-suited for scenes with a large number of objects. Unlike previous approaches, our
                    method does not require an upper bound on the number of objects or object tracking across frames. To
                    demonstrate the scalability of the slow-fast clustering, we create a new semi-realistic dataset
                    called the Messy Rooms dataset, which features scenes with up to 500 objects per scene. Our approach
                    outperforms the state-of-the-art on challenging scenes from the ScanNet, Hypersim, and Replica
                    datasets, as well as on our newly created Messy Rooms dataset, demonstrating the effectiveness and
                    scalability of our slow-fast clustering method.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71150">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-241"></span>

        <script>
        add_bookmark_click(
            71150,
             1,
            'bookmark-number-241',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71150">Learning Layer-wise Equivariances Automatically
                using Gradients</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Tycho van der Ouderaa · Alexander Immer · Mark van der Wilk</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71150">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71150" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71150" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71150">
                    Abstract <i id="caret-71150" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71150">
            <div class="abstract-display">
                <p>Convolutions encode equivariance symmetries into neural networks leading to better generalisation
                    performance. However, symmetries provide fixed hard constraints on the functions a network can
                    represent, need to be specified in advance, and can not be adapted. Our goal is to allow flexible
                    symmetry constraints that can automatically be learned from data using gradients. Learning symmetry
                    and associated weight connectivity structures from scratch is difficult for two reasons. First, it
                    requires efficient and flexible parameterisations of layer-wise equivariances. Secondly, symmetries
                    act as constraints and are therefore not encouraged by training losses measuring data fit. To
                    overcome these challenges, we improve parameterisations of soft equivariance and learn the amount of
                    equivariance in layers by optimising the marginal likelihood, estimated using differentiable Laplace
                    approximations. The objective balances data fit and model complexity enabling layer-wise symmetry
                    discovery in deep networks. We demonstrate the ability to automatically learn layer-wise
                    equivariances on image classification tasks, achieving equivalent or improved performance over
                    baselines with hard-coded symmetry.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71056">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-242"></span>

        <script>
        add_bookmark_click(
            71056,
             1,
            'bookmark-number-242',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71056">On the Role of Randomization in Adversarially
                Robust Classification</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Lucas Gnecco Heredia · Muni Sreenivas Pydi · Laurent Meunier · Benjamin Negrevergne ·
            Yann Chevaleyre
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71056">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71056-thumb.png?t=1699028372.2105541" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71056" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71056" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71056">
                    Abstract <i id="caret-71056" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71056">
            <div class="abstract-display">
                <p>Deep neural networks are known to be vulnerable to small adversarial perturbations in test data. To
                    defend against adversarial attacks, probabilistic classifiers have been proposed as an alternative
                    to deterministic ones. However, literature has conflicting findings on the effectiveness of
                    probabilistic classifiers in comparison to deterministic ones. In this paper, we clarify the role of
                    randomization in building adversarially robust classifiers.Given a base hypothesis set of
                    deterministic classifiers, we show the conditions under which a randomized ensemble outperforms the
                    hypothesis set in adversarial risk, extending previous results.Additionally, we show that for any
                    probabilistic binary classifier (including randomized ensembles), there exists a deterministic
                    classifier that outperforms it. Finally, we give an explicit description of the deterministic
                    hypothesis set that contains such a deterministic classifier for many types of commonly used
                    probabilistic classifiers, <em>i.e.</em> randomized ensembles and parametric/input noise injection.
                </p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71466">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-243"></span>

        <script>
        add_bookmark_click(
            71466,
             1,
            'bookmark-number-243',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71466">Delegated Classification</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Eden Saig · Inbal Talgam-Cohen · Nir Rosenfeld</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71466">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71466-thumb.png?t=1701793162.1440268" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71466" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71466" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71466">
                    Abstract <i id="caret-71466" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71466">
            <div class="abstract-display">
                <p>When machine learning is outsourced to a rational agent, conflicts of interest might arise and
                    severely impact predictive performance. In this work, we propose a theoretical framework for
                    incentive-aware delegation of machine learning tasks. We model delegation as a principal-agent game,
                    in which accurate learning can be incentivized by the principal using performance-based contracts.
                    Adapting the economic theory of contract design to this setting, we define budget-optimal contracts
                    and prove they take a simple threshold form under reasonable assumptions. In the binary-action case,
                    the optimality of such contracts is shown to be equivalent to the classic Neyman-Pearson lemma,
                    establishing a formal connection between contract design and statistical hypothesis testing.
                    Empirically, we demonstrate that budget-optimal contracts can be constructed using small-scale data,
                    leveraging recent advances in the study of learning curves and scaling laws. Performance and
                    economic outcomes are evaluated using synthetic and real-world classification tasks.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71298">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-244"></span>

        <script>
        add_bookmark_click(
            71298,
             1,
            'bookmark-number-244',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71298">3D-LLM: Injecting the 3D World into Large Language
                Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yining Hong · Haoyu Zhen · Peihao Chen · Shuhong Zheng · Yilun Du · Zhenfang Chen ·
            Chuang Gan
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71298">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71298" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71298" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71298">
                    Abstract <i id="caret-71298" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71298">
            <div class="abstract-display">
                <p>Large language models (LLMs) and Vision-Language Models (VLMs) have been proved to excel at multiple
                    tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the
                    3D physical world, which involves richer concepts such as spatial relationships, affordances,
                    physics, layout, and so on. In this work, we propose to inject the 3D world into large language
                    models, and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds
                    and their features as input and perform a diverse set of 3D-related tasks, including captioning,
                    dense captioning, 3D question answering, task decomposition, 3Dgrounding, 3D-assisted dialog,
                    navigation, and so on. Using three types of prompting mechanisms that we design, we are able to
                    collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first
                    utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we
                    use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism,
                    3D-LLMs could better capture 3D spatial information. Experiments on ScanQA show that our model
                    outperforms state-of-the-art baselines by a large margin (\textit{e.g.}, the BLEU-1 score surpasses
                    state-of-the-art score by 9\%). Furthermore, experiments on our held-in …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71411">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-245"></span>

        <script>
        add_bookmark_click(
            71411,
             1,
            'bookmark-number-245',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71411">Full-Atom Protein Pocket Design via Iterative
                Refinement</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">ZAIXI ZHANG · Zepu Lu · Hao Zhongkai · Marinka Zitnik · Qi Liu</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71411">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71411-thumb.png?t=1699197332.7173243" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71411" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71411" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71411">
                    Abstract <i id="caret-71411" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71411">
            <div class="abstract-display">
                The design of \emph{de novo} functional proteins that bind with specific ligand molecules is crucial in
                various domains like therapeutics and bio-engineering. One vital yet challenging step is to design the
                protein pocket, the cavity region of protein where the ligand binds with. Existing methods suffer from
                inefficient generation, insufficient context modeling (ligand molecule), and incapability of generating
                sidechain atoms. To overcome the limitations, we propose a \textbf{F}ull-\textbf{A}tom
                \textbf{I}terative \textbf{R}efinement framework (\textbf{FAIR}) for protein pocket sequence (i.e.,
                residue types) and 3D structure co-design. Generally, FAIR consists of two steps that follow a
                coarse-to-fine pipeline (backbone atoms to full atoms including sidechain) for full-atom generation. For
                efficiency, all residue types and structures are updated together in each round (i.e., full-shot
                refinement). In the first step, the residue types and backbone coordinates are updated with a
                hierarchical context encoder and two structure refinement modules capturing inter-residue and
                pocket-ligand interactions. The second step further models the sidechain atoms of pockets and updates
                residue types to achieve sequence-structure consistency. The structure of the binding ligand is also
                updated along with the above refinement iterations accounting for its flexibility. Finally, extensive
                evaluations showthat FAIR outperforms baselines in efficiently designing high-quality pocket sequences
                and structures. Specifically, …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71273">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-246"></span>

        <script>
        add_bookmark_click(
            71273,
             1,
            'bookmark-number-246',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71273">Pareto Frontiers in Deep Feature Learning: Data,
                Compute, Width, and Luck</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Benjamin Edelman · Surbhi Goel · Sham Kakade · Eran Malach · Cyril Zhang</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71273">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71273" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71273" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71273">
                    Abstract <i id="caret-71273" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71273">
            <div class="abstract-display">
                <p>In modern deep learning, algorithmic choices (such as width, depth, and learning rate) are known to
                    modulate nuanced resource tradeoffs. This work investigates how these complexities necessarily arise
                    for feature learning in the presence of computational-statistical gaps. We begin by considering
                    offline sparse parity learning, a supervised classification problem which admits a statistical query
                    lower bound for gradient-based training of a multilayer perceptron. This lower bound can be
                    interpreted as a <em>multi-resource tradeoff frontier</em>: successful learning can only occur if
                    one is sufficiently rich (large model), knowledgeable (large dataset), patient (many training
                    iterations), or lucky (many random guesses). We show, theoretically and experimentally, that sparse
                    initialization and increasing network width yield significant improvements in sample efficiency in
                    this setting. Here, width plays the role of parallel search: it amplifies the probability of finding
                    "lottery ticket" neurons, which learn sparse features more sample-efficiently. Finally, we show that
                    the synthetic sparse parity task can be useful as a proxy for real problems requiring axis-aligned
                    feature learning. We demonstrate improved sample efficiency on tabular classification benchmarks by
                    using wide, sparsely-initialized MLP models; these networks sometimes outperform tuned random
                    forests.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71426">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-247"></span>

        <script>
        add_bookmark_click(
            71426,
             1,
            'bookmark-number-247',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71426">Exposing Attention Glitches with Flip-Flop Language
                Modeling</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Bingbin Liu · Jordan Ash · Surbhi Goel · Akshay Krishnamurthy · Cyril Zhang</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71426">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71426" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71426" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71426">
                    Abstract <i id="caret-71426" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71426">
            <div class="abstract-display">
                <p>Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning?
                    The brittleness of these models, particularly when executing long chains of reasoning, currently
                    seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing
                    knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved
                    problem, this work identifies and analyzes the phenomenon of <em>attention glitches</em>, in which
                    the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To
                    isolate the issue, we introduce <em>flip-flop language modeling</em> (FFLM), a parametric family of
                    synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This
                    simple generative task requires a model to copy binary symbols over long-range dependencies,
                    ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic
                    reasoning errors, some of which we can eliminate using various regularization techniques. Our
                    preliminary mechanistic analyses show why the remaining errors may be very difficult to diagnose and
                    resolve. We hypothesize that attention glitches account for (some of) the closed-domain
                    hallucinations in natural LLMs.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72616">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-248"></span>

        <script>
        add_bookmark_click(
            72616,
             1,
            'bookmark-number-248',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72616">Distance-Restricted Folklore Weisfeiler-Leman GNNs
                with Provable Cycle Counting Power</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Junru Zhou · Jiarui Feng · Xiyuan Wang · Muhan Zhang</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72616">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72616-thumb.png?t=1699605844.4612687" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72616" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72616" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72616">
                    Abstract <i id="caret-72616" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72616">
            <div class="abstract-display">
                The ability of graph neural networks (GNNs) to count certain graph substructures, especially cycles, is
                important for the success of GNNs on a wide range of tasks. It has been recently used as a popular
                metric for evaluating the expressive power of GNNs. Many of the proposed GNN models with provable cycle
                counting power are based on subgraph GNNs, i.e., extracting a bag of subgraphs from the input graph,
                generating representations for each subgraph, and using them to augment the representation of the input
                graph. However, those methods require heavy preprocessing, and suffer from high time and memory costs.
                In this paper, we overcome the aforementioned limitations of subgraph GNNs by proposing a novel class of
                GNNs---$d$-Distance-Restricted FWL(2) GNNs, or $d$-DRFWL(2) GNNs, based on the well-known FWL(2)
                algorithm. As a heuristic method for graph isomorphism testing, FWL(2) colors all node pairs in a graph
                and performs message passing among those node pairs. In order to balance the expressive power and
                complexity, $d$-DRFWL(2) GNNs simplify FWL(2) by restricting the range of message passing to node pairs
                whose mutual distances are at most $d$. This way, $d$-DRFWL(2) GNNs exploit graph sparsity while
                avoiding the expensive subgraph extraction operations in subgraph GNNs, making …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71545">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-249"></span>

        <script>
        add_bookmark_click(
            71545,
             1,
            'bookmark-number-249',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71545">The Rashomon Importance Distribution: Getting RID
                of Unstable, Single Model-based Variable Importance</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jon Donnelly · Srikar Katta · Cynthia Rudin · Edward Browne</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71545">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71545" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71545" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71545">
                    Abstract <i id="caret-71545" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71545">
            <div class="abstract-display">
                <p>Quantifying variable importance is essential for answering high-stakes questions in fields like
                    genetics, public policy, and medicine. Current methods generally calculate variable importance for a
                    given model trained on a given dataset. However, for a given dataset, there may be many models that
                    explain the target outcome equally well; without accounting for all possible explanations, different
                    researchers may arrive at many conflicting yet equally valid conclusions given the same data.
                    Additionally, even when accounting for all possible explanations for a given dataset, these insights
                    may not generalize because not all good explanations are stable across reasonable data
                    perturbations. We propose a new variable importance framework that quantifies the importance of a
                    variable across the set of all good models and is stable across the data distribution. Our framework
                    is extremely flexible and can be integrated with most existing model classes and global variable
                    importance metrics. We demonstrate through experiments that our framework recovers variable
                    importance rankings for complex simulation setups where other methods fail. Further, we show that
                    our framework accurately estimates the <em>true importance</em> of a variable for the underlying
                    data distribution. We provide theoretical guarantees on the consistency and finite sample error
                    rates for our estimator. Finally, we …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71636">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-250"></span>

        <script>
        add_bookmark_click(
            71636,
             1,
            'bookmark-number-250',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71636">SwiftSage: A Generative Agent with Fast and Slow
                Thinking for Complex Interactive Tasks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Bill Yuchen Lin · Yicheng Fu · Karina Yang · Faeze Brahman · Shiyu Huang · Chandra
            Bhagavatula · Prithviraj Ammanabrolu · Yejin Choi · Xiang Ren
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71636">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71636" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71636" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71636">
                    Abstract <i id="caret-71636" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71636">
            <div class="abstract-display">
                <p>We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human
                    cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage
                    integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance
                    task completion performance. The framework comprises two primary modules: the Swift module,
                    representing fast and intuitive thinking, and the Sage module, emulating deliberate thought
                    processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action
                    trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding.
                    We develop a heuristic method to harmoniously integrate the two modules, resulting in a more
                    efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage
                    significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its
                    effectiveness in solving complex interactive tasks.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71059">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-251"></span>

        <script>
        add_bookmark_click(
            71059,
             1,
            'bookmark-number-251',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71059">Privacy Assessment on Reconstructed Images: Are
                Existing Evaluation Metrics Faithful to Human Perception?</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Xiaoxiao Sun · Nidham Gazagnadou · Vivek Sharma · Lingjuan Lyu · Hongdong Li · Liang
            Zheng
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71059">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71059-thumb.png?t=1701842943.8731823" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71059" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71059" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71059">
                    Abstract <i id="caret-71059" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71059">
            <div class="abstract-display">
                <p>Hand-crafted image quality metrics, such as PSNR and SSIM, are commonly used to evaluate model
                    privacy risk under reconstruction attacks. Under these metrics, reconstructed images that are
                    determined to resemble the original one generally indicate more privacy leakage. Images determined
                    as overall dissimilar, on the other hand, indicate higher robustness against attack. However, there
                    is no guarantee that these metrics well reflect human opinions, which offers trustworthy judgement
                    for model privacy leakage. In this paper, we comprehensively study the faithfulness of these
                    hand-crafted metrics to human perception of privacy information from the reconstructed images. On 5
                    datasets ranging from natural images, faces, to fine-grained classes, we use 4 existing attack
                    methods to reconstruct images from many different classification models and, for each reconstructed
                    image, we ask multiple human annotators to assess whether this image is recognizable. Our studies
                    reveal that the hand-crafted metrics only have a weak correlation with the human evaluation of
                    privacy leakage and that even these metrics themselves often contradict each other. These
                    observations suggest risks of current metrics in the community. To address this potential risk, we
                    propose a learning-based measure called SemSim to evaluate the Semantic Similarity between the
                    original and reconstructed images. SemSim is …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71406">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-252"></span>

        <script>
        add_bookmark_click(
            71406,
             1,
            'bookmark-number-252',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71406">Max-Margin Token Selection in Attention
                Mechanism</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Davoud Ataee Tarzanagh · Yingcong Li · Xuechen Zhang · Samet Oymak</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71406">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71406" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71406" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71406">
                    Abstract <i id="caret-71406" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71406">
            <div class="abstract-display">
                Attention mechanism is a central component of the transformer architecture which led to the phenomenal
                success of large language models. However, the theoretical principles underlying the attention mechanism
                are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the
                seminal softmax-attention model $f(X)=\langle Xv, \texttt{softmax}(XWp)\rangle$, where $X$ is the token
                sequence and $(v,W,p)$ are trainable parameters. We prove that running gradient descent on $p$, or
                equivalently $W$, converges in direction to a max-margin solution that separates *locally-optimal*
                tokens from non-optimal ones. This clearly formalizes attention as an optimal token selection mechanism.
                Remarkably, our results are applicable to general data and precisely characterize *optimality* of tokens
                in terms of the value embeddings $Xv$ and problem geometry. We also provide a broader regularization
                path analysis that establishes the margin maximizing nature of attention even for nonlinear prediction
                heads. When optimizing $v$ and $p$ simultaneously with logistic loss, we identify conditions under which
                the regularization paths directionally converge to their respective hard-margin SVM solutions where $v$
                separates the input features based on their labels. Interestingly, the SVM formulation of $p$ is
                influenced by the support vector geometry of $v$. Finally, we verify our theoretical findings via
                numerical experiments and provide …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71333">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-253"></span>

        <script>
        add_bookmark_click(
            71333,
             1,
            'bookmark-number-253',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71333">Regularized Behavior Cloning for Blocking the
                Leakage of Past Action Information</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Seokin Seo · HyeongJoo Hwang · Hongseok Yang · Kee-Eung Kim</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71333">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71333-thumb.png?t=1701792694.953788" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71333" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71333" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71333">
                    Abstract <i id="caret-71333" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71333">
            <div class="abstract-display">
                <p>For partially observable environments, imitation learning with observation histories (ILOH) assumes
                    that control-relevant information is sufficiently captured in the observation histories for
                    imitating the expert actions. In the offline setting wherethe agent is required to learn to imitate
                    without interaction with the environment, behavior cloning (BC) has been shown to be a simple yet
                    effective method for imitation learning. However, when the information about the actions executed in
                    the past timesteps leaks into the observation histories, ILOH via BC often ends up imitating its own
                    past actions. In this paper, we address this catastrophic failure by proposing a principled
                    regularization for BC, which we name Past Action Leakage Regularization (PALR). The main idea behind
                    our approach is to leverage the classical notion of conditional independence to mitigate the
                    leakage. We compare different instances of our framework with natural choices of conditional
                    independence metric and its estimator. The result of our comparison advocates the use of a
                    particular kernel-based estimator for the conditional independence metric. We conduct an extensive
                    set of experiments on benchmark datasets in order to assess the effectiveness of our regularization
                    method. The experimental results show that our method significantly outperforms prior related
                    approaches, highlighting its potential to …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71051">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-254"></span>

        <script>
        add_bookmark_click(
            71051,
             1,
            'bookmark-number-254',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71051">On the Gini-impurity Preservation For Privacy
                Random Forests</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">XinRan Xie · Man-Jie Yuan · Xuetong Bai · Wei Gao · Zhi-Hua Zhou</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71051">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71051-thumb.png?t=1701832251.0118012" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71051" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71051" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71051">
                    Abstract <i id="caret-71051" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71051">
            <div class="abstract-display">
                <p>Random forests have been one successful ensemble algorithms in machine learning. Various techniques
                    have been utilized to preserve the privacy of random forests from anonymization, differential
                    privacy, homomorphic encryption, etc., whereas it rarely takes into account some crucial ingredients
                    of learning algorithm. This work presents a new encryption to preserve data's Gini impurity, which
                    plays a crucial role during the construction of random forests. Our basic idea is to modify the
                    structure of binary search tree to store several examples in each node, and encrypt data features by
                    incorporating label and order information. Theoretically, we prove that our scheme preserves the
                    minimum Gini impurity in ciphertexts without decrypting, and present the security guarantee for
                    encryption. For random forests, we encrypt data features based on our Gini-impurity-preserving
                    scheme, and take the homomorphic encryption scheme CKKS to encrypt data labels due to their
                    importance and privacy. We conduct extensive experiments to show the effectiveness, efficiency and
                    security of our proposed method.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71684">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-255"></span>

        <script>
        add_bookmark_click(
            71684,
             1,
            'bookmark-number-255',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71684">Beyond Myopia: Learning from Positive and Unlabeled
                Data through Holistic Predictive Trends</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Wang Xinrui · Wenhai Wan · Chuanxing Geng · Shao-Yuan Li · Songcan Chen</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71684">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71684" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71684" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71684">
                    Abstract <i id="caret-71684" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71684">
            <div class="abstract-display">
                Learning binary classifiers from positive and unlabeled data (PUL) is vital in many real-world
                applications, especially when verifying negative examples is difficult. Despite the impressive empirical
                performance of recent PUL methods, challenges like accumulated errors and increased estimation bias
                persist due to the absence of negative labels. In this paper, we unveil an intriguing yet
                long-overlooked observation in PUL: \textit{resampling the positive data in each training iteration to
                ensure a balanced distribution between positive and unlabeled examples results in strong early-stage
                performance. Furthermore, predictive trends for positive and negative classes display distinctly
                different patterns.} Specifically, the scores (output probability) of unlabeled negative examples
                consistently decrease, while those of unlabeled positive examples show largely chaotic trends. Instead
                of focusing on classification within individual time frames, we innovatively adopt a holistic approach,
                interpreting the scores of each example as a temporal point process (TPP). This reformulates the core
                problem of PUL as recognizing trends in these scores. We then propose a novel TPP-inspired measure for
                trend detection and prove its asymptotic unbiasedness in predicting changes. Notably, our method
                accomplishes PUL without requiring additional parameter tuning or prior assumptions, offering an
                alternative perspective for tackling this problem. Extensive experiments verify the superiority of …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71351">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-256"></span>

        <script>
        add_bookmark_click(
            71351,
             1,
            'bookmark-number-256',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71351">Provable Training for Graph Contrastive
                Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yue Yu · Xiao Wang · Mengmei Zhang · Nian Liu · Chuan Shi</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71351">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71351-thumb.png?t=1697786582.7100928" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71351" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71351" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71351">
                    Abstract <i id="caret-71351" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71351">
            <div class="abstract-display">
                <p>Graph Contrastive Learning (GCL) has emerged as a popular training approach for learning node
                    embeddings from augmented graphs without labels. Despite the key principle that maximizing the
                    similarity between positive node pairs while minimizing it between negative node pairs is well
                    established, some fundamental problems are still unclear. Considering the complex graph structure,
                    are some nodes consistently well-trained and following this principle even with different graph
                    augmentations? Or are there some nodes more likely to be untrained across graph augmentations and
                    violate the principle? How to distinguish these nodes and further guide the training of GCL? To
                    answer these questions, we first present experimental evidence showing that the training of GCL is
                    indeed imbalanced across all nodes. To address this problem, we propose the metric "node
                    compactness", which is the lower bound of how a node follows the GCL principle related to the range
                    of augmentations. We further derive the form of node compactness theoretically through bound
                    propagation, which can be integrated into binary cross-entropy as a regularization. To this end, we
                    propose the PrOvable Training (POT) for GCL, which regularizes the training of GCL to encode node
                    embeddings that follows the GCL principle better. Through extensive experiments on various …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71096">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-257"></span>

        <script>
        add_bookmark_click(
            71096,
             1,
            'bookmark-number-257',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71096">A Scalable Neural Network for DSIC Affine Maximizer
                Auction Design</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zhijian Duan · Haoran Sun · Yurong Chen · Xiaotie Deng</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71096">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71096" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71096" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71096">
                    Abstract <i id="caret-71096" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71096">
            <div class="abstract-display">
                <p>Automated auction design aims to find empirically high-revenue mechanisms through machine learning.
                    Existing works on multi item auction scenarios can be roughly divided into RegretNet-like and affine
                    maximizer auctions (AMAs) approaches. However, the former cannot strictly ensure dominant strategy
                    incentive compatibility (DSIC), while the latter faces scalability issue due to the large number of
                    allocation candidates. To address these limitations, we propose AMenuNet, a scalable neural network
                    that constructs the AMA parameters (even including the allocation menu) from bidder and item
                    representations. AMenuNet is always DSIC and individually rational (IR) due to the properties of
                    AMAs, and it enhances scalability by generating candidate allocations through a neural network.
                    Additionally, AMenuNet is permutation equivariant, and its number of parameters is independent of
                    auction scale. We conduct extensive experiments to demonstrate that AMenuNet outperforms strong
                    baselines in both contextual and non-contextual multi-item auctions, scales well to larger auctions,
                    generalizes well to different settings, and identifies useful deterministic allocations. Overall,
                    our proposed approach offers an effective solution to automated DSIC auction design, with improved
                    scalability and strong revenue performance in various settings.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71520">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-258"></span>

        <script>
        add_bookmark_click(
            71520,
             1,
            'bookmark-number-258',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71520">Can Language Models Solve Graph Problems in Natural
                Language?</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Heng Wang · Shangbin Feng · Tianxing He · Zhaoxuan Tan · Xiaochuang Han · Yulia
            Tsvetkov
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71520">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71520" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71520" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71520">
                    Abstract <i id="caret-71520" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71520">
            <div class="abstract-display">
                <p>Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical
                    structures, such as planning in robotics, multi-hop question answering or knowledge probing,
                    structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these
                    tasks with structure implications, whether LLMs could explicitly process textual descriptions of
                    graphs and structures, map them to grounded conceptual spaces, and perform structured operations
                    remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive
                    benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370
                    problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as
                    connectivity and shortest path up to complex problems such as maximum flow and simulating graph
                    neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph
                    benchmark and find that 1) language models do demonstrate preliminary graph reasoning abilities, 2)
                    the benefit of advanced prompting and in-context learning diminishes on more complex graph problems,
                    while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and
                    problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two
                    instruction-based approaches to enhance LLMs in solving natural language graph problems.
                    Build-a-Graph and …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71068">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-259"></span>

        <script>
        add_bookmark_click(
            71068,
             1,
            'bookmark-number-259',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71068">Group Fairness in Peer Review</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Haris Aziz · Evi Micha · Nisarg Shah</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71068">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71068" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71068" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71068">
                    Abstract <i id="caret-71068" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71068">
            <div class="abstract-display">
                <p>Large conferences such as NeurIPS and AAAI serve as crossroads of various AI fields, since they
                    attract submissions from a vast number of communities. However, in some cases, this has resulted in
                    a poor reviewing experience for some communities, whose submissions get assigned to less qualified
                    reviewers outside of their communities. An often-advocated solution is to break up any such large
                    conference into smaller conferences, but this can lead to isolation of communities and harm
                    interdisciplinary research. We tackle this challenge by introducing a notion of group fairness,
                    called the core, which requires that every possible community (subset of researchers) to be treated
                    in a way that prevents them from unilaterally benefiting by withdrawing from a large conference. We
                    study a simple peer review model, prove that it always admits a reviewing assignment in the core,
                    and design an efficient algorithm to find one such assignment. We use real data from CVPR and ICLR
                    conferences to compare our algorithm to existing reviewing assignment algorithms on a number of
                    metrics.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71358">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-260"></span>

        <script>
        add_bookmark_click(
            71358,
             1,
            'bookmark-number-260',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71358">Aleatoric and Epistemic Discrimination: Fundamental
                Limits of Fairness Interventions</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Hao Wang · Luxi He · Rui Gao · Flavio Calmon</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71358">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71358-thumb.png?t=1701482342.270928" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71358" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71358" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71358">
                    Abstract <i id="caret-71358" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71358">
            <div class="abstract-display">
                <p>Machine learning (ML) models can underperform on certain population groups due to choices made during
                    model development and bias inherent in the data. We categorize sources of discrimination in the ML
                    pipeline into two classes: aleatoric discrimination, which is inherent in the data distribution, and
                    epistemic discrimination, which is due to decisions made during model development. We quantify
                    aleatoric discrimination by determining the performance limits of a model under fairness
                    constraints, assuming perfect knowledge of the data distribution. We demonstrate how to characterize
                    aleatoric discrimination by applying Blackwell's results on comparing statistical experiments. We
                    then quantify epistemic discrimination as the gap between a model's accuracy when fairness
                    constraints are applied and the limit posed by aleatoric discrimination. We apply this approach to
                    benchmark existing fairness interventions and investigate fairness risks in data with missing
                    values. Our results indicate that state-of-the-art fairness interventions are effective at removing
                    epistemic discrimination on standard (overused) tabular datasets. However, when data has missing
                    values, there is still significant room for improvement in handling aleatoric discrimination.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71319">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-261"></span>

        <script>
        add_bookmark_click(
            71319,
             1,
            'bookmark-number-261',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71319">Posterior Contraction Rates for Matérn Gaussian
                Processes on Riemannian Manifolds</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Paul Rosa · Slava Borovitskiy · Alexander Terenin · Judith Rousseau</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71319">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71319-thumb.png?t=1701607020.4607003" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71319" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71319" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71319">
                    Abstract <i id="caret-71319" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71319">
            <div class="abstract-display">
                Gaussian processes are used in many machine learning applications that rely on uncertainty
                quantification. Recently, computational tools for working with these models in geometric settings, such
                as when inputs lie on a Riemannian manifold, have been developed. This raises the question: can these
                intrinsic models be shown theoretically to lead to better performance, compared to simply embedding all
                relevant quantities into $\mathbb{R}^d$ and using the restriction of an ordinary Euclidean Gaussian
                process? To study this, we prove optimal contraction rates for intrinsic Matérn Gaussian processes
                defined on compact Riemannian manifolds. We also prove analogous rates for extrinsic processes using
                trace and extension theorems between manifold and ambient Sobolev spaces: somewhat surprisingly, the
                rates obtained turn out to coincide with those of the intrinsic processes, provided that their
                smoothness parameters are matched appropriately. We illustrate these rates empirically on a number of
                examples, which, mirroring prior work, show that intrinsic processes can achieve better performance in
                practice. Therefore, our work shows that finer-grained analyses are needed to distinguish between
                different levels of data-efficiency of geometric Gaussian processes, particularly in settings which
                involve small data set sizes and non-asymptotic behavior.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71397">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-262"></span>

        <script>
        add_bookmark_click(
            71397,
             1,
            'bookmark-number-262',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71397">Differentiable Registration of Images and LiDAR
                Point Clouds with VoxelPoint-to-Pixel Matching</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Junsheng Zhou · Baorui Ma · Wenyuan Zhang · Yi Fang · Yu-Shen Liu · Zhizhong Han</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71397">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71397-thumb.png?t=1697096520.5700564" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71397" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71397" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71397">
                    Abstract <i id="caret-71397" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71397">
            <div class="abstract-display">
                <p>Cross-modality registration between 2D images captured by cameras and 3D point clouds from LiDARs is
                    a crucial task in computer vision and robotic. Previous methods estimate 2D-3D correspondences by
                    matching point and pixel patterns learned by neural networks, and use Perspective-n-Points (PnP) to
                    estimate rigid transformation during post-processing. However, these methods struggle to map points
                    and pixels to a shared latent space robustly since points and pixels have very different
                    characteristics with patterns learned in different manners (MLP and CNN), and they also fail to
                    construct supervision directly on the transformation since the PnP is non-differentiable, which
                    leads to unstable registration results. To address these problems, we propose to learn a structured
                    cross-modality latent space to represent pixel features and 3D features via a differentiable
                    probabilistic PnP solver. Specifically, we design a triplet network to learn VoxelPoint-to-Pixel
                    matching, where we represent 3D elements using both voxels and points to learn the cross-modality
                    latent space with pixels. We design both the voxel and pixel branch based on CNNs to operate
                    convolutions on voxels/pixels represented in grids, and integrate an additional point branch to
                    regain the information lost during voxelization. We train our framework end-to-end by imposing
                    supervisions directly on the predicted …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71497">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-263"></span>

        <script>
        add_bookmark_click(
            71497,
             1,
            'bookmark-number-263',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71497">Double Gumbel Q-Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">David Yu-Tung Hui · Aaron Courville · Pierre-Luc Bacon</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71497">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71497-thumb.png?t=1699525497.953891" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71497" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71497" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71497">
                    Abstract <i id="caret-71497" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71497">
            <div class="abstract-display">
                <p>We show that Deep Neural Networks introduce two heteroscedastic Gumbel noise sources into Q-Learning.
                    To account for these noise sources, we propose Double Gumbel Q-Learning, a Deep Q-Learning algorithm
                    applicable for both discrete and continuous control. In discrete control, we derive a closed-form
                    expression for the loss function of our algorithm. In continuous control, this loss function is
                    intractable and we therefore derive an approximation with a hyperparameter whose value regulates
                    pessimism in Q-Learning. We present a default value for our pessimism hyperparameter that enables
                    DoubleGum to outperform DDPG, TD3, SAC, XQL, quantile regression, and Mixture-of-Gaussian Critics in
                    aggregate over 33 tasks from DeepMind Control, MuJoCo, MetaWorld, and Box2D and show that tuning
                    this hyperparameter may further improve sample efficiency.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71070">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-264"></span>

        <script>
        add_bookmark_click(
            71070,
             1,
            'bookmark-number-264',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71070">Context-PIPs: Persistent Independent Particles
                Demands Context Features</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Weikang Bian · Zhaoyang Huang · Xiaoyu Shi · Yitong Dong · Yijin Li · Hongsheng Li</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71070">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71070-thumb.png?t=1702008195.937084" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71070" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71070" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71070">
                    Abstract <i id="caret-71070" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71070">
            <div class="abstract-display">
                <p>We tackle the problem of Persistent Independent Particles (PIPs), also called Tracking Any Point
                    (TAP), in videos, which specifically aims at estimating persistent long-term trajectories of query
                    points in videos. Previous methods attempted to estimate these trajectories independently to
                    incorporate longer image sequences, therefore, ignoring the potential benefits of incorporating
                    spatial context features. We argue that independent video point tracking also demands spatial
                    context features.To this end, we propose a novel framework Context-PIPs, which effectively improves
                    point trajectory accuracy by aggregating spatial context features in videos. Context-PIPs contains
                    two main modules: 1) a SOurse Feature Enhancement (SOFE) module, and 2) a TArget Feature Aggregation
                    (TAFA) module. Context-PIPs significantly improves PIPs all-sided, reducing 11.4\% Average
                    Trajectory Error of Occluded Points (ATE-Occ) on CroHD and increasing 11.8\% Average Percentage of
                    Correct Keypoint (A-PCK) on TAP-Vid-Kinectics. Demos are available at
                    \url{https://wkbian.github.io/Projects/Context-PIPs/}.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71338">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-265"></span>

        <script>
        add_bookmark_click(
            71338,
             1,
            'bookmark-number-265',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71338">Structure-free Graph Condensation: From Large-scale
                Graphs to Condensed Graph-free Data</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Xin Zheng · Miao Zhang · Chunyang Chen · Quoc Viet Hung Nguyen · Xingquan Zhu · Shirui
            Pan
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71338">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71338-thumb.png?t=1699489298.8282487" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71338" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71338" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71338">
                    Abstract <i id="caret-71338" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71338">
            <div class="abstract-display">
                <p>Graph condensation, which reduces the size of a large-scale graph by synthesizing a small-scale
                    condensed graph as its substitution, has immediate benefits for various graph learning
                    tasks.However, existing graph condensation methods rely on the joint optimization of nodes and
                    structures in the condensed graph, and overlook critical issues in effectiveness and generalization
                    ability.In this paper, we advocate a new Structure-Free Graph Condensation paradigm, named SFGC, to
                    distill a large-scale graph into a small-scale graph node set without explicit graph structures,
                    i.e., graph-free data.Our idea is to implicitly encode topology structure information into the node
                    attributes in the synthesized graph-free data, whose topology is reduced to an identity
                    matrix.Specifically, SFGC contains two collaborative components: (1) a training trajectory
                    meta-matching scheme for effectively synthesizing small-scale graph-free data;(2) a graph neural
                    feature score metric for dynamically evaluating the quality of the condensed data. Through training
                    trajectory meta-matching, SFGC aligns the long-term GNN learning behaviors between the large-scale
                    graph and the condensed small-scale graph-free data, ensuring comprehensive and compact transfer of
                    informative knowledge to the graph-free data.Afterward, the underlying condensed graph-free data
                    would be dynamically evaluated with the graph neural feature score, which is a closed-form metric
                    for ensuring the excellent expressiveness of the …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71062">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-266"></span>

        <script>
        add_bookmark_click(
            71062,
             1,
            'bookmark-number-266',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71062">Demystifying Softmax Gating Function in Gaussian
                Mixture of Experts</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Huy Nguyen · TrungTin Nguyen · Nhat Ho</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71062">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71062-thumb.png?t=1699221848.8639617" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71062" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71062" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71062">
                    Abstract <i id="caret-71062" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71062">
            <div class="abstract-display">
                <p>Understanding the parameter estimation of softmax gating Gaussian mixture of experts has remained a
                    long-standing open problem in the literature. It is mainly due to three fundamental theoretical
                    challenges associated with the softmax gating function: (i) the identifiability only up to the
                    translation of parameters; (ii) the intrinsic interaction via partial differential equations between
                    the softmax gating and the expert functions in the Gaussian density; (iii) the complex dependence
                    between the numerator and denominator of the conditional density of softmax gating Gaussian mixture
                    of experts. We resolve these challenges by proposing novel Voronoi loss functions among parameters
                    and establishing the convergence rates of maximum likelihood estimator (MLE) for solving parameter
                    estimation in these models. When the true number of experts is unknown and over-specified, our
                    findings show a connection between the convergence rate of the MLE and a solvability problem of a
                    system of polynomial equations.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71166">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-267"></span>

        <script>
        add_bookmark_click(
            71166,
             1,
            'bookmark-number-267',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71166">Convergence of Alternating Gradient Descent for
                Matrix Factorization</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Rachel Ward · Tamara Kolda</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71166">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71166" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71166" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71166">
                    Abstract <i id="caret-71166" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71166">
            <div class="abstract-display">
                We consider alternating gradient descent (AGD) with fixed step size applied to the asymmetric matrix
                factorization objective. We show that, for a rank-$r$ matrix $A \in \mathbb{R}^{m \times n}$, $T = C (
                \frac{\sigma_1(A)}{\sigma_r(A)} )^2 \log(1/\epsilon)$ iterations of alternating gradient descent suffice
                to reach an $\epsilon$-optimal factorization $\| A - X_{T} Y_{T}' \|^2 \leq \epsilon \| A \|^2$ with
                high probability starting from an atypical random initialization. The factors have rank $d \geq r$ so
                that $X_{T}\in \mathbb{R}^{m \times d}$ and $Y_{T} \in\mathbb{R}^{n \times d}$, and mild
                overparameterization suffices for the constant $C$ in the iteration complexity $T$ to be an absolute
                constant. Experiments suggest that our proposed initialization is not merely of theoretical benefit, but
                rather significantly improves the convergence rate of gradient descent in practice. Our proof is
                conceptually simple: a uniform Polyak-Lojasiewicz (PL) inequality and uniform Lipschitz smoothness
                constant are guaranteed for a sufficient number of iterations, starting from our random initialization.
                Our proof method should be useful for extending and simplifying convergence analyses for a broader class
                of nonconvex low-rank factorization problems.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71246">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-268"></span>

        <script>
        add_bookmark_click(
            71246,
             1,
            'bookmark-number-268',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71246">A Graph-Theoretic Framework for Understanding
                Open-World Semi-Supervised Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yiyou Sun · Zhenmei Shi · Yixuan Li</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71246">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71246-thumb.png?t=1699020573.2646656" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71246" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71246" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71246">
                    Abstract <i id="caret-71246" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71246">
            <div class="abstract-display">
                <p>Open-world semi-supervised learning aims at inferring both known and novel classes in unlabeled data,
                    by harnessing prior knowledge from a labeled set with known classes. Despite its importance, there
                    is a lack of theoretical foundations for this problem. This paper bridges the gap by formalizing a
                    graph-theoretic framework tailored for the open-world setting, where the clustering can be
                    theoretically characterized by graph factorization. Our graph-theoretic framework illuminates
                    practical algorithms and provides guarantees. In particular, based on our graph formulation, we
                    apply the algorithm called Spectral Open-world Representation Learning (SORL), and show that
                    minimizing our loss is equivalent to performing spectral decomposition on the graph. Such
                    equivalence allows us to derive a provable error bound on the clustering performance for both known
                    and novel classes, and analyze rigorously when labeled data helps. Empirically, SORL can match or
                    outperform several strong baselines on common benchmark datasets, which is appealing for practical
                    usage while enjoying theoretical guarantees.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71125">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-269"></span>

        <script>
        add_bookmark_click(
            71125,
             1,
            'bookmark-number-269',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71125">Parallel Sampling of Diffusion Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Andy Shih · Suneel Belkhale · Stefano Ermon · Dorsa Sadigh · Nima Anari</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71125">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71125-thumb.png?t=1699553824.8476462" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71125" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71125" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71125">
                    Abstract <i id="caret-71125" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71125">
            <div class="abstract-display">
                <p>Diffusion models are powerful generative models but suffer from slow sampling, often taking 1000
                    sequential denoising steps for one sample. As a result, considerable efforts have been directed
                    toward reducing the number of denoising steps, but these methods hurt sample quality. Instead of
                    reducing the number of denoising steps (trading quality for speed), in this paper we explore an
                    orthogonal approach: can we run the denoising steps in parallel (trading compute for speed)? In
                    spite of the sequential nature of the denoising steps, we show that surprisingly it is possible to
                    parallelize sampling via Picard iterations, by guessing the solution of future denoising steps and
                    iteratively refining until convergence. With this insight, we present ParaDiGMS, a novel method to
                    accelerate the sampling of pretrained diffusion models by denoising multiple steps in parallel.
                    ParaDiGMS is the first diffusion sampling method that enables trading compute for speed and is even
                    compatible with existing fast sampling techniques such as DDIM and DPMSolver. Using ParaDiGMS, we
                    improve sampling speed by 2-4x across a range of robotics and image generation models, giving
                    state-of-the-art sampling speeds of 0.2s on 100-step DiffusionPolicy and 14.6s on 1000-step
                    StableDiffusion-v2 with no measurable degradation of task reward, FID score, or …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71036">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-270"></span>

        <script>
        add_bookmark_click(
            71036,
             1,
            'bookmark-number-270',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71036">Separable Physics-Informed Neural Networks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Junwoo Cho · Seungtae Nam · Hyunmo Yang · Seok-Bae Yun · Youngjoon Hong · Eunbyung
            Park
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71036">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71036-thumb.png?t=1698738386.2725105" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71036" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71036" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71036">
                    Abstract <i id="caret-71036" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71036">
            <div class="abstract-display">
                Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers
                showing encouraging results on various PDEs. However, there is a fundamental limitation of training
                PINNs to solve multi-dimensional PDEs and approximate very complex solution functions.The number of
                training points (collocation points) required on these challenging PDEs grows substantially, and it is
                severely limited due to the expensive computational costs and heavy memory overhead.To overcome this
                limit, we propose a network architecture and training algorithm for PINNs.The proposed method, separable
                PINN (SPINN), operates on a per-axis basis to decrease the number of network propagations in
                multi-dimensional PDEs instead of point-wise processing in conventional PINNs.We also propose using
                forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals,
                enabling a large number of collocation points ($&gt;10^7$) on a single commodity GPU. The experimental
                results show significantly reduced computational costs ($62\times$ in wall-clock time, $1,394\times$ in
                FLOPs given the same number of collocation points) in multi-dimensional PDEs while achieving better
                accuracy.Furthermore, we present that SPINN can solve a chaotic (2+1)-d Navier-Stokes equation much
                faster than the best-performing prior method (9 minutes vs. 10 hours in a single GPU), maintaining
                accuracy.Finally, we showcase that SPINN can accurately obtain the …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71196">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-271"></span>

        <script>
        add_bookmark_click(
            71196,
             1,
            'bookmark-number-271',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71196">A Dynamical System View of Langevin-Based
                Non-Convex Sampling</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Mohammad Reza Karimi Jaghargh · Ya-Ping Hsieh · Andreas Krause</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71196">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71196" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71196" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71196">
                    Abstract <i id="caret-71196" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71196">
            <div class="abstract-display">
                <p>Non-convex sampling is a key challenge in machine learning, central to non-convex optimization in
                    deep learning as well as to approximate probabilistic inference. Despite its significance,
                    theoretically there remain some important challenges: Existing guarantees suffer from the drawback
                    of lacking guarantees for the last-iterates, and little is known beyond the elementary schemes of
                    stochastic gradient Langevin dynamics. To address these issues, we develop a novel framework that
                    lifts the above issues by harnessing several tools from the theory of dynamical systems. Our key
                    result is that, for a large class of state-of-the-art sampling schemes, their last-iterate
                    convergence in Wasserstein distances can be reduced to the study of their continuous-time
                    counterparts, which is much better understood. Coupled with standard assumptions of MCMC sampling,
                    our theory immediately yields the last-iterate Wasserstein convergence of many advanced sampling
                    schemes such as mirror Langevin, proximal, randomized mid-point, and Runge-Kutta methods.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72083">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-272"></span>

        <script>
        add_bookmark_click(
            72083,
             1,
            'bookmark-number-272',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72083">Smoothed Online Learning for Prediction in
                Piecewise Affine Systems</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Adam Block · Max Simchowitz · Russ Tedrake</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72083">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72083-thumb.png?t=1701382372.4644897" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72083" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72083" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72083">
                    Abstract <i id="caret-72083" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72083">
            <div class="abstract-display">
                <p>The problem of piecewise affine (PWA) regression and planning is of foundational importance to the
                    study of online learning, control, and robotics, where it provides a theoretically and empirically
                    tractable setting to study systems undergoing sharp changes in the dynamics. Unfortunately, due to
                    the discontinuities that arise when crossing into different ``pieces,'' learning in general
                    sequential settings is impossible and practical algorithms are forced to resort to heuristic
                    approaches. This paper builds on the recently developed smoothed online learning framework and
                    provides the first algorithms for prediction and simulation in PWA systems whose regret is
                    polynomial in all relevant problem parameters under a weak smoothness assumption; moreover, our
                    algorithms are efficient in the number of calls to an optimization oracle. We further apply our
                    results to the problems of one-step prediction and multi-step simulation regret in piecewise affine
                    dynamical systems, where the learner is tasked with simulating trajectories and regret is measured
                    in terms of the Wasserstein distance between simulated and true data. Along the way, we develop
                    several technical tools of more general interest.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-71007">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-273"></span>

        <script>
        add_bookmark_click(
            71007,
             1,
            'bookmark-number-273',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/71007">Sample Complexity of Forecast Aggregation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Tao Lin · Yiling Chen</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-71007">Thu 14 Dec 12:00 AM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/71007-thumb.png?t=1701408763.5466018" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-71007" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-71007" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-71007">
                    Abstract <i id="caret-71007" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-71007">
            <div class="abstract-display">
                We consider a Bayesian forecast aggregation model where $n$ experts, after observing private signals
                about an unknown binary event, report their posterior beliefs about the event to a principal, who then
                aggregates the reports into a single prediction for the event. The signals of the experts and the
                outcome of the event follow a joint distribution that is unknown to the principal, but the principal has
                access to i.i.d. "samples" from the distribution, where each sample is a tuple of the experts' reports
                (not signals) and the realization of the event. Using these samples, the principal aims to find an
                $\varepsilon$-approximately optimal aggregator, where optimality is measured in terms of the expected
                squared distance between the aggregated prediction and the realization of the event. We show that the
                sample complexity of this problem is at least $\tilde \Omega(m^{n-2} / \varepsilon)$ for arbitrary
                discrete distributions, where $m$ is the size of each expert's signal space. This sample complexity
                grows exponentially in the number of experts $n$. But, if the experts' signals are independent
                conditioned on the realization of the event, then the sample complexity is significantly reduced, to
                $\tilde O(1 / \varepsilon^2)$, which does not depend on $n$. Our results can …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70342">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-274"></span>

        <script>
        add_bookmark_click(
            70342,
             1,
            'bookmark-number-274',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70342">High-Fidelity Audio Compression with Improved
                RVQGAN</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Rithesh Kumar · Prem Seetharaman · Alejandro Luebs · Ishaan Kumar · Kundan Kumar</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70342">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70342" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70342" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70342">
                    Abstract <i id="caret-70342" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70342">
            <div class="abstract-display">
                <p>Language models have been successfully used to model natural signals, such as images, speech, and
                    music. A key component of these models is a high quality neural compression model that can compress
                    high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a
                    high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1
                    KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in
                    high-fidelity audio generation with better vector quantization techniques from the image domain,
                    along with improved adversarial and reconstruction losses. We compress all domains (speech,
                    environment, music, etc.) with a single universal model, making it widely applicable to generative
                    modeling of all audio. We compare with competing audio compression algorithms, and find our method
                    outperforms them significantly. We provide thorough ablations for every design choice, as well as
                    open-source code and trained model weights. We hope our work can lay the foundation for the next
                    generation of high-fidelity audio modeling.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70740">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-275"></span>

        <script>
        add_bookmark_click(
            70740,
             1,
            'bookmark-number-275',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70740">Parallel Submodular Function Minimization</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Deeparnab Chakrabarty · Andrei Graur · Haotian Jiang · Aaron Sidford</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70740">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70740-thumb.png?t=1701999450.0218012" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70740" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70740" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70740">
                    Abstract <i id="caret-70740" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70740">
            <div class="abstract-display">
                We consider the parallel complexity of submodular function minimization (SFM). We provide a pair of
                methods which obtain two new query versus depth trade-offs a submodular function defined on subsets of
                $n$ elements that has integer values between $-M$ and $M$. The first method has depth $2$ and query
                complexity $n^{O(M)}$ and the second method has depth $\widetilde{O}(n^{1/3} M^{2/3})$ and query
                complexity $O(\mathrm{poly}(n, M))$. Despite a line of work on improved parallel lower bounds for SFM,
                prior to our work the only known algorithms for parallel SFM either followed from more general methods
                for sequential SFM or highly-parallel minimization of convex $\ell_2$-Lipschitz functions.
                Interestingly, to obtain our second result we provide the first highly-parallel algorithm for minimizing
                $\ell_\infty$-Lipschitz function over the hypercube which obtains near-optimal depth for obtaining
                constant accuracy.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70349">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-276"></span>

        <script>
        add_bookmark_click(
            70349,
             1,
            'bookmark-number-276',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70349">Parsel🐍: Algorithmic Reasoning with Language Models
                by Composing Decompositions</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Eric Zelikman · Qian Huang · Gabriel Poesia · Noah Goodman · Nick Haber</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70349">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70349-thumb.png?t=1701393594.0275273" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70349" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70349" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70349">
                    Abstract <i id="caret-70349" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70349">
            <div class="abstract-display">
                <p>Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical
                    multi-step reasoning tasks like generating complex programs. For these tasks, humans often start
                    with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a
                    framework enabling automatic implementation and validation of complex algorithms with code LLMs.
                    With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language
                    function descriptions and then search over combinations of possible function implementations using
                    tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including
                    program synthesis and robotic planning. We find that, using Parsel, LLMs solve more
                    competition-level problems in the APPS dataset, resulting in pass rates over 75\% higher than prior
                    results from directly sampling AlphaCode and Codex, while often using a smaller sample budget.
                    Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art
                    pass@1 performance on HumanEval from 67\% to 85\%. We also find that LLM-generated robotic plans
                    using Parsel are more than twice as likely to be considered accurate than directly generated plans.
                    Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for
                    human programmers. We release our code at https://github.com/ezelikman/parsel.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70507">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-277"></span>

        <script>
        add_bookmark_click(
            70507,
             1,
            'bookmark-number-277',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70507">Memory Efficient Optimizers with 4-bit States</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Bingrui Li · Jianfei Chen · Jun Zhu</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70507">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70507-thumb.png?t=1701444083.224655" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70507" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70507" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70507">
                    Abstract <i id="caret-70507" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70507">
            <div class="abstract-display">
                <p>Optimizer states are a major source of memory consumption for training neural networks, limiting the
                    maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit
                    floating points to lower bitwidth is promising to reduce the training memory footprint, while the
                    current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to
                    4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that
                    moments have complicated outlier patterns, that current block-wise quantization cannot accurately
                    approximate. We use a smaller block size and propose to utilize both row-wise and column-wise
                    information for better quantization. We further identify a zero point problem of quantizing the
                    second moment, and solve this problem with a linear quantizer that excludes the zero point. Our
                    4-bit optimizers are evaluated on a wide variety of benchmarks including natural language
                    understanding, machine translation, image classification, and instruction tuning. On all the tasks
                    our optimizers can achieve comparable accuracy with their full-precision counterparts, while
                    enjoying better memory efficiency.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70778">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-278"></span>

        <script>
        add_bookmark_click(
            70778,
             1,
            'bookmark-number-278',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70778">$SE(3)$ Equivariant Convolution and Transformer in
                Ray Space</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yinshuang Xu · Jiahui Lei · Kostas Daniilidis</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70778">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70778-thumb.png?t=1701834127.32113" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70778" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70778" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70778">
                    Abstract <i id="caret-70778" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70778">
            <div class="abstract-display">
                3D reconstruction and novel view rendering can greatly benefit from geometric priors when the input
                views are not sufficient in terms of coverage and inter-view baselines. Deep learning of geometric
                priors from 2D images requires each image to be represented in a $2D$ canonical frame and the prior to
                be learned in a given or learned $3D$ canonical frame. In this paper, given only the relative poses of
                the cameras, we show how to learn priors from multiple views equivariant to coordinate frame
                transformations by proposing an $SE(3)$-equivariant convolution and transformer in the space of rays in
                3D. We model the ray space as a homogeneous space of $SE(3)$ and introduce the $SE(3)$-equivariant
                convolution in ray space. Depending on the output domain of the convolution, we present
                convolution-based $SE(3)$-equivariant maps from ray space to ray space and to $\mathbb{R}^3$. Our
                mathematical framework allows us to go beyond convolution to $SE(3)$-equivariant attention in the ray
                space. We showcase how to tailor and adapt the equivariant convolution and transformer in the tasks of
                equivariant $3D$ reconstruction and equivariant neural rendering from multiple views. We demonstrate
                $SE(3)$-equivariance by obtaining robust results in roto-translated datasets without performing
                transformation augmentation.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70529">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-279"></span>

        <script>
        add_bookmark_click(
            70529,
             1,
            'bookmark-number-279',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70529">Fast Optimal Transport through Sliced Generalized
                Wasserstein Geodesics</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Guillaume Mahey · Laetitia Chapel · Gilles Gasso · Clément Bonet · Nicolas Courty</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70529">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70529-thumb.png?t=1701846204.37939" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70529" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70529" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70529">
                    Abstract <i id="caret-70529" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70529">
            <div class="abstract-display">
                Wasserstein distance (WD) and the associated optimal transport plan have been proven useful in many
                applications where probability measures are at stake. In this paper, we propose a new proxy of the
                squared WD, coined $\textnormal{min-SWGG}$, that is based on the transport map induced by an optimal
                one-dimensional projection of the two input distributions. We draw connections between
                $\textnormal{min-SWGG}$, and Wasserstein generalized geodesics in which the pivot measure is supported
                on a line. We notably provide a new closed form for the exact Wasserstein distance in the particular
                case of one of the distributions supported on a line allowing us to derive a fast computational scheme
                that is amenable to gradient descent optimization. We show that $\textnormal{min-SWGG}$, is an upper
                bound of WD and that it has a complexity similar to as Sliced-Wasserstein, with the additional feature
                of providing an associated transport plan. We also investigate some theoretical properties such as
                metricity, weak convergence, computational and topological properties. Empirical evidences support the
                benefits of $\textnormal{min-SWGG}$, in various contexts, from gradient flows, shape matching and image
                colorization, among others.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70845">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-280"></span>

        <script>
        add_bookmark_click(
            70845,
             1,
            'bookmark-number-280',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70845">Best Arm Identification with Fixed Budget: A Large
                Deviation Perspective</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Po-An Wang · Ruo-Chun Tzeng · Alexandre Proutiere</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70845">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70845" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70845" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70845">
                    Abstract <i id="caret-70845" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70845">
            <div class="abstract-display">
                <p>We consider the problem of identifying the best arm in stochastic Multi-Armed Bandits (MABs) using a
                    fixed sampling budget. Characterizing the minimal instance-specific error probability for this
                    problem constitutes one of the important remaining open problems in MABs. When arms are selected
                    using a static sampling strategy, the error probability decays exponentially with the number of
                    samples at a rate that can be explicitly derived via Large Deviation techniques. Analyzing the
                    performance of algorithms with adaptive sampling strategies is however much more challenging. In
                    this paper, we establish a connection between the Large Deviation Principle (LDP) satisfied by the
                    empirical proportions of arm draws and that satisfied by the empirical arm rewards. This connection
                    holds for any adaptive algorithm, and is leveraged (i) to improve error probability upper bounds of
                    some existing algorithms, such as the celebrated SR (Successive Rejects) algorithm
                    \cite{audibert2010best}, and (ii) to devise and analyze new algorithms. In particular, we present CR
                    (Continuous Rejects), a truly adaptive algorithm that can reject arms in {\it any} round based on
                    the observed empirical gaps between the rewards of various arms. Applying our Large Deviation
                    results, we prove that CR enjoys better performance guarantees than existing algorithms, including
                    SR. Extensive …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70302">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-281"></span>

        <script>
        add_bookmark_click(
            70302,
             1,
            'bookmark-number-281',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70302">DreamHuman: Animatable 3D Avatars from Text</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Nikos Kolotouros · Thiemo Alldieck · Andrei Zanfir · Eduard Bazavan · Mihai Fieraru ·
            Cristian Sminchisescu
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70302">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70302" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70302" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70302">
                    Abstract <i id="caret-70302" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70302">
            <div class="abstract-display">
                <p>We present \emph{DreamHuman}, a method to generate realistic animatable 3D human avatar models
                    entirely from textual descriptions. Recent text-to-3D methods have made considerable strides in
                    generation, but are still lacking in important aspects. Control and often spatial resolution remain
                    limited, existing methods produce fixed rather than 3D human models that can be placed in different
                    poses (i.e. re-posable or animatable), and anthropometric consistency for complex structures like
                    people remains a challenge. \emph{DreamHuman} connects large text-to-image synthesis models, neural
                    radiance fields, and statistical human body models in a novel optimization framework. This makes it
                    possible to generate dynamic 3D human avatars with high-quality textures and learnt per-instance
                    rigid and non rigid geometric deformations. We demonstrate that our method is capable to generate a
                    wide variety of animatable, realistic 3D human models from text. These have diverse appearance,
                    clothing, skin tones and body shapes, and outperform both generic text-to-3D approaches and previous
                    text-based 3D avatar generators in visual fidelity.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70736">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-282"></span>

        <script>
        add_bookmark_click(
            70736,
             1,
            'bookmark-number-282',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70736">Adversarial Training from Mean Field
                Perspective</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Soichiro Kumano · Hiroshi Kera · Toshihiko Yamasaki</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70736">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70736-thumb.png?t=1701184767.7060544" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70736" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70736" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70736">
                    Abstract <i id="caret-70736" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70736">
            <div class="abstract-display">
                Although adversarial training is known to be effective against adversarial examples, training dynamics
                are not well understood. In this study, we present the first theoretical analysis of adversarial
                training in random deep neural networks without any assumptions on data distributions. We introduce a
                new theoretical framework based on mean field theory, which addresses the limitations of existing mean
                field-based approaches. Based on the framework, we derive the (empirically tight) upper bounds of
                $\ell_q$ norm-based adversarial loss with $\ell_p$ norm-based adversarial examples for various values of
                $p$ and $q$. Moreover, we prove that networks without shortcuts are generally not adversarially
                trainable and that adversarial training reduces network capacity. We also show that the network width
                alleviates these issues. Furthermore, the various impacts of input and output dimensions on the upper
                bounds and time evolution of weight variance are presented.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70630">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-283"></span>

        <script>
        add_bookmark_click(
            70630,
             1,
            'bookmark-number-283',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70630">Understanding Multi-phase Optimization Dynamics and
                Rich Nonlinear Behaviors of ReLU Networks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Mingze Wang · Chao Ma</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70630">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70630" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70630" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70630">
                    Abstract <i id="caret-70630" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70630">
            <div class="abstract-display">
                <p>The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. The
                    nonlinearity of models and non-convexity of loss pose significant challenges for theoretical
                    analysis. Therefore, most previous theoretical works on the optimization dynamics of neural networks
                    focus either on local analysis (like the end of training) or approximate linear models (like Neural
                    Tangent Kernel). In this work, we conduct a complete theoretical characterization of the training
                    process of a two-layer ReLU network trained by Gradient Flow on a linearly separable data. In this
                    specific setting, our analysis captures the whole optimization process starting from random
                    initialization to final convergence. Despite the relatively simple model and data that we studied,
                    we reveal four different phases from the whole training process showing a general
                    simplifying-to-complicating learning trend.Specific nonlinear behaviors can also be precisely
                    identified and captured theoretically, such asinitial condensation, saddle-to-plateau dynamics,
                    plateau escape, changes of activation patterns, learning with increasing complexity, etc.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70480">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-284"></span>

        <script>
        add_bookmark_click(
            70480,
             1,
            'bookmark-number-284',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70480">Individual Arbitrariness and Group Fairness</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Carol Long · Hsiang Hsu · Wael Alghamdi · Flavio Calmon</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70480">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70480" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70480" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70480">
                    Abstract <i id="caret-70480" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70480">
            <div class="abstract-display">
                <p>Machine learning tasks may admit multiple competing models that achieve similar performance yet
                    produce conflicting outputs for individual samples---a phenomenon known as predictive multiplicity.
                    We demonstrate that fairness interventions in machine learning optimized solely for group fairness
                    and accuracy can exacerbate predictive multiplicity. Consequently, state-of-the-art fairness
                    interventions can mask high predictive multiplicity behind favorable group fairness and accuracy
                    metrics. We argue that a third axis of ``arbitrariness'' should be considered when deploying models
                    to aid decision-making in applications of individual-level impact.To address this challenge, we
                    propose an ensemble algorithm applicable to any fairness intervention that provably ensures more
                    consistent predictions.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70802">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-285"></span>

        <script>
        add_bookmark_click(
            70802,
             1,
            'bookmark-number-285',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70802">Optimal Guarantees for Algorithmic Reproducibility
                and Gradient Complexity in Convex Optimization</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Liang Zhang · Junchi YANG · Amin Karbasi · Niao He</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70802">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70802-thumb.png?t=1701810331.4679477" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70802" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70802" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70802">
                    Abstract <i id="caret-70802" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70802">
            <div class="abstract-display">
                <p>Algorithmic reproducibility measures the deviation in outputs of machine learning algorithms upon
                    minor changes in the training process. Previous work suggests that first-order methods would need to
                    trade-off convergence rate (gradient complexity) for better reproducibility. In this work, we
                    challenge this perception and demonstrate that both optimal reproducibility and near-optimal
                    convergence guarantees can be achieved for smooth convex minimization and smooth convex-concave
                    minimax problems under various error-prone oracle settings. Particularly, given the inexact
                    initialization oracle, our regularization-based algorithms achieve the best of both worlds --
                    optimal reproducibility and near-optimal gradient complexity -- for minimization and minimax
                    optimization. With the inexact gradient oracle, the near-optimal guarantees also hold for minimax
                    optimization. Additionally, with the stochastic gradient oracle, we show that stochastic gradient
                    descent ascent is optimal in terms of both reproducibility and gradient complexity. We believe our
                    results contribute to an enhanced understanding of the reproducibility-convergence trade-off in the
                    context of convex optimization.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70496">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-286"></span>

        <script>
        add_bookmark_click(
            70496,
             1,
            'bookmark-number-286',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70496">Regret Matching+: (In)Stability and Fast
                Convergence in Games</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Gabriele Farina · Julien Grand-Clément · Christian Kroer · Chung-Wei Lee · Haipeng Luo
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70496">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70496-thumb.png?t=1701378034.9407523" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70496" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70496" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70496">
                    Abstract <i id="caret-70496" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70496">
            <div class="abstract-display">
                Regret Matching$^+$ (RM$^+$) and its variants are important algorithms for solving large-scale
                games.However, a theoretical understanding of their success in practice is still a mystery.Moreover,
                recent advances on fast convergence in games are limited to no-regret algorithms such as online mirror
                descent, which satisfy stability.In this paper, we first give counterexamples showing that RM+ and its
                predictive version can be unstable, which might cause other players to suffer large regret. We then
                provide two fixes: restarting and chopping off the positive orthant that RM$^+$ works in.We show that
                these fixes are sufficient to get $O(T^{1/4})$ individual regret and $O(1)$ social regret in normal-form
                games via RM$^+$ with predictions.We also apply our stabilizing techniques to clairvoyant updates in the
                uncoupled learning setting for RM$^+$ and prove desirable results akin to recent works for Clairvoyant
                online mirror descent. Our experiments show the advantages of our algorithms over vanilla RM$^+$-based
                algorithms in matrix and extensive-form games.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70910">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-287"></span>

        <script>
        add_bookmark_click(
            70910,
             1,
            'bookmark-number-287',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70910">Dynamics of Finite Width Kernel and Prediction
                Fluctuations in Mean Field Neural Networks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Blake Bordelon · Cengiz Pehlevan</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70910">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70910" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70910" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70910">
                    Abstract <i id="caret-70910" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70910">
            <div class="abstract-display">
                We analyze the dynamics of finite width effects in wide but finite feature learning neural networks.
                Starting from a dynamical mean field theory description of infinite width deep neural network kernel and
                prediction dynamics, we provide a characterization of the $\mathcal{O}(1/\sqrt{\text{width}})$
                fluctuations of the DMFT order parameters over random initializations of the network weights. Our
                results, while perturbative in width, unlike prior analyses, are non-perturbative in the strength of
                feature learning. In the lazy limit of network training, all kernels are random but static in time and
                the prediction variance has a universal form. However, in the rich, feature learning regime, the
                fluctuations of the kernels and predictions are dynamically coupled with a variance that can be computed
                self-consistently. In two layer networks, we show how feature learning can dynamically reduce the
                variance of the final tangent kernel and final network predictions. We also show how initialization
                variance can slow down online learning in wide but finite networks. In deeper networks, kernel variance
                can dramatically accumulate through subsequent layers at large feature learning strengths, but feature
                learning continues to improve the signal-to-noise ratio of the feature kernels. In discrete time, we
                demonstrate that large learning rate phenomena such as edge …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70878">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-288"></span>

        <script>
        add_bookmark_click(
            70878,
             1,
            'bookmark-number-288',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70878">Text-to-Image Diffusion Models are Zero Shot
                Classifiers</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Kevin Clark · Priyank Jaini</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70878">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70878" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70878" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70878">
                    Abstract <i id="caret-70878" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70878">
            <div class="abstract-display">
                <p>The excellent generative capabilities of text-to-image diffusion models suggest they learn
                    informative representations of image-text data.However, what knowledge their representations capture
                    is not fully understood, and they have not been thoroughly explored on downstream tasks.We
                    investigate diffusion models by proposing a method for evaluating them as zero-shot classifiers.The
                    key idea is using a diffusion model's ability to denoise a noised image given a text description of
                    a label as a proxy for that label's likelihood.We apply our method to Stable Diffusion and Imagen,
                    using it to probe fine-grained aspects of the models' knowledge and comparing them with CLIP's
                    zero-shot abilities. They perform competitively with CLIP on a wide range of zero-shot image
                    classification datasets. Additionally, they achieve state-of-the-art results on shape/texture bias
                    tests and can successfully perform attribute binding while CLIP cannot.Although generative
                    pre-training is prevalent in NLP, visual foundation models often use other methods such as
                    contrastive learning. Based on our findings, we argue that generative pre-training should be
                    explored as a compelling alternative for vision and vision-language problems.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70292">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-289"></span>

        <script>
        add_bookmark_click(
            70292,
             1,
            'bookmark-number-289',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70292">Reconstructing the Mind's Eye: fMRI-to-Image with
                Contrastive Learning and Diffusion Priors</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Paul Scotti · Atmadeep Banerjee · Jimmie Goode · Stepan Shabalin · Alex Nguyen · ethan
            cohen · Aidan Dempster · Nathalie Verlinde · Elad Yundler · David Weisberg · Kenneth Norman · Tanishq
            Abraham
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70292">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70292" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70292" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70292">
                    Abstract <i id="caret-70292" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70292">
            <div class="abstract-display">
                <p>We present MindEye, a novel fMRI-to-image approach to retrieve and reconstruct viewed images from
                    brain activity. Our model comprises two parallel submodules that are specialized for retrieval
                    (using contrastive learning) and reconstruction (using a diffusion prior). MindEye can map fMRI
                    brain activity to any high dimensional multimodal latent space, like CLIP image space, enabling
                    image reconstruction using generative models that accept embeddings from this latent space. We
                    comprehensively compare our approach with other existing methods, using both qualitative
                    side-by-side comparisons and quantitative evaluations, and show that MindEye achieves
                    state-of-the-art performance in both reconstruction and retrieval tasks. In particular, MindEye can
                    retrieve the exact original image even among highly similar candidates indicating that its brain
                    embeddings retain fine-grained image-specific information. This allows us to accurately retrieve
                    images even from large-scale databases like LAION-5B. We demonstrate through ablations that
                    MindEye's performance improvements over previous methods result from specialized submodules for
                    retrieval and reconstruction, improved training techniques, and training models with orders of
                    magnitude more parameters. Furthermore, we show that MindEye can better preserve low-level image
                    features in the reconstructions by using img2img, with outputs from a separate autoencoder. All code
                    is available on GitHub.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70835">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-290"></span>

        <script>
        add_bookmark_click(
            70835,
             1,
            'bookmark-number-290',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70835">Combating Representation Learning Disparity with
                Geometric Harmonization</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zhihan Zhou · Jiangchao Yao · Feng Hong · Ya Zhang · Bo Han · Yanfeng Wang</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70835">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70835-thumb.png?t=1699436032.899601" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70835" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70835" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70835">
                    Abstract <i id="caret-70835" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70835">
            <div class="abstract-display">
                <p>Self-supervised learning (SSL) as an effective paradigm of representation learning has achieved
                    tremendous success on various curated datasets in diverse scenarios. Nevertheless, when facing the
                    long-tailed distribution in real-world applications, it is still hard for existing methods to
                    capture transferable and robust representation. The attribution is that the vanilla SSL methods that
                    pursue the sample-level uniformity easily leads to representation learning disparity, where head
                    classes with the huge sample number dominate the feature regime but tail classes with the small
                    sample number passively collapse. To address this problem, we propose a novel Geometric
                    Harmonization (GH) method to encourage the category-level uniformity in representation learning,
                    which is more benign to the minority and almost does not hurt the majority under long-tailed
                    distribution. Specially, GH measures the population statistics of the embedding space on top of
                    self-supervised learning, and then infer an fine-grained instance-wise calibration to constrain the
                    space expansion of head classes and avoid the passive collapse of tail classes. Our proposal does
                    not alter the setting of SSL and can be easily integrated into existing methods in a low-cost
                    manner. Extensive results on a range of benchmark datasets show the effectiveness of \methodspace
                    with high tolerance to the distribution skewness.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70970">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-291"></span>

        <script>
        add_bookmark_click(
            70970,
             1,
            'bookmark-number-291',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70970">What Planning Problems Can A Relational Neural
                Network Solve?</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jiayuan Mao · Tomás Lozano-Pérez · Josh Tenenbaum · Leslie Kaelbling</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70970">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70970-thumb.png?t=1701407535.7423246" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70970" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70970" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70970">
                    Abstract <i id="caret-70970" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70970">
            <div class="abstract-display">
                <p>Goal-conditioned policies are generally understood to be "feed-forward" circuits, in the form of
                    neural networks that map from the current state and the goal specification to the next action to
                    take. However, under what circumstances such a policy can be learned and how efficient the policy
                    will be are not well understood. In this paper, we present a circuit complexity analysis for
                    relational neural networks (such as graph neural networks and transformers) representing policies
                    for planning problems, by drawing connections with serialized goal regression search (S-GRS). We
                    show that there are three general classes of planning problems, in terms of the growth of circuit
                    width and depth as a function of the number of objects and planning horizon, providing constructive
                    proofs. We also illustrate the utility of this analysis for designing neural networks for policy
                    learning.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70433">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-292"></span>

        <script>
        add_bookmark_click(
            70433,
             1,
            'bookmark-number-292',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70433">Principle-Driven Self-Alignment of Language Models
                from Scratch with Minimal Human Supervision</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zhiqing Sun · Yikang Shen · Qinhong Zhou · Hongxin Zhang · Zhenfang Chen · David Cox ·
            Yiming Yang · Chuang Gan
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70433">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70433-thumb.png?t=1701988518.3159597" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70433" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70433" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70433">
                    Abstract <i id="caret-70433" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70433">
            <div class="abstract-display">
                <p>Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with
                    human annotations and reinforcement learning from human feedback (RLHF) to align the output of large
                    language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable.
                    However, this dependence can significantly constrain the true potential of AI-assistant agents due
                    to the high cost of obtaining human supervision and the related issues on quality, reliability,
                    diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel
                    approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of
                    LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses
                    four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to
                    augment the prompt diversity; second, we use a small set of human-written principles for AI models
                    to follow, and guide the LLM through in-context learning from demonstrations (of principles
                    application) to produce helpful, ethical, and reliable responses to user's queries; third, we
                    fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model
                    can generate desirable responses for each query directly without the principle set and the
                    demonstrations anymore; and finally, we offer …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70535">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-293"></span>

        <script>
        add_bookmark_click(
            70535,
             1,
            'bookmark-number-293',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70535">Transient Neural Radiance Fields for Lidar View
                Synthesis and 3D Reconstruction</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Anagh Malik · Parsa Mirdehghan · Sotiris Nousias · Kyros Kutulakos · David Lindell</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70535">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70535" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70535" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70535">
                    Abstract <i id="caret-70535" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70535">
            <div class="abstract-display">
                <p>Neural radiance fields (NeRFs) have become a ubiquitous tool for modeling scene appearance and
                    geometry from multiview imagery. Recent work has also begun to explore how to use additional
                    supervision from lidar or depth sensor measurements in the NeRF framework. However, previous
                    lidar-supervised NeRFs focus on rendering conventional camera imagery and use lidar-derived point
                    cloud data as auxiliary supervision; thus, they fail to incorporate the underlying image formation
                    model of the lidar. Here, we propose a novel method for rendering transient NeRFs that take as input
                    the raw, time-resolved photon count histograms measured by a single-photon lidar system, and we seek
                    to render such histograms from novel views. Different from conventional NeRFs, the approach relies
                    on a time-resolved version of the volume rendering equation to render the lidar measurements and
                    capture transient light transport phenomena at picosecond timescales. We evaluate our method on a
                    first-of-its-kind dataset of simulated and captured transient multiview scans from a prototype
                    single-photon lidar. Overall, our work brings NeRFs to a new dimension of imaging at transient
                    timescales, newly enabling rendering of transient imagery from novel views. Additionally, we show
                    that our approach recovers improved geometry and conventional appearance compared to point
                    cloud-based supervision when training …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70932">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-294"></span>

        <script>
        add_bookmark_click(
            70932,
             1,
            'bookmark-number-294',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70932">Model Spider: Learning to Rank Pre-Trained Models
                Efficiently</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yi-Kai Zhang · Ting-Ji Huang · Yao-Xiang Ding · De-Chuan Zhan · Han-Jia Ye</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70932">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70932-thumb.png?t=1702252563.6312394" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70932" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70932" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70932">
                    Abstract <i id="caret-70932" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70932">
            <div class="abstract-display">
                <p>Figuring out which Pre-Trained Model (PTM) from a model zoo fits the target task is essential to take
                    advantage of plentiful model resources. With the availability of numerous heterogeneous PTMs from
                    diverse fields, efficiently selecting the most suitable one is challenging due to the time-consuming
                    costs of carrying out forward or backward passes over all PTMs. In this paper, we propose Model
                    Spider, which tokenizes both PTMs and tasks by summarizing their characteristics into vectors to
                    enable efficient PTM selection. By leveraging the approximated performance of PTMs on a separate set
                    of training tasks, Model Spider learns to construct representation and measure the fitness score
                    between a model-task pair via their representation. The ability to rank relevant PTMs higher than
                    others generalizes to new tasks. With the top-ranked PTM candidates, we further learn to enrich task
                    repr. with their PTM-specific semantics to re-rank the PTMs for better selection. Model Spider
                    balances efficiency and selection ability, making PTM selection like a spider preying on a web.
                    Model Spider exhibits promising performance across diverse model zoos, including visual models and
                    Large Language Models (LLMs). Code is available at https://github.com/zhangyikaii/Model-Spider.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70753">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-295"></span>

        <script>
        add_bookmark_click(
            70753,
             1,
            'bookmark-number-295',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70753">Segment Any Point Cloud Sequences by Distilling
                Vision Foundation Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Youquan Liu · Lingdong Kong · Jun CEN · Runnan Chen · Wenwei Zhang · Liang Pan · Kai
            Chen · Ziwei Liu
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70753">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70753-thumb.png?t=1697488904.5651321" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70753" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70753" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70753">
                    Abstract <i id="caret-70753" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70753">
            <div class="abstract-display">
                <p>Recent advancements in vision foundation models (VFMs) have opened up new possibilities for versatile
                    and efficient visual perception. In this work, we introduce Seal, a novel framework that harnesses
                    VFMs for segmenting diverse automotive point cloud sequences. Seal exhibits three appealing
                    properties: i) Scalability: VFMs are directly distilled into point clouds, obviating the need for
                    annotations in either 2D or 3D during pretraining. ii) Consistency: Spatial and temporal
                    relationships are enforced at both the camera-to-LiDAR and point-to-segment regularization stages,
                    facilitating cross-modal representation learning. iii) Generalizability: Seal enables knowledge
                    transfer in an off-the-shelf manner to downstream tasks involving diverse point clouds, including
                    those from real/synthetic, low/high-resolution, large/small-scale, and clean/corrupted datasets.
                    Extensive experiments conducted on eleven different point cloud datasets showcase the effectiveness
                    and superiority of Seal. Notably, Seal achieves a remarkable 45.0% mIoU on nuScenes after linear
                    probing, surpassing random initialization by 36.9% mIoU and outperforming prior arts by 6.1% mIoU.
                    Moreover, Seal demonstrates significant performance gains over existing methods across 20 different
                    few-shot fine-tuning tasks on all eleven tested point cloud datasets. The code is available at this
                    link.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70651">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-296"></span>

        <script>
        add_bookmark_click(
            70651,
             1,
            'bookmark-number-296',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70651">On Learning Necessary and Sufficient Causal
                Graphs</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Hengrui Cai · Yixin Wang · Michael Jordan · Rui Song</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70651">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70651" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70651" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70651">
                    Abstract <i id="caret-70651" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70651">
            <div class="abstract-display">
                <p>The causal revolution has stimulated interest in understanding complex relationships in various
                    fields. Most of the existing methods aim to discover causal relationships among all variables within
                    a complex large-scale graph. However, in practice, only a small subset of variables in the graph are
                    relevant to the outcomes of interest. Consequently, causal estimation with the full causal
                    graph---particularly given limited data---could lead to numerous <em>falsely discovered,
                        spurious</em> variables that exhibit high correlation with, but exert no causal impact on, the
                    target outcome. In this paper, we propose learning a class of <em>necessary and sufficient causal
                        graphs (NSCG)</em> that exclusively comprises causally relevant variables for an outcome of
                    interest, which we term <em>causal features</em>. The key idea is to employ <em>probabilities of
                        causation</em> to systematically evaluate the importance of features in the causal graph,
                    allowing us to identify a subgraph relevant to the outcome of interest. To learn NSCG from data, we
                    develop a <em>necessary and sufficient causal structural learning (NSCSL)</em> algorithm, by
                    establishing theoretical properties and relationships between probabilities of causation and natural
                    causal effects of features. Across empirical studies of simulated and real data, we demonstrate that
                    NSCSL outperforms existing algorithms and can reveal crucial yeast genes for …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70718">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-297"></span>

        <script>
        add_bookmark_click(
            70718,
             1,
            'bookmark-number-297',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70718">Saddle-to-Saddle Dynamics in Diagonal Linear
                Networks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Scott Pesme · Nicolas Flammarion</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70718">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70718" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70718" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70718">
                    Abstract <i id="caret-70718" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70718">
            <div class="abstract-display">
                In this paper we fully describe the trajectory of gradient flow over $2$-layer diagonal linear networks
                for the regression setting in the limit of vanishing initialisation. We show that the limiting flow
                successively jumps from a saddle of the training loss to another until reaching the minimum
                $\ell_1$-norm solution. We explicitly characterise the visited saddles as well as the jump times through
                a recursive algorithm reminiscent of the LARS algorithm used for computing the Lasso path. Starting from
                the zero vector, coordinates are successively activated until the minimum $\ell_1$-norm solution is
                recovered, revealing an incremental learning. Our proof leverages a convenient arc-length
                time-reparametrisation which enables to keep track of the transitions between the jumps. Our analysis
                requires negligible assumptions on the data, applies to both under and overparametrised settings and
                covers complex cases where there is no monotonicity of the number of active coordinates. We provide
                numerical experiments to support our findings.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70691">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-298"></span>

        <script>
        add_bookmark_click(
            70691,
             1,
            'bookmark-number-298',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70691">Spuriosity Rankings: Sorting Data to Measure and
                Mitigate Biases</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Mazda Moayeri · Wenxiao Wang · Sahil Singla · Soheil Feizi</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70691">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70691" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70691" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70691">
                    Abstract <i id="caret-70691" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70691">
            <div class="abstract-display">
                We present a simple but effective method to measure and mitigate model biases caused by reliance on
                spurious cues. Instead of requiring costly changes to one's data or model training, our method better
                utilizes the data one already has by sorting them. Specifically, we rank images within their classes
                based on spuriosity (the degree to which common spurious cues are present), proxied via deep neural
                features of an interpretable network. With spuriosity rankings, it is easy to identify minority
                subpopulations (i.e. low spuriosity images) and assess model bias as the gap in accuracy between high
                and low spuriosity images. One can even efficiently remove a model's bias at little cost to accuracy by
                finetuning its classification head on low spuriosity images, resulting in fairer treatment of samples
                regardless of spuriosity. We demonstrate our method on ImageNet, annotating $5000$ class-feature
                dependencies ($630$ of which we find to be spurious) and generating a dataset of $325k$ soft
                segmentations for these features along the way. Having computed spuriosity rankings via the identified
                spurious neural features, we assess biases for $89$ diverse models and find that class-wise biases are
                highly correlated across models. Our results suggest that model bias due to spurious feature reliance …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70448">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-299"></span>

        <script>
        add_bookmark_click(
            70448,
             1,
            'bookmark-number-299',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70448">Online (Multinomial) Logistic Bandit: Improved
                Regret and Constant Computation Cost</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yu-Jie Zhang · Masashi Sugiyama</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70448">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70448" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70448" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70448">
                    Abstract <i id="caret-70448" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70448">
            <div class="abstract-display">
                This paper investigates the logistic bandit problem, a variant of the generalized linear bandit model
                that utilizes a logistic model to depict the feedback from an action. While most existing research
                focuses on the binary logistic bandit problem, the multinomial case, which considers more than two
                possible feedback values, offers increased practical relevance and adaptability for use in complex
                decision-making problems such as reinforcement learning. In this paper, we provide an algorithm that
                enjoys both statistical and computational efficiency for the logistic bandit problem. In the binary
                case, our method improves the state-of-the-art binary logistic bandit method by reducing the per-round
                computation cost from $\mathcal{O}(\log T)$ to $\mathcal{O}(1)$ with respect to the time horizon $T$,
                while still preserving the minimax optimal guarantee up to logarithmic factors. In the multinomial case,
                with $K+1$ potential feedback values, our algorithm achieves an $\tilde{\mathcal{O}}(K\sqrt{T})$ regret
                bound with $\mathcal{O}(1)$ computational cost per round. The result not only improves the
                $\tilde{\mathcal{O}}(K\sqrt{\kappa T})$ bound for the best-known tractable algorithm—where the large
                constant $\kappa$ increases exponentially with the diameter of the parameter domain—but also reduces the
                $\mathcal{O}(T)$ computational complexity demanded by the previous method.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70394">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-300"></span>

        <script>
        add_bookmark_click(
            70394,
             1,
            'bookmark-number-300',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70394">ProlificDreamer: High-Fidelity and Diverse
                Text-to-3D Generation with Variational Score Distillation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zhengyi Wang · Cheng Lu · Yikai Wang · Fan Bao · Chongxuan LI · Hang Su · Jun Zhu</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70394">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70394-thumb.png?t=1701261039.0895913" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70394" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70394" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70394">
                    Abstract <i id="caret-70394" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70394">
            <div class="abstract-display">
                Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling
                pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing,
                and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable
                instead of a constant as in SDS and present *variational score distillation* (VSD), a principled
                particle-based variational framework to explain and address the aforementioned issues in text-to-3D
                generation. We show that SDS is a special case of VSD and leads to poor samples with both small and
                large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from
                diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight
                (i.e., 7.5). We further present various improvements in the design space for text-to-3D such as
                distillation time schedule and density initialization, which are orthogonal to the distillation
                algorithm yet not well explored. Our overall approach, dubbed *ProlificDreamer*, can generate high
                rendering resolution (i.e., 512$\times$512) and high-fidelity NeRF with rich structure and complex
                effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are
                meticulously detailed and photo-realistic.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70841">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-301"></span>

        <script>
        add_bookmark_click(
            70841,
             1,
            'bookmark-number-301',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70841">Object-Centric Slot Diffusion</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jindong Jiang · Fei Deng · Gautam Singh · Sungjin Ahn</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70841">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70841-thumb.png?t=1702074223.7558908" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70841" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70841" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70841">
                    Abstract <i id="caret-70841" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70841">
            <div class="abstract-display">
                <p>The recent success of transformer-based image generative models in object-centric learning highlights
                    the importance of powerful image generators for handling complex scenes. However, despite the high
                    expressiveness of diffusion models in image generation, their integration into object-centric
                    learning remains largely unexplored in this domain. In this paper, we explore the feasibility and
                    potential of integrating diffusion models into object-centric learning and investigate the pros and
                    cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual
                    purposes: it is the first object-centric learning model to replace conventional slot decoders with a
                    latent diffusion model conditioned on object slots, and it is also the first unsupervised
                    compositional conditional diffusion model that operates without the need for supervised annotations
                    like text. Through experiments on various object-centric tasks, including the first application of
                    the FFHQ dataset in this field, we demonstrate that LSD significantly outperforms state-of-the-art
                    transformer-based decoders, particularly in more complex scenes, and exhibits superior unsupervised
                    compositional generation quality. In addition, we conduct a preliminary investigation into the
                    integration of pre-trained diffusion models in LSD and demonstrate its effectiveness in real-world
                    image segmentation and generation. Project page is available at
                    https://latentslotdiffusion.github.io</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70324">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-302"></span>

        <script>
        add_bookmark_click(
            70324,
             1,
            'bookmark-number-302',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70324">Grounding Neural Inference with Satisfiability
                Modulo Theories</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zifan Wang · Saranya Vijayakumar · Kaiji Lu · Vijay Ganesh · Somesh Jha · Matt
            Fredrikson
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70324">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70324" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70324" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70324">
                    Abstract <i id="caret-70324" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70324">
            <div class="abstract-display">
                <p>Recent techniques that integrate solver layers into Deep Neural Networks (DNNs) have shown promise in
                    bridging a long-standing gap between inductive learning and symbolic reasoning techniques. In this
                    paper we present a set of techniques for integrating Satisfiability Modulo Theories (SMT) solvers
                    into the forward and backward passes of a deep network layer, called SMTLayer.Using this approach,
                    one can encode rich domain knowledge into the network in the form of mathematical formulas.In the
                    forward pass, the solver uses symbols produced by prior layers, along with these formulas, to
                    construct inferences; in the backward pass, the solver informs updates to the network, driving it
                    towards representations that are compatible with the solver's theory.Notably, the solver need not be
                    differentiable. We implement SMTLayer as a Pytorch module, and our empirical results show that it
                    leads to models that 1) require fewer training samples than conventional models, 2) that are robust
                    to certain types of covariate shift, and 3) that ultimately learn representations that are
                    consistent with symbolic knowledge, and thus naturally interpretable.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70930">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-303"></span>

        <script>
        add_bookmark_click(
            70930,
             1,
            'bookmark-number-303',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70930">Continual Learning for Instruction Following from
                Realtime Feedback</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Alane Suhr · Yoav Artzi</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70930">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70930" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70930" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70930">
                    Abstract <i id="caret-70930" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70930">
            <div class="abstract-display">
                <p>We propose and deploy an approach to continually train an instruction-following agent from feedback
                    provided by users during collaborative interactions. During interaction, human users instruct an
                    agent using natural language, and provide realtime binary feedback as they observe the agent
                    following their instructions. We design a contextual bandit learning approach, converting user
                    feedback to immediate reward. We evaluate through thousands of human-agent interactions,
                    demonstrating 15.4% absolute improvement in instruction execution accuracy over time. We also show
                    our approach is robust to several design variations, and that the feedback signal is roughly
                    equivalent to the learning signal of supervised demonstration data.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70395">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-304"></span>

        <script>
        add_bookmark_click(
            70395,
             1,
            'bookmark-number-304',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70395">Wasserstein Quantum Monte Carlo: A Novel Approach
                for Solving the Quantum Many-Body Schrödinger Equation</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Kirill Neklyudov · Jannes Nys · Luca Thiede · Juan Carrasquilla · Qiang Liu · Max
            Welling · Alireza Makhzani
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70395">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70395" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70395" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70395">
                    Abstract <i id="caret-70395" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70395">
            <div class="abstract-display">
                <p>Solving the quantum many-body Schrödinger equation is a fundamental and challenging problem in the
                    fields of quantum physics, quantum chemistry, and material sciences. One of the common computational
                    approaches to this problem is Quantum Variational Monte Carlo (QVMC), in which ground-state
                    solutions are obtained by minimizing the energy of the system within a restricted family of
                    parameterized wave functions. Deep learning methods partially address the limitations of traditional
                    QVMC by representing a rich family of wave functions in terms of neural networks. However, the
                    optimization objective in QVMC remains notoriously hard to minimize and requires second-order
                    optimization methods such as natural gradient. In this paper, we first reformulate energy functional
                    minimization in the space of Born distributions corresponding to particle-permutation
                    (anti-)symmetric wave functions, rather than the space of wave functions. We then interpret QVMC as
                    the Fisher--Rao gradient flow in this distributional space, followed by a projection step onto the
                    variational manifold. This perspective provides us with a principled framework to derive new QMC
                    algorithms, by endowing the distributional space with better metrics, and following the projected
                    gradient flow induced by those metrics. More specifically, we propose ``Wasserstein Quantum Monte
                    Carlo'' (WQMC), which uses the gradient flow induced by the …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73442">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-305"></span>

        <script>
        add_bookmark_click(
            73442,
             1,
            'bookmark-number-305',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73442">The Harvard USPTO Patent Dataset: A Large-Scale,
                Well-Structured, and Multi-Purpose Corpus of Patent Applications</a>
        </div>
        <div class="type_display_name_virtual_card">Poster</div>
        <div class="author-str">Mirac Suzgun · Luke Melas-Kyriazi · Suproteem Sarkar · Scott D Kominers · Stuart
            Shieber
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73442">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73442" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73442" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73442">
                    Abstract <i id="caret-73442" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73442">
            <div class="abstract-display">
                <p>Innovation is a major driver of economic and social development, and information about many kinds of
                    innovation is embedded in semi-structured data from patents and patent applications. Though the
                    impact and novelty of innovations expressed in patent data are difficult to measure through
                    traditional means, machine learning offers a promising set of techniques for evaluating novelty,
                    summarizing contributions, and embedding semantics. In this paper, we introduce the Harvard USPTO
                    Patent Dataset (HUPD), a large-scale, well-structured, and multi-purpose corpus of English-language
                    patent applications filed to the United States Patent and Trademark Office (USPTO) between 2004 and
                    2018. With more than 4.5 million patent documents, HUPD is two to three times larger than comparable
                    corpora. Unlike other NLP patent datasets, HUPD contains the inventor-submitted versions of patent
                    applications, not the final versions of granted patents, allowing us to study patentability at the
                    time of filing using NLP methods for the first time. It is also novel in its inclusion of rich
                    structured data alongside the text of patent filings: By providing each application’s metadata along
                    with all of its text fields, HUPD enables researchers to perform new sets of NLP tasks that leverage
                    variation in structured covariates. As a case study on …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70318">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-306"></span>

        <script>
        add_bookmark_click(
            70318,
             1,
            'bookmark-number-306',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70318">Adversarial Counterfactual Environment Model
                Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Xiong-Hui Chen · Yang Yu · Zhengmao Zhu · ZhiHua Yu · Chen Zhenjun · Chenghe Wang ·
            Yinan Wu · Rong-Jun Qin · Hongqiu Wu · Ruijin Ding · Huang Fangsheng
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70318">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70318-thumb.png?t=1702123776.8026476" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70318" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70318" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70318">
                    Abstract <i id="caret-70318" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70318">
            <div class="abstract-display">
                <p>An accurate environment dynamics model is crucial for various downstream tasks, such as
                    counterfactual prediction, off-policy evaluation, and offline reinforcement learning. Currently,
                    these models were learned through empirical risk minimization (ERM) by step-wise fitting of
                    historical transition data.However, we first show that, particularly in the sequential
                    decision-making setting, this approach may catastrophically fail to predict counterfactual action
                    effects due to the selection bias of behavior policies during data collection.To tackle this
                    problem, we introduce a novel model-learning objective called adversarial weighted empirical risk
                    minimization (AWRM). AWRM incorporates an adversarial policy that exploits the model to generate a
                    data distribution that weakens the model's prediction accuracy, and subsequently, the model is
                    learned under this adversarial data distribution.We implement a practical algorithm, GALILEO, for
                    AWRM and evaluate it on two synthetic tasks, three continuous-control tasks, and \textit{a
                    real-world application}. The experiments demonstrate that GALILEO can accurately predict
                    counterfactual actions and improve various downstream tasks, including offline policy evaluation and
                    improvement, as well as online decision-making.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70724">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-307"></span>

        <script>
        add_bookmark_click(
            70724,
             1,
            'bookmark-number-307',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70724">AIMS: All-Inclusive Multi-Level Segmentation for
                Anything</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Lu Qi · Jason Kuen · Weidong Guo · Jiuxiang Gu · Zhe Lin · Bo Du · Yu Xu · Ming-Hsuan
            Yang
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70724">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70724-thumb.png?t=1697749405.6474113" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70724" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70724" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70724">
                    Abstract <i id="caret-70724" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70724">
            <div class="abstract-display">
                <p>Despite the progress of image segmentation for accurate visual entity segmentation, completing the
                    diverse requirements of image editing applications for different-level region-of-interest selections
                    remains unsolved. In this paper, we propose a new task, All-Inclusive Multi-Level Segmentation
                    (AIMS), which segments visual regions into three levels: part, entity, and relation (two entities
                    with some semantic relationships). We also build a unified AIMS model through multi-dataset
                    multi-task training to address the two major challenges of annotation inconsistency and task
                    correlation. Specifically, we propose task complementarity, association, and prompt mask encoder for
                    three-level predictions. Extensive experiments demonstrate the effectiveness and generalization
                    capacity of our method compared to other state-of-the-art methods on a single dataset or the
                    concurrent work on segment anything. We will make our code and training model publicly
                    available.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70518">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-308"></span>

        <script>
        add_bookmark_click(
            70518,
             1,
            'bookmark-number-308',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70518">Private Distribution Learning with Public Data: The
                View from Sample Compression</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Shai Ben-David · Alex Bie · Clément L Canonne · Gautam Kamath · Vikrant Singhal</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70518">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70518-thumb.png?t=1702139738.3875268" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70518" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70518" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70518">
                    Abstract <i id="caret-70518" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70518">
            <div class="abstract-display">
                We study the problem of private distribution learning with access to public data. In this setup, which
                we refer to as *public-private learning*, the learner is given public and private samples drawn from an
                unknown distribution $p$ belonging to a class $\mathcal Q$, with the goal of outputting an estimate of
                $p$ while adhering to privacy constraints (here, pure differential privacy) only with respect to the
                private samples. We show that the public-private learnability of a class $\mathcal Q$ is connected to
                the existence of a sample compression scheme for $\mathcal Q$, as well as to an intermediate notion we
                refer to as \emph{list learning}. Leveraging this connection: (1) approximately recovers previous
                results on Gaussians over $\mathbb R^d$; and (2) leads to new ones, including sample complexity upper
                bounds for arbitrary $k$-mixtures of Gaussians over $\mathbb R^d$, results for agnostic and
                distribution-shift resistant learners, as well as closure properties for public-private learnability
                under taking mixtures and products of distributions. Finally, via the connection to list learning, we
                show that for Gaussians in $\mathbb R^d$, at least $d$ public samples are necessary for private
                learnability, which is close to the known upper bound of $d+1$ public samples.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70357">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-309"></span>

        <script>
        add_bookmark_click(
            70357,
             1,
            'bookmark-number-309',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70357">Alleviating the Semantic Gap for Generalized
                fMRI-to-Image Reconstruction</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Tao Fang · Qian Zheng · Gang Pan</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70357">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70357-thumb.png?t=1697534394.2327204" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70357" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70357" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70357">
                    Abstract <i id="caret-70357" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70357">
            <div class="abstract-display">
                <p>Although existing fMRI-to-image reconstruction methods could predict high-quality images, they do not
                    explicitly consider the semantic gap between training and testing data, resulting in reconstruction
                    with unstable and uncertain semantics. This paper addresses the problem of generalized fMRI-to-image
                    reconstruction by explicitly alleviates the semantic gap. Specifically, we leverage the pre-trained
                    CLIP model to map the training data to a compact feature representation, which essentially extends
                    the sparse semantics of training data to dense ones, thus alleviating the semantic gap of the
                    instances nearby known concepts (i.e., inside the training super-classes). Inspired by the robust
                    low-level representation in fMRI data, which could help alleviate the semantic gap for instances
                    that far from the known concepts (i.e., outside the training super-classes), we leverage structural
                    information as a general cue to guide image reconstruction. Further, we quantify the semantic
                    uncertainty based on probability density estimation and achieve Generalized fMRI-to-image
                    reconstruction by adaptively integrating Expanded Semantics and Structural information (GESS) within
                    a diffusion process. Experimental results demonstrate that the proposed GESS model outperforms
                    state-of-the-art methods, and we propose a generalized scenario split strategy to evaluate the
                    advantage of GESS in closing the semantic gap.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-72500">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-310"></span>

        <script>
        add_bookmark_click(
            72500,
             1,
            'bookmark-number-310',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/72500">Selective Amnesia: A Continual Learning Approach to
                Forgetting in Deep Generative Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Alvin Heng · Harold Soh</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-72500">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/72500-thumb.png?t=1701397677.8972805" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-72500" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-72500" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-72500">
                    Abstract <i id="caret-72500" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-72500">
            <div class="abstract-display">
                <p>The recent proliferation of large-scale text-to-image models has led to growing concerns that such
                    models may be misused to generate harmful, misleading, and inappropriate content. Motivated by this
                    issue, we derive a technique inspired by continual learning to selectively forget concepts in
                    pretrained deep generative models. Our method, dubbed Selective Amnesia, enables controllable
                    forgetting where a user can specify how a concept should be forgotten. Selective Amnesia can be
                    applied to conditional variational likelihood models, which encompass a variety of popular deep
                    generative frameworks, including variational autoencoders and large-scale text-to-image diffusion
                    models. Experiments across different models demonstrate that our approach induces forgetting on a
                    variety of concepts, from entire classes in standard datasets to celebrity and nudity prompts in
                    text-to-image models.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70670">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-311"></span>

        <script>
        add_bookmark_click(
            70670,
             1,
            'bookmark-number-311',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70670">Deep Reinforcement Learning with Plasticity
                Injection</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Evgenii Nikishin · Junhyuk Oh · Georg Ostrovski · Clare Lyle · Razvan Pascanu · Will
            Dabney · Andre Barreto
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70670">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70670-thumb.png?t=1701380557.371509" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70670" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70670" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70670">
                    Abstract <i id="caret-70670" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70670">
            <div class="abstract-display">
                <p>A growing body of evidence suggests that neural networks employed in deep reinforcement learning (RL)
                    gradually lose their plasticity, the ability to learn from new data; however, the analysis and
                    mitigation of this phenomenon is hampered by the complex relationship between plasticity,
                    exploration, and performance in RL. This paper introduces plasticity injection, a minimalistic
                    intervention that increases the network plasticity without changing the number of trainable
                    parameters or biasing the predictions. The applications of this intervention are two-fold: first, as
                    a diagnostic tool — if injection increases the performance, we may conclude that an agent's network
                    was losing its plasticity. This tool allows us to identify a subset of Atari environments where the
                    lack of plasticity causes performance plateaus, motivating future studies on understanding and
                    combating plasticity loss. Second, plasticity injection can be used to improve the computational
                    efficiency of RL training if the agent has to re-learn from scratch due to exhausted plasticity or
                    by growing the agent's network dynamically without compromising performance. The results on Atari
                    show that plasticity injection attains stronger performance compared to alternative methods while
                    being computationally efficient.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70765">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-312"></span>

        <script>
        add_bookmark_click(
            70765,
             1,
            'bookmark-number-312',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70765">AMDP: An Adaptive Detection Procedure for False
                Discovery Rate Control in High-Dimensional Mediation Analysis</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jiarong Ding · Xuehu ZHU</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70765">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70765-thumb.png?t=1699604395.726636" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70765" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70765" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70765">
                    Abstract <i id="caret-70765" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70765">
            <div class="abstract-display">
                <p>High-dimensional mediation analysis is often associated with a multiple testing problem for detecting
                    significant mediators. Assessing the uncertainty of this detecting process via false discovery rate
                    (FDR) has garnered great interest. To control the FDR in multiple testing, two essential steps are
                    involved: ranking and selection. Existing approaches either construct p-values without calibration
                    or disregard the joint information across tests, leading to conservation in FDR control or
                    non-optimal ranking rules for multiple hypotheses. In this paper, we develop an adaptive mediation
                    detection procedure (referred to as "AMDP") to identify relevant mediators while asymptotically
                    controlling the FDR in high-dimensional mediation analysis. AMDP produces the optimal rule for
                    ranking hypotheses and proposes a data-driven strategy to determine the threshold for mediator
                    selection. This novel method captures information from the proportions of composite null hypotheses
                    and the distribution of p-values, which turns the high dimensionality into an advantage instead of a
                    limitation. The numerical studies on synthetic and real data sets illustrate the performances of
                    AMDP compared with existing approaches.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70563">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-313"></span>

        <script>
        add_bookmark_click(
            70563,
             1,
            'bookmark-number-313',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70563">Learning List-Level Domain-Invariant
                Representations for Ranking</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Ruicheng Xian · Honglei Zhuang · Zhen Qin · Hamed Zamani · Jing Lu · Ji Ma · Kai Hui ·
            Han Zhao · Xuanhui Wang · Michael Bendersky
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70563">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70563-thumb.png?t=1696600259.0375066" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70563" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70563" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70563">
                    Abstract <i id="caret-70563" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70563">
            <div class="abstract-display">
                <p>Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to
                    (low-resource) target domains, and a popular method is invariant representation learning, which
                    matches and aligns the data distributions on the feature space. Although this method is studied
                    extensively and applied on classification and regression problems, its adoption on ranking problems
                    is sporadic, and the few existing implementations lack theoretical justifications. This paper
                    revisits invariant representation learning for ranking. Upon reviewing prior work, we found that
                    they implement what we call item-level alignment, which aligns the distributions of the items being
                    ranked from all lists in aggregate but ignores their list structure. However, the list structure
                    should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are
                    defined and computed on lists, not the items by themselves. To close this discrepancy, we propose
                    list-level alignment—learning domain-invariant representations at the higher level of lists. The
                    benefits are twofold: it leads to the first domain adaptation generalization bound for ranking, in
                    turn providing theoretical support for the proposed method, and it achieves better empirical
                    transfer performance for unsupervised domain adaptation on ranking tasks, including passage
                    reranking.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-73651">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-314"></span>

        <script>
        add_bookmark_click(
            73651,
             1,
            'bookmark-number-314',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/73651">Video Timeline Modeling For News Story
                Understanding</a>
        </div>
        <div class="type_display_name_virtual_card">Poster</div>
        <div class="author-str">Meng Liu · Mingda Zhang · Jialu Liu · Hanjun Dai · Ming-Hsuan Yang · Shuiwang Ji ·
            Zheyun Feng · Boqing Gong
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-73651">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-73651" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-73651" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-73651">
                    Abstract <i id="caret-73651" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-73651">
            <div class="abstract-display">
                In this paper, we present a novel problem, namely video timeline modeling. Our objective is to create a
                video-associated timeline from a set of videos related to a specific topic, thereby facilitating the
                content and structure understanding of the story being told. This problem has significant potential in
                various real-world applications, for instance, news story summarization. To bootstrap research in this
                area, we curate a realistic benchmark dataset, YouTube-News-Timeline, consisting of over $12$k timelines
                and $300$k YouTube news videos. Additionally, we propose a set of quantitative metrics to
                comprehensively evaluate and compare methodologies. With such a testbed, we further develop and
                benchmark several deep learning approaches to tackling this problem. We anticipate that this exploratory
                work will pave the way for further research in video timeline modeling. The assets are available via
                https://github.com/google-research/google-research/tree/master/video_timeline_modeling.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70417">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-315"></span>

        <script>
        add_bookmark_click(
            70417,
             1,
            'bookmark-number-315',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70417">Kernelized Cumulants: Beyond Kernel Mean
                Embeddings</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Patric Bonnier · Harald Oberhauser · Zoltan Szabo</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70417">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70417" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70417" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70417">
                    Abstract <i id="caret-70417" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70417">
            <div class="abstract-display">
                In $\mathbb{R}^d$, it is well-known that cumulants provide an alternative to moments that can achieve
                the same goals with numerous benefits such as lower variance estimators. In this paper we extend
                cumulants to reproducing kernel Hilbert spaces (RKHS) using tools from tensor algebras and show that
                they are computationally tractable by a kernel trick. These kernelized cumulants provide a new set of
                all-purpose statistics; the classical maximum mean discrepancy and Hilbert-Schmidt independence
                criterion arise as the degree one objects in our general construction. We argue both theoretically and
                empirically (on synthetic, environmental, and traffic data analysis) that going beyond degree one has
                several advantages and can be achieved with the same computational complexity and minimal overhead in
                our experiments.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70303">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-316"></span>

        <script>
        add_bookmark_click(
            70303,
             1,
            'bookmark-number-316',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70303">Counterfactual Evaluation of Peer-Review Assignment
                Policies</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Martin Saveski · Steven Jecmen · Nihar Shah · Johan Ugander</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70303">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70303-thumb.png?t=1702168585.028972" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70303" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70303" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70303">
                    Abstract <i id="caret-70303" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70303">
            <div class="abstract-display">
                <p>Peer review assignment algorithms aim to match research papers to suitable expert reviewers, working
                    to maximize the quality of the resulting reviews. A key challenge in designing effective assignment
                    policies is evaluating how changes to the assignment algorithm map to changes in review quality. In
                    this work, we leverage recently proposed policies that introduce randomness in peer-review
                    assignment—in order to mitigate fraud—as a valuable opportunity to evaluate counterfactual
                    assignment policies. Specifically, we exploit how such randomized assignments provide a positive
                    probability of observing the reviews of many assignment policies of interest. To address challenges
                    in applying standard off-policy evaluation methods, such as violations of positivity, we introduce
                    novel methods for partial identification based on monotonicity and Lipschitz smoothness assumptions
                    for the mapping between reviewer-paper covariates and outcomes. We apply our methods to peer-review
                    data from two computer science venues: the TPDP'21 workshop (95 papers and 35 reviewers) and the
                    AAAI'22 conference (8,450 papers and 3,145 reviewers). We consider estimates of (i) the effect on
                    review quality when changing weights in the assignment algorithm, e.g., weighting reviewers' bids
                    vs. textual similarity (between the review's past papers and the submission), and (ii) the "cost of
                    randomization", capturing the difference in expected quality …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70954">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-317"></span>

        <script>
        add_bookmark_click(
            70954,
             1,
            'bookmark-number-317',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70954">Inferring the Future by Imagining the Past</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Kartik Chandra · Tony Chen · Tzu-Mao Li · Jonathan Ragan-Kelley · Josh Tenenbaum</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70954">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70954-thumb.png?t=1701377163.581385" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70954" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70954" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70954">
                    Abstract <i id="caret-70954" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70954">
            <div class="abstract-display">
                <p>A single panel of a comic book can say a lot: it can depict not only where the characters currently
                    are, but also their motions, their motivations, their emotions, and what they might do next. More
                    generally, humans routinely infer complex sequences of past and future events from a <em>static
                        snapshot</em> of a <em>dynamic scene</em>, even in situations they have never seen before.In
                    this paper, we model how humans make such rapid and flexible inferences. Building on a long line of
                    work in cognitive science, we offer a Monte Carlo algorithm whose inferences correlate well with
                    human intuitions in a wide variety of domains, while only using a small, cognitively-plausible
                    number of samples. Our key technical insight is a surprising connection between our inference
                    problem and Monte Carlo path tracing, which allows us to apply decades of ideas from the computer
                    graphics community to this seemingly-unrelated theory of mind task.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70435">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-318"></span>

        <script>
        add_bookmark_click(
            70435,
             1,
            'bookmark-number-318',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70435">Temperature Balancing, Layer-wise Weight Analysis,
                and Neural Network Training</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Yefan Zhou · TIANYU PANG · Keqin Liu · charles martin · Michael Mahoney · Yaoqing Yang
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70435">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70435-thumb.png?t=1701658546.2325406" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70435" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70435" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70435">
                    Abstract <i id="caret-70435" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70435">
            <div class="abstract-display">
                <p>Regularization in modern machine learning is crucial, and it can take various forms in algorithmic
                    design: training set, model family, error function, regularization terms, and optimizations. In
                    particular, the learning rate, which can be interpreted as a temperature-like parameter within the
                    statistical mechanics of learning, plays a crucial role in neural network training. Indeed, many
                    widely adopted training strategies basically just define the decay of the learning rate over time.
                    This process can be interpreted as decreasing a temperature, using either a global learning rate
                    (for the entire model) or a learning rate that varies for each parameter. This paper proposes
                    TempBalance, a straightforward yet effective layer-wise learning rate method. TempBalance is based
                    on Heavy-Tailed Self-Regularization (HT-SR) Theory, an approach which characterizes the implicit
                    self-regularization of different layers in trained models. We demonstrate the efficacy of using
                    HT-SR-motivated metrics to guide the scheduling and balancing of temperature across all network
                    layers during model training, resulting in improved performance during testing. We implement
                    TempBalance on CIFAR10, CIFAR100, SVHN, and TinyImageNet datasets using ResNets, VGGs and
                    WideResNets with various depths and widths. Our results show that TempBalance significantly
                    outperforms ordinary SGD and carefully-tuned spectral norm regularization. We also show that
                    TempBalance outperforms …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70886">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-319"></span>

        <script>
        add_bookmark_click(
            70886,
             1,
            'bookmark-number-319',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70886">Efficient Adversarial Contrastive Learning via
                Robustness-Aware Coreset Selection</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Xilie Xu · Jingfeng ZHANG · Feng Liu · Masashi Sugiyama · Mohan Kankanhalli</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70886">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70886" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70886" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70886">
                    Abstract <i id="caret-70886" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70886">
            <div class="abstract-display">
                <p>Adversarial contrastive learning (ACL) does not require expensive data annotations but outputs a
                    robust representation that withstands adversarial attacks and also generalizes to a wide range of
                    downstream tasks. However, ACL needs tremendous running time to generate the adversarial variants of
                    all training data, which limits its scalability to large datasets. To speed up ACL, this paper
                    proposes a robustness-aware coreset selection (RCS) method. RCS does not require label information
                    and searches for an informative subset that minimizes a representational divergence, which is the
                    distance of the representation between natural data and their virtual adversarial variants. The
                    vanilla solution of RCS via traversing all possible subsets is computationally prohibitive.
                    Therefore, we theoretically transform RCS into a surrogate problem of submodular maximization, of
                    which the greedy search is an efficient solution with an optimality guarantee for the original
                    problem. Empirically, our comprehensive results corroborate that RCS can speed up ACL by a large
                    margin without significantly hurting the robustness transferability. Notably, to the best of our
                    knowledge, we are the first to conduct ACL efficiently on the large-scale ImageNet-1K dataset to
                    obtain an effective robust representation via RCS. Our source code is at
                    https://github.com/GodXuxilie/Efficient<em>ACL</em>via_RCS.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70791">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-320"></span>

        <script>
        add_bookmark_click(
            70791,
             1,
            'bookmark-number-320',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70791">Unified Embedding: Battle-Tested Feature
                Representations for Web-Scale ML Systems</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Benjamin Coleman · Wang-Cheng Kang · Matthew Fahrbach · Ruoxi Wang · Lichan Hong · Ed
            Chi · Derek Cheng
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70791">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70791-thumb.png?t=1702226561.1894503" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70791" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70791" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70791">
                    Abstract <i id="caret-70791" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70791">
            <div class="abstract-display">
                Learning high-quality feature embeddings efficiently and effectively is critical for the performance of
                web-scale machine learning systems. A typical model ingests hundreds of features with vocabularies on
                the order of millions to billions of tokens. The standard approach is to represent each feature value as
                a $d$-dimensional embedding, which introduces hundreds of billions of parameters for extremely
                high-cardinality features. This bottleneck has led to substantial progress in alternative embedding
                algorithms. Many of these methods, however, make the assumption that each feature uses an independent
                embedding table. This work introduces a simple yet highly effective framework, Feature Multiplexing,
                where one single representation space is used for many different categorical features. Our theoretical
                and empirical analysis reveals that multiplexed embeddings can be decomposed into components from each
                constituent feature, allowing models to distinguish between features. We show that multiplexed
                representations give Pareto-optimal space-accuracy tradeoffs for three public benchmark datasets.
                Further, we propose a highly practical approach called Unified Embedding with three major benefits:
                simplified feature configuration, strong adaptation to dynamic data distributions, and compatibility
                with modern hardware. Unified embedding gives significant improvements in offline and online metrics
                compared to highly competitive baselines across five web-scale search, ads, and recommender systems,
                where it …
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70822">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-321"></span>

        <script>
        add_bookmark_click(
            70822,
             1,
            'bookmark-number-321',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70822">Diffusion with Forward Models: Solving Stochastic
                Inverse Problems Without Direct Supervision</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Ayush Tewari · Tianwei Yin · George Cazenavette · Semon Rezchikov · Josh Tenenbaum ·
            Fredo Durand · Bill Freeman · Vincent Sitzmann
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70822">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70822" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70822" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70822">
                    Abstract <i id="caret-70822" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70822">
            <div class="abstract-display">
                <p>Denoising diffusion models are a powerful type of generative models used to capture complex
                    distributions of real-world signals. However, their applicability is limited to scenarios where
                    training samples are readily available, which is not always the case in real-world applications. For
                    example, in inverse graphics, the goal is to generate samples from a distribution of 3D scenes that
                    align with a given image, but ground-truth 3D scenes are unavailable and only 2D images are
                    accessible. To address this limitation, we propose a novel class of denoising diffusion
                    probabilistic models that learn to sample from distributions of signals that are never directly
                    observed. Instead, these signals are measured indirectly through a known differentiable forward
                    model, which produces partial observations of the unknown signal. Our approach involves integrating
                    the forward model directly into the denoising process. A key contribution of our work is the
                    integration of a differentiable forward model into the denoising process. This integration
                    effectively connects the generative modeling of observations with the generative modeling of the
                    underlying signals, allowing for end-to-end training of a conditional generative model over signals.
                    During inference, our approach enables sampling from the distribution of underlying signals that are
                    consistent with a given partial observation. …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70274">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-322"></span>

        <script>
        add_bookmark_click(
            70274,
             1,
            'bookmark-number-322',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70274">Error Bounds for Learning with Vector-Valued Random
                Features</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Samuel Lanthaler · Nicholas H. Nelsen</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70274">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70274-thumb.png?t=1698980154.3902972" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70274" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70274" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70274">
                    Abstract <i id="caret-70274" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70274">
            <div class="abstract-display">
                <p>This paper provides a comprehensive error analysis of learning with vector-valued random features
                    (RF). The theory is developed for RF ridge regression in a fully general infinite-dimensional
                    input-output setting, but nonetheless applies to and improves existing finite-dimensional analyses.
                    In contrast to comparable work in the literature, the approach proposed here relies on a direct
                    analysis of the underlying risk functional and completely avoids the explicit RF ridge regression
                    solution formula in terms of random matrices. This removes the need for concentration results in
                    random matrix theory or their generalizations to random operators. The main results established in
                    this paper include strong consistency of vector-valued RF estimators under model misspecification
                    and minimax optimal convergence rates in the well-specified setting. The parameter complexity
                    (number of random features) and sample complexity (number of labeled data) required to achieve such
                    rates are comparable with Monte Carlo intuition and free from logarithmic factors.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70864">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-323"></span>

        <script>
        add_bookmark_click(
            70864,
             1,
            'bookmark-number-323',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70864">Theoretical and Practical Perspectives on what
                Influence Functions Do</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Andrea Schioppa · Katja Filippova · Ivan Titov · Polina Zablotskaia</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70864">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70864-thumb.png?t=1701888136.3790941" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70864" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70864" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70864">
                    Abstract <i id="caret-70864" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70864">
            <div class="abstract-display">
                <p>Influence functions (IF) have been seen as a technique for explaining model predictions through the
                    lens of the training data. Their utility is assumed to be in identifying training examples
                    "responsible" for a prediction so that, for example, correcting a prediction is possible by
                    intervening on those examples (removing or editing them) and retraining the model. However, recent
                    empirical studies have shown that the existing methods of estimating IF predict the
                    leave-one-out-and-retrain effect poorly. In order to understand the mismatch between the theoretical
                    promise and the practical results, we analyse five assumptions made by IF methods which are
                    problematic for modern-scale deep neural networks and which concern convexity, numeric stability,
                    training trajectory and parameter divergence. This allows us to clarify what can be expected
                    theoretically from IF. We show that while most assumptions can be addressed successfully, the
                    parameter divergence poses a clear limitation on the predictive power of IF: influence fades over
                    training time even with deterministic training. We illustrate this theoretical result with BERT and
                    ResNet models.Another conclusion from the theoretical analysis is that IF are still useful for model
                    debugging and correcting even though some of the assumptions made in prior work do not hold: using
                    natural …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70553">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-324"></span>

        <script>
        add_bookmark_click(
            70553,
             1,
            'bookmark-number-324',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70553">Scaling Open-Vocabulary Object Detection</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Matthias Minderer · Alexey Gritsenko · Neil Houlsby</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70553">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70553-thumb.png?t=1701429363.1216366" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70553" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70553" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70553">
                    Abstract <i id="caret-70553" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70553">
            <div class="abstract-display">
                <p>Open-vocabulary object detection has benefited greatly from pretrained vision-language models, but is
                    still limited by the amount of available detection training data. While detection training data can
                    be expanded by using Web image-text pairs as weak supervision, this has not been done at scales
                    comparable to image-level pretraining. Here, we scale up detection data with self-training, which
                    uses an existing detector to generate pseudo-box annotations on image-text pairs. Major challenges
                    in scaling self-training are the choice of label space, pseudo-annotation filtering, and training
                    efficiency. We present the OWLv2 model and OWL-ST self-training recipe, which address these
                    challenges. OWLv2 surpasses the performance of previous state-of-the-art open-vocabulary detectors
                    already at comparable training scales (~10M examples). However, with OWL-ST, we can scale to over 1B
                    examples, yielding further large improvement: With an L/14 architecture, OWL-ST improves AP on LVIS
                    rare classes, for which the model has seen no human box annotations, from 31.2% to 44.6% (43%
                    relative improvement). OWL-ST unlocks Web-scale training for open-world localization, similar to
                    what has been seen for image classification and language modelling. Code and checkpoints are
                    available on GitHub.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70454">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-325"></span>

        <script>
        add_bookmark_click(
            70454,
             1,
            'bookmark-number-325',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70454">Follow-ups Also Matter: Improving Contextual
                Bandits via Post-serving Contexts</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Chaoqi Wang · Ziyu Ye · Zhe Feng · Ashwinkumar Badanidiyuru Varadaraja · Haifeng Xu
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70454">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70454-thumb.png?t=1697784159.6259408" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70454" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70454" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70454">
                    Abstract <i id="caret-70454" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70454">
            <div class="abstract-display">
                <p>Standard contextual bandit problem assumes that all the relevant contexts are observed before the
                    algorithm chooses an arm. This modeling paradigm, while useful, often falls short when dealing with
                    problems in which additional valuable contexts can be observed after arm selection. For example,
                    content recommendation platforms like Youtube, Instagram, Tiktok receive much additional features
                    about a user's reward after the user clicks a content (e.g., how long the user stayed, what is the
                    user's watch speed, etc.). To improve online learning efficiency in these applications, we study a
                    novel contextual bandit problem with post-serving contexts and design a new algorithm, poLinUCB,
                    that achieves tight regret under standard assumptions. Core to our technical proof is a robustified
                    and generalized version of the well-known Elliptical Potential Lemma (EPL), which can accommodate
                    noise in data. Such robustification is necessary for tackling our problem, though we believe it
                    could also be of general interest.Extensive empirical tests on both synthetic and real-world
                    datasets demonstrate the significant benefit of utilitzing post-serving contexts as well as the
                    superior performance of our algorithm over the state-of-the-art approaches.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70656">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-326"></span>

        <script>
        add_bookmark_click(
            70656,
             1,
            'bookmark-number-326',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70656">Coherent Soft Imitation Learning</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Joe Watson · Sandy Huang · Nicolas Heess</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70656">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70656-thumb.png?t=1701593723.1852844" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70656" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70656" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70656">
                    Abstract <i id="caret-70656" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70656">
            <div class="abstract-display">
                <p>Imitation learning methods seek to learn from an expert either through behavioral cloning (BC) for
                    the policy or inverse reinforcement learning (IRL) for the reward.Such methods enable agents to
                    learn complex tasks from humans that are difficult to capture with hand-designed reward
                    functions.Choosing between BC or IRL for imitation depends on the quality and state-action coverage
                    of the demonstrations, as well as additional access to the Markov decision process. Hybrid
                    strategies that combine BC and IRL are rare, as initial policy optimization against inaccurate
                    rewards diminishes the benefit of pretraining the policy with BC.Our work derives an imitation
                    method that captures the strengths of both BC and IRL.In the entropy-regularized (`soft')
                    reinforcement learning setting, we show that the behavioral-cloned policy can be used as both a
                    shaped reward and a critic hypothesis space by inverting the regularized policy update. This
                    coherency facilitates fine-tuning cloned policies using the reward estimate and additional
                    interactions with the environment.This approach conveniently achieves imitation learning through
                    initial behavioral cloning and subsequent refinement via RL with online or offline data sources.The
                    simplicity of the approach enables graceful scaling to high-dimensional and vision-based tasks, with
                    stable learning and minimal hyperparameter tuning, in contrast to adversarial approaches.For the
                    …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70588">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-327"></span>

        <script>
        add_bookmark_click(
            70588,
             1,
            'bookmark-number-327',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70588">DoReMi: Optimizing Data Mixtures Speeds Up Language
                Model Pretraining</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Sang Michael Xie · Hieu Pham · Xuanyi Dong · Nan Du · Hanxiao Liu · Yifeng Lu · Percy
            Liang · Quoc V Le · Tengyu Ma · Adams Wei Yu
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70588">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70588-thumb.png?t=1701377400.5447967" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70588" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70588" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70588">
                    Abstract <i id="caret-70588" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70588">
            <div class="abstract-display">
                <p>The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect
                    language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax
                    Optimization (DoReMi), which first trains a small proxy model using group distributionally robust
                    optimization (Group DRO) over domains to produce domain weights (mixture proportions) without
                    knowledge of downstream tasks. We then resample a dataset with these domain weights and train a
                    larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find
                    domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi
                    improves perplexity across all domains, even when it downweights a domain. DoReMi improves average
                    few-shot downstream accuracy by 6.5% points over a baseline model trained using The Pile's default
                    domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM
                    dataset, DoReMi, which has no knowledge of downstream tasks, even matches the performance of using
                    domain weights tuned on downstream tasks.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70293">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-328"></span>

        <script>
        add_bookmark_click(
            70293,
             1,
            'bookmark-number-328',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70293">Break It Down: Evidence for Structural
                Compositionality in Neural Networks</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Michael Lepori · Thomas Serre · Ellie Pavlick</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70293">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70293-thumb.png?t=1702000600.2678251" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70293" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70293" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70293">
                    Abstract <i id="caret-70293" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70293">
            <div class="abstract-display">
                <p>Though modern neural networks have achieved impressive performance in both vision and language tasks,
                    we know little about the functions that they implement. One possibility is that neural networks
                    implicitly break down complex tasks into subroutines, implement modular solutions to these
                    subroutines, and compose them into an overall solution to a task --- a property we term structural
                    compositionality. Another possibility is that they may simply learn to match new inputs to learned
                    templates, eliding task decomposition entirely. Here, we leverage model pruning techniques to
                    investigate this question in both vision and language across a variety of architectures, tasks, and
                    pretraining regimens. Our results demonstrate that models oftentimes implement solutions to
                    subroutines via modular subnetworks, which can be ablated while maintaining the functionality of
                    other subnetworks. This suggests that neural networks may be able to learn compositionality,
                    obviating the need for specialized symbolic mechanisms.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70422">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-329"></span>

        <script>
        add_bookmark_click(
            70422,
             1,
            'bookmark-number-329',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70422">Optimal Exploration for Model-Based RL in Nonlinear
                Systems</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Andrew Wagenmaker · Guanya Shi · Kevin Jamieson</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70422">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70422-thumb.png?t=1701318116.7875025" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70422" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70422" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70422">
                    Abstract <i id="caret-70422" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70422">
            <div class="abstract-display">
                <p>Learning to control unknown nonlinear dynamical systems is a fundamental problem in reinforcement
                    learning and control theory. A commonly applied approach is to first explore the environment
                    (exploration), learn an accurate model of it (system identification), and then compute an optimal
                    controller with the minimum cost on this estimated system (policy optimization). While existing work
                    has shown that it is possible to learn a uniformly good model of the system (Mania et al., 2020), in
                    practice, if we aim to learn a good controller with a low cost on the actual system, certain system
                    parameters may be significantly more critical than others, and we therefore ought to focus our
                    exploration on learning such parameters.In this work, we consider the setting of nonlinear dynamical
                    systems and seek to formally quantify, in such settings, (a) which parameters are most relevant to
                    learning a good controller, and (b) how we can best explore so as to minimize uncertainty in such
                    parameters. Inspired by recent work in linear systems (Wagenmaker et al., 2021), we show that
                    minimizing the controller loss in nonlinear systems translates to estimating the system parameters
                    in a particular, task-dependent metric. Motivated by this, we develop an algorithm able to
                    efficiently …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70901">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-330"></span>

        <script>
        add_bookmark_click(
            70901,
             1,
            'bookmark-number-330',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70901">Multi Time Scale World Models</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Vaisakh Shaj Kumar · SALEH GHOLAM ZADEH · Ozan Demir · Luiz Douat · Gerhard Neumann
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70901">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70901" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70901" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70901">
                    Abstract <i id="caret-70901" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70901">
            <div class="abstract-display">
                <p>Intelligent agents use internal world models to reason and make predictions about different courses
                    of their actions at many scales. Devising learning paradigms and architectures that allow machines
                    to learn world models that operate at multiple levels of temporal abstractions while dealing with
                    complex uncertainty predictions is a major technical hurdle. In this work, we propose a
                    probabilistic formalism to learn multi-time scale world models which we call the Multi Time Scale
                    State Space (MTS3) model. Our model uses a computationally efficient inference scheme on multiple
                    time scales for highly accurate long-horizon predictions and uncertainty estimates over several
                    seconds into the future. Our experiments, which focus on action conditional long horizon future
                    predictions, show that MTS3 outperforms recent methods on several system identification benchmarks
                    including complex simulated and real-world dynamical systems.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70365">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-331"></span>

        <script>
        add_bookmark_click(
            70365,
             1,
            'bookmark-number-331',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70365">VaRT: Variational Regression Trees</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Sebastian Salazar</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70365">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70365" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70365" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70365">
                    Abstract <i id="caret-70365" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70365">
            <div class="abstract-display">
                <p>Decision trees are a well-established tool in machine learning for classification and regression
                    tasks. In this paper, we introduce a novel non-parametric Bayesian model that uses variational
                    inference to approximate a posterior distribution over the space of stochastic decision trees. We
                    evaluate the model's performance on 18 datasets and demonstrate its competitiveness with other
                    state-of-the-art methods in regression tasks. We also explore its application to causal inference
                    problems. We provide a fully vectorized implementation of our algorithm in PyTorch.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70397">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-332"></span>

        <script>
        add_bookmark_click(
            70397,
             1,
            'bookmark-number-332',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70397">Learning Probabilistic Symmetrization for
                Architecture Agnostic Equivariance</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jinwoo Kim · Dat Nguyen · Ayhan Suleymanzade · Hyeokjun An · Seunghoon Hong</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70397">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70397-thumb.png?t=1701961044.411222" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70397" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70397" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70397">
                    Abstract <i id="caret-70397" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70397">
            <div class="abstract-display">
                <p>We present a novel framework to overcome the limitations of equivariant architectures in learning
                    functions with group symmetries. In contrary to equivariant architectures, the framework uses an
                    arbitrary backbone (such as an MLP or a transformer) and symmetrizes it to be equivariant to given
                    group by employing a small equivariant network that parameterizes the probabilistic distribution
                    underlying the symmetrization. The distribution is end-to-end trained with the backbone which can
                    maximize performance while reducing sample complexity of symmetrization. We show that this approach
                    ensures not only equivariance to the given group but also universal approximation ability in
                    expectation. We implement our method on a simple patch-based transformer backbone initialized from
                    pretrained vision transformer, and test it for a wide range of symmetry groups including permutation
                    and Euclidean groups and their combinations. Empirical tests show competitive results against
                    tailored equivariant architectures, suggesting the potential for learning equivariant functions for
                    diverse groups using a non-equivariant universal backbone. We further show evidence of enhanced
                    learning in symmetric modalities, like graphs, when pretrained from non-symmetric modalities, like
                    vision.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70443">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-333"></span>

        <script>
        add_bookmark_click(
            70443,
             1,
            'bookmark-number-333',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70443">Explore In-Context Learning for 3D Point Cloud
                Understanding</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Zhongbin Fang · Xiangtai Li · Xia Li · Joachim M Buhmann · Chen Change Loy · Mengyuan
            Liu
        </div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70443">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70443-thumb.png?t=1698200747.4926763" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70443" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70443" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70443">
                    Abstract <i id="caret-70443" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70443">
            <div class="abstract-display">
                <p>With the rise of large-scale models trained on broad data, in-context learning has become a new
                    learning paradigm that has demonstrated significant potential in natural language processing and
                    computer vision tasks. Meanwhile, in-context learning is still largely unexplored in the 3D point
                    cloud domain. Although masked modeling has been successfully applied for in-context learning in 2D
                    vision, directly extending it to 3D point clouds remains a formidable challenge. In the case of
                    point clouds, the tokens themselves are the point cloud positions (coordinates) that are masked
                    during inference. Moreover, position embedding in previous works may inadvertently introduce
                    information leakage. To address these challenges, we introduce a novel framework, named
                    Point-In-Context, designed especially for in-context learning in 3D point clouds, where both inputs
                    and outputs are modeled as coordinates for each task. Additionally, we propose the Joint Sampling
                    module, carefully designed to work in tandem with the general point sampling operator, effectively
                    resolving the aforementioned technical issues. We conduct extensive experiments to validate the
                    versatility and adaptability of our proposed methods in handling a wide range of tasks. Furthermore,
                    with a more effective prompt selection strategy, our framework surpasses the results of individually
                    trained models.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70829">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-334"></span>

        <script>
        add_bookmark_click(
            70829,
             1,
            'bookmark-number-334',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70829">SimMTM: A Simple Pre-Training Framework for Masked
                Time-Series Modeling</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jiaxiang Dong · Haixu Wu · Haoran Zhang · Li Zhang · Jianmin Wang · Mingsheng Long</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70829">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70829-thumb.png?t=1697799634.0980868" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70829" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70829" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70829">
                    Abstract <i id="caret-70829" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70829">
            <div class="abstract-display">
                <p>Time series analysis is widely used in extensive areas. Recently, to reduce labeling expenses and
                    benefit various tasks, self-supervised pre-training has attracted immense interest. One mainstream
                    paradigm is masked modeling, which successfully pre-trains deep models by learning to reconstruct
                    the masked content based on the unmasked part. However, since the semantic information of time
                    series is mainly contained in temporal variations, the standard way of randomly masking a portion of
                    time points will seriously ruin vital temporal variations of time series, making the reconstruction
                    task too difficult to guide representation learning. We thus present SimMTM, a Simple pre-training
                    framework for Masked Time-series Modeling. By relating masked modeling to manifold learning, SimMTM
                    proposes to recover masked time points by the weighted aggregation of multiple neighbors outside the
                    manifold, which eases the reconstruction task by assembling ruined but complementary temporal
                    variations from multiple masked series. SimMTM further learns to uncover the local structure of the
                    manifold, which is helpful for masked modeling. Experimentally, SimMTM achieves state-of-the-art
                    fine-tuning performance compared to the most advanced time series pre-training methods in two
                    canonical time series analysis tasks: forecasting and classification, covering both in- and
                    cross-domain settings.</p>

            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70312">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-335"></span>

        <script>
        add_bookmark_click(
            70312,
             1,
            'bookmark-number-335',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70312">Locality Sensitive Hashing in Fourier Frequency
                Domain For Soft Set Containment Search</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Indradyumna Roy · Rishi Agarwal · Soumen Chakrabarti · Anirban Dasgupta · Abir De</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70312">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70312" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70312" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70312">
                    Abstract <i id="caret-70312" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70312">
            <div class="abstract-display">
                <p>In many search applications related to passage retrieval, text entailment, and subgraph search, the
                    query and each 'document' is a set of elements, with a document being relevant if it contains the
                    query. These elements are not represented by atomic IDs, but by embedded representations, thereby
                    extending set containment to <em>soft</em> set containment. Recent applications address soft set
                    containment by encoding sets into fixed-size vectors and checking for elementwise <em>vector</em>
                    <em>dominance</em>. This 0/1 property can be relaxed to an asymmetric <em>hinge</em>
                    <em>distance</em> for scoring and ranking candidate documents. Here we focus on data-sensitive,
                    trainable indices for fast retrieval of relevant documents. Existing LSH methods are designed for
                    mostly symmetric or few simple asymmetric distance functions, which are not suitable for hinge
                    distance. Instead, we transform hinge distance into a proposed <em>dominance</em>
                    <em>similarity</em> measure, to which we then apply a Fourier transform, thereby expressing
                    dominance similarity as an expectation of inner products of functions in the frequency domain. Next,
                    we approximate the expectation with an importance-sampled estimate. The overall consequence is that
                    now we can use a traditional LSH, but in the frequency domain. To ensure that the LSH uses hash bits
                    efficiently, we learn hash functions that are sensitive …</p>
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70826">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-336"></span>

        <script>
        add_bookmark_click(
            70826,
             1,
            'bookmark-number-336',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70826">Coop: Memory is not a Commodity</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Jianhao Zhang · Shihan Ma · Peihong Liu · Jinhui Yuan</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70826">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70826" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70826" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70826">
                    Abstract <i id="caret-70826" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70826">
            <div class="abstract-display">
                Tensor rematerialization allows the training of deep neural networks (DNNs) under limited memory budgets
                by checkpointing the models and recomputing the evicted tensors as needed. However, the existing tensor
                rematerialization techniques overlook the memory system in deep learning frameworks and implicitly
                assume that free memory blocks at different addresses are identical. Under this flawed assumption,
                discontiguous tensors are evicted, among which some are not used to allocate the new tensor. This leads
                to severe memory fragmentation and increases the cost of potential rematerializations.To address this
                issue, we propose to evict tensors within a sliding window to ensure all evictions are contiguous and
                are immediately used. Furthermore, we proposed cheap tensor partitioning and recomputable in-place to
                further reduce the rematerialization cost by optimizing the tensor allocation.We named our method \name/
                as it is a co-optimization of tensor allocation and tensor rematerialization. We evaluated \name/ on
                eight representative DNNs. The experimental results demonstrate that \name/ achieves up to $2\times$
                memory saving and hugely reduces compute overhead, search latency, and memory fragmentation compared to
                the state-of-the-art baselines.
            </div>
        </div>

    </div>

    <div class="displaycards touchup-date" id="event-70813">


        <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
              id="bookmark-number-337"></span>

        <script>
        add_bookmark_click(
            70813,
             1,
            'bookmark-number-337',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

        </script>


        <div style="width:80%;margin:auto;">
            <a class="small-title" href="/virtual/2023/poster/70813">Improved Convergence in High Probability of Clipped
                Gradient Methods with Heavy Tailed Noise</a>
        </div>
        <div class="type_display_name_virtual_card">Spotlight Poster</div>
        <div class="author-str">Ta Duy Nguyen · Thien H Nguyen · Alina Ene · Huy Nguyen</div>
        <div class="author-str higher"></div>
        <div class="text-muted touchup-date-div" id="touchup-date-event-70813">Thu 14 Dec 05:45 PM CET</div>


        <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


        <img src="/media/PosterPDFs/NeurIPS%202023/70813-thumb.png?t=1701887076.6682277" class="social-img-thumb"
             alt="thumbnail">


        <div class="abstract-section">

            <div>
                <a id="abstract-link-70813" class="abstract-link" data-toggle="collapse"
                   href="#collapse-event-abstract-70813" role="button" aria-expanded="false"
                   aria-controls="collapse-event-abstract-70813">
                    Abstract <i id="caret-70813" class="fas fa-caret-right"></i>
                </a>
            </div>


        </div>
        <div class="collapse" id="collapse-event-abstract-70813">
            <div class="abstract-display">
                In this work, we study the convergence in high probability of clipped gradient methods when the noise
                distribution has heavy tails, i.e., with bounded $p$th moments, for some $1<p\le2$. prior="" works=""
                in="" this="" setting="" follow="" the="" same="" recipe="" of="" using="" concentration=""
                inequalities="" and="" an="" inductive="" argument="" with="" union="" bound="" to="" iterates=""
                across="" all="" iterations.="" method="" results="" increase="" failure="" probability="" by="" a=""
                factor="" $t$,="" where="" $t$="" is="" number="" we="" instead="" propose="" new="" analysis=""
                approach="" based="" on="" bounding="" moment="" generating="" function="" well="" chosen=""
                supermartingale="" sequence.="" improve="" dependency="" convergence="" guarantee="" for="" wide=""
                range="" algorithms="" clipped="" gradients,="" including="" stochastic="" (accelerated)="" mirror=""
                descent="" convex="" objectives="" gradient="" nonconvex="" objectives.="" our="" high="" bounds=""
                achieve="" optimal="" rates="" match="" best="" currently="" known="" in-expectation="" bounds.=""
                naturally="" allows="" use="" time-varying="" step="" sizes="" clipping="" parameters="" when="" time=""
                horizon="" unknown,="" which="" appears="" difficult="" or="" even="" impossible="" techniques=""
                from="" works.="" furthermore,="" show="" that="" case="" descent,="" several="" problem=""
                constants,="" initial="" …="" <="" div="">
                </p\le2$.>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70622">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-338"></span>

            <script>
        add_bookmark_click(
            70622,
             1,
            'bookmark-number-338',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70622">Minimum-Risk Recalibration of Classifiers</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Zeyu Sun · Dogyoon Song · Alfred Hero</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70622">Thu 14 Dec 05:45 PM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70622-thumb.png?t=1702183943.2804384" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70622" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70622" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70622">
                        Abstract <i id="caret-70622" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70622">
                <div class="abstract-display">
                    Recalibrating probabilistic classifiers is vital for enhancing the reliability and accuracy of
                    predictive models. Despite the development of numerous recalibration algorithms, there is still a
                    lack of a comprehensive theory that integrates calibration and sharpness (which is essential for
                    maintaining predictive power). In this paper, we introduce the concept of minimum-risk recalibration
                    within the framework of mean-squared-error (MSE) decomposition, offering a principled approach for
                    evaluating and recalibrating probabilistic classifiers. Using this framework, we analyze the
                    uniform-mass binning (UMB) recalibration method and establish a finite-sample risk upper bound of
                    order $\tilde{O}(B/n + 1/B^2)$ where $B$ is the number of bins and $n$ is the sample size. By
                    balancing calibration and sharpness, we further determine that the optimal number of bins for UMB
                    scales with $n^{1/3}$, resulting in a risk bound of approximately $O(n^{-2/3})$. Additionally, we
                    tackle the challenge of label shift by proposing a two-stage approach that adjusts the recalibration
                    function using limited labeled data from the target domain. Our results show that transferring a
                    calibrated classifier requires significantly fewer target samples compared to recalibrating from
                    scratch. We validate our theoretical findings through numerical simulations, which confirm the
                    tightness of the proposed bounds, the optimal number of bins, and the effectiveness …
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70914">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-339"></span>

            <script>
        add_bookmark_click(
            70914,
             1,
            'bookmark-number-339',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70914">Calibrated Stackelberg Games: Learning Optimal
                    Commitments Against Calibrated Agents</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Nika Haghtalab · Chara Podimata · Kunhe Yang</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70914">Thu 14 Dec 05:45 PM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70914" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70914" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70914">
                        Abstract <i id="caret-70914" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70914">
                <div class="abstract-display">
                    <p>In this paper, we introduce a generalization of the standard Stackelberg Games (SGs) framework:
                        <em>Calibrated Stackelberg Games</em>. In CSGs, a principal repeatedly interacts with an agent
                        who (contrary to standard SGs) does not have direct access to the principal's action but instead
                        best responds to <em>calibrated forecasts</em> about it. CSG is a powerful modeling tool that
                        goes beyond assuming that agents use ad hoc and highly specified algorithms for interacting in
                        strategic settings to infer the principal's actions and thus more robustly addresses real-life
                        applications that SGs were originally intended to capture. Along with CSGs, we also introduce a
                        stronger notion of calibration, termed <em>adaptive calibration</em>, that provides fine-grained
                        any-time calibration guarantees against adversarial sequences. We give a general approach for
                        obtaining adaptive calibration algorithms and specialize them for finite CSGs. In our main
                        technical result, we show that in CSGs, the principal can achieve utility that converges to the
                        optimum Stackelberg value of the game both in <em>finite</em> and <em>continuous</em> settings
                        and that no higher utility is achievable. Two prominent and immediate applications of our
                        results are the settings of learning in Stackelberg Security Games and strategic classification,
                        both against <em>calibrated</em> agents.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70856">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-340"></span>

            <script>
        add_bookmark_click(
            70856,
             1,
            'bookmark-number-340',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70856">One Fits All: Power General Time Series
                    Analysis by Pretrained LM</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Tian Zhou · Peisong Niu · xue wang · Liang Sun · Rong Jin</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70856">Thu 14 Dec 05:45 PM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70856-thumb.png?t=1701397656.0714998" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70856" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70856" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70856">
                        Abstract <i id="caret-70856" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70856">
                <div class="abstract-display">
                    <p>Although we have witnessed great success of pre-trained models in natural language processing
                        (NLP) and computer vision (CV), limited progress has been made for general time series analysis.
                        Unlike NLP and CV where a unified model can be used to perform different tasks, specially
                        designed approach still dominates in each time series analysis task such as classification,
                        anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the
                        development of pre-trained model for time series analysis is the lack of a large amount of data
                        for training. In this work, we address this challenge by leveraging language or CV models,
                        pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from
                        altering the self-attention and feedforward layers of the residual blocks in the pre-trained
                        language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is
                        evaluated through fine-tuning on all major types of tasks involving time series. Our results
                        demonstrate that pre-trained models on natural language or images can lead to a comparable or
                        state-of-the-art performance in all main time series analysis tasks, as illustrated in Figure1.
                        We also found both theoretically and empirically that the self-attention module behaviors
                        similarly to principle component analysis …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-71858">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-341"></span>

            <script>
        add_bookmark_click(
            71858,
             1,
            'bookmark-number-341',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/71858">Prefix-Tree Decoding for Predicting Mass
                    Spectra from Molecules</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Samuel Goldman · John Bradshaw · Jiayi Xin · Connor Coley</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-71858">Thu 14 Dec 05:45 PM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/71858-thumb.png?t=1698071422.6775618" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-71858" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-71858" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-71858">
                        Abstract <i id="caret-71858" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-71858">
                <div class="abstract-display">
                    <p>Computational predictions of mass spectra from molecules have enabled the discovery of clinically
                        relevant metabolites. However, such predictive tools are still limited as they occupy one of two
                        extremes, either operating (a) by fragmenting molecules combinatorially with overly rigid
                        constraints on potential rearrangements and poor time complexity or (b) by decoding lossy and
                        nonphysical discretized spectra vectors. In this work, we use a new intermediate strategy for
                        predicting mass spectra from molecules by treating mass spectra as sets of molecular formulae,
                        which are themselves multisets of atoms. After first encoding an input molecular graph, we
                        decode a set of molecular subformulae, each of which specify a predicted peak in the mass
                        spectrum, the intensities of which are predicted by a second model. Our key insight is to
                        overcome the combinatorial possibilities for molecular subformulae by decoding the formula set
                        using a prefix tree structure, atom-type by atom-type, representing a general method for ordered
                        multiset decoding. We show promising empirical results on mass spectra prediction tasks.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70895">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-342"></span>

            <script>
        add_bookmark_click(
            70895,
             1,
            'bookmark-number-342',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70895">Neural Foundations of Mental Simulation: Future
                    Prediction of Latent Representations on Dynamic Scenes</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Aran Nayebi · Rishi Rajalingham · Mehrdad Jazayeri · Guangyu Robert Yang</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70895">Thu 14 Dec 05:45 PM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70895-thumb.png?t=1702051436.0162678" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70895" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70895" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70895">
                        Abstract <i id="caret-70895" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70895">
                <div class="abstract-display">
                    <p>Humans and animals have a rich and flexible understanding of the physical world, which enables
                        them to infer the underlying dynamical trajectories of objects and events, plausible future
                        states, and use that to plan and anticipate the consequences of actions.However, the neural
                        mechanisms underlying these computations are unclear.We combine a goal-driven modeling approach
                        with dense neurophysiological data and high-throughput human behavioral readouts that contain
                        thousands of comparisons to directly impinge on this question.Specifically, we construct and
                        evaluate several classes of sensory-cognitive networks to predict the future state of rich,
                        ethologically-relevant environments, ranging from self-supervised end-to-end models with
                        pixel-wise or object-slot objectives, to models that future predict in the latent space of
                        purely static image-pretrained or dynamic video-pretrained foundation models.We find that
                        ``scale is \emph{not} all you need'', and that many state-of-the-art machine learning models
                        fail to perform well on our neural and behavioral benchmarks for future prediction.In fact, only
                        one class of models matches these data well overall.We find that neural responses are currently
                        best predicted by models trained to predict the future state of their environment in the
                        \emph{latent} space of pretrained foundation models optimized for \emph{dynamic} scenes in a
                        self-supervised manner.These models also approach the neurons' ability to …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70593">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-343"></span>

            <script>
        add_bookmark_click(
            70593,
             1,
            'bookmark-number-343',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70593">Masked Space-Time Hash Encoding for Efficient
                    Dynamic Scene Reconstruction</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Feng Wang · Zilong Chen · Guokang Wang · Yafei Song · Huaping Liu</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70593">Thu 14 Dec 05:45 PM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70593" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70593" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70593">
                        Abstract <i id="caret-70593" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70593">
                <div class="abstract-display">
                    <p>In this paper, we propose the Masked Space-Time Hash encoding (MSTH), a novel method for
                        efficiently reconstructing dynamic 3D scenes from multi-view or monocular videos. Based on the
                        observation that dynamic scenes often contain substantial static areas that result in redundancy
                        in storage and computations, MSTH represents a dynamic scene as a weighted combination of a 3D
                        hash encoding and a 4D hash encoding. The weights for the two components are represented by a
                        learnable mask which is guided by an uncertainty-based objective to reflect the spatial and
                        temporal importance of each 3D position. With this design, our method can reduce the hash
                        collision rate by avoiding redundant queries and modifications on static areas, making it
                        feasible to represent a large number of space-time voxels by hash tables with small
                        size.Besides, without the requirements to fit the large numbers of temporally redundant features
                        independently, our method is easier to optimize and converge rapidly with only twenty minutes of
                        training for a 300-frame dynamic scene. We evaluate our method on extensive dynamic scenes. As a
                        result, MSTH obtains consistently better results than previous state-of-the-art methods with
                        only 20 minutes of training time and 130 MB of memory storage.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70197">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-344"></span>

            <script>
        add_bookmark_click(
            70197,
             1,
            'bookmark-number-344',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70197">Tracr: Compiled Transformers as a Laboratory
                    for Interpretability</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">David Lindner · Janos Kramar · Sebastian Farquhar · Matthew Rahtz · Tom McGrath ·
                Vladimir Mikulik
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70197">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70197-thumb.png?t=1701442758.2643414" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70197" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70197" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70197">
                        Abstract <i id="caret-70197" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70197">
                <div class="abstract-display">
                    <p>We show how to "compile" human-readable programs into standard decoder-only transformer models.
                        Our compiler, Tracr, generates models with known structure. This structure can be used to design
                        experiments. For example, we use it to study "superposition" in transformers that execute
                        multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as
                        <em>ground-truth</em> for evaluating interpretability methods. Commonly, because the "programs"
                        learned by transformers are unknown it is unclear whether an interpretation succeeded. We
                        demonstrate our approach by implementing and examining programs including computing token
                        frequencies, sorting, and parenthesis checking. We provide an open-source implementation of
                        Tracr at https://github.com/google-deepmind/tracr.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70047">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-345"></span>

            <script>
        add_bookmark_click(
            70047,
             1,
            'bookmark-number-345',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70047">Stein $\Pi$-Importance Sampling</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Congye Wang · Ye Chen · Heishiro Kanagawa · Chris Oates</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70047">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70047-thumb.png?t=1697135250.5473504" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70047" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70047" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70047">
                        Abstract <i id="caret-70047" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70047">
                <div class="abstract-display">
                    Stein discrepancies have emerged as a powerful tool for retrospective improvement of Markov chain
                    Monte Carlo output. However, the question of how to design Markov chains that are well-suited to
                    such post-processing has yet to be addressed. This paper studies Stein importance sampling, in which
                    weights are assigned to the states visited by a $\Pi$-invariant Markov chain to obtain a consistent
                    approximation of $P$, the intended target. Surprisingly, the optimal choice of $\Pi$ is not
                    identical to the target $P$; we therefore propose an explicit construction for $\Pi$ based on a
                    novel variational argument. Explicit conditions for convergence of Stein $\Pi$-Importance Sampling
                    are established. For $\approx 70$% of tasks in the PosteriorDB benchmark, a significant improvement
                    over the analogous post-processing of $P$-invariant Markov chains is reported.
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70268">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-346"></span>

            <script>
        add_bookmark_click(
            70268,
             1,
            'bookmark-number-346',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70268">Let the Flows Tell: Solving Graph Combinatorial
                    Problems with GFlowNets</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Dinghuai Zhang · Hanjun Dai · Nikolay Malkin · Aaron Courville · Yoshua Bengio ·
                Ling Pan
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70268">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70268" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70268" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70268">
                        Abstract <i id="caret-70268" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70268">
                <div class="abstract-display">
                    <p>Combinatorial optimization (CO) problems are often NP-hard and thus out of reach for exact
                        algorithms, making them a tempting domain to apply machine learning methods. The highly
                        structured constraints in these problems can hinder either optimization or sampling directly in
                        the solution space.On the other hand, GFlowNets have recently emerged as a powerful machinery to
                        efficiently sample from composite unnormalized densities sequentially and have the potential to
                        amortize such solution-searching processes in CO, as well as generate diverse solution
                        candidates.In this paper, we design Markov decision processes (MDPs) for different combinatorial
                        problems and propose to train conditional GFlowNets to sample from the solution space. Efficient
                        training techniques are also developed to benefit long-range credit assignment.Through extensive
                        experiments on a variety of different CO tasks with synthetic and realistic data, we demonstrate
                        that GFlowNet policies can efficiently find high-quality solutions.Our implementation is
                        open-sourced at https://github.com/zdhNarsil/GFlowNet-CombOpt.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-69893">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-347"></span>

            <script>
        add_bookmark_click(
            69893,
             1,
            'bookmark-number-347',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/69893">Sounding Bodies: Modeling 3D Spatial Sound of
                    Humans Using Body Pose and Audio</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Xudong XU · Dejan Markovic · Jacob Sandakly · Todd Keebler · Steven Krenn ·
                Alexander Richard
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-69893">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/69893-thumb.png?t=1701888744.1088603" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-69893" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-69893" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-69893">
                        Abstract <i id="caret-69893" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-69893">
                <div class="abstract-display">
                    <p>While 3D human body modeling has received much attention in computer vision, modeling the
                        acoustic equivalent, i.e. modeling 3D spatial audio produced by body motion and speech, has
                        fallen short in the community. To close this gap, we present a model that can generate accurate
                        3D spatial audio for full human bodies. The system consumes, as input, audio signals from
                        headset microphones and body pose, and produces, as output, a 3D sound field surrounding the
                        transmitter's body, from which spatial audio can be rendered at any arbitrary position in the 3D
                        space. We collect a first-of-its-kind multimodal dataset of human bodies, recorded with multiple
                        cameras and a spherical array of 345 microphones. In an empirical evaluation, we demonstrate
                        that our model can produce accurate body-induced sound fields when trained with a suitable loss.
                        Dataset and code are available online.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-73656">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-348"></span>

            <script>
        add_bookmark_click(
            73656,
             1,
            'bookmark-number-348',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/73656">EHRSHOT: An EHR Benchmark for Few-Shot
                    Evaluation of Foundation Models</a>
            </div>
            <div class="type_display_name_virtual_card">Poster</div>
            <div class="author-str">Michael Wornow · Rahul Thapa · Ethan Steinberg · Jason Fries · Nigam Shah</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-73656">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/73656-thumb.png?t=1701460934.910296" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-73656" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-73656" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-73656">
                        Abstract <i id="caret-73656" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-73656">
                <div class="abstract-display">
                    <p>While the general machine learning (ML) community has benefited from public datasets, tasks, and
                        models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The
                        success of foundation models creates new challenges for healthcare ML by requiring access to
                        shared pretrained models to validate performance benefits. We help address these challenges
                        through three contributions. First, we publish a new dataset, EHRSHOT, which contains
                        de-identified structured data from the electronic health records (EHRs) of 6,739 patients from
                        Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal
                        and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M
                        parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We
                        are one of the first to fully release such a model for coded EHR data; in contrast, most prior
                        models released for clinical data (e.g. GatorTron, ClinicalBERT) only work with unstructured
                        text and cannot process the rich, structured data within an EHR. We provide an end-to-end
                        pipeline for the community to validate and build upon its performance. Third, we define 15
                        few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as
                        sample efficiency …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-73529">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-349"></span>

            <script>
        add_bookmark_click(
            73529,
             1,
            'bookmark-number-349',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/73529">Can LLM Already Serve as A Database Interface?
                    A BIg Bench for Large-Scale Database Grounded Text-to-SQLs</a>
            </div>
            <div class="type_display_name_virtual_card">Poster</div>
            <div class="author-str">Jinyang Li · Binyuan Hui · Ge Qu · Jiaxi Yang · Binhua Li · Bowen Li · Bailin Wang ·
                Bowen Qin · Ruiying Geng · Nan Huo · Xuanhe Zhou · Ma Chenhao · Guoliang Li · Kevin Chang · Fei Huang ·
                Reynold Cheng · Yongbin Li
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-73529">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/73529-thumb.png?t=1697392857.546309" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-73529" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-73529" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-73529">
                        Abstract <i id="caret-73529" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-73529">
                <div class="abstract-display">
                    <p>Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs,
                        has gained increasing attention in recent years. In particular, GPT-4 and Claude-2 have shown
                        impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and
                        WikiSQL, focus on database schema with few rows of database contents leaving the gap between
                        academic study and real-world applications. To mitigate this gap, we present BIRD, a BIg
                        benchmark for laRge-scale Database grounded in text-to-SQL tasks, containing 12,751 pairs of
                        text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional
                        domains. Our emphasis on database values highlights the new challenges of dirty database
                        contents, external knowledge between NL questions and database contents, and SQL efficiency,
                        particularly in the context of massive databases. To solve these problems, text-to-SQL models
                        must feature database value comprehension in addition to semantic parsing. The experimental
                        results demonstrate the significance of database values in generating accurate text-to-SQLs for
                        big databases. Furthermore, even the most popular and effective text-to-SQL models, i.e. GPT-4,
                        only achieve 54.89% in execution accuracy, which is still far from the human result of 92.96%,
                        proving that challenges still stand. We also provide an efficiency analysis to offer …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-73485">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-350"></span>

            <script>
        add_bookmark_click(
            73485,
             1,
            'bookmark-number-350',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/73485">Mind2Web: Towards a Generalist Agent for the
                    Web</a>
            </div>
            <div class="type_display_name_virtual_card">Poster</div>
            <div class="author-str">Xiang Deng · Yu Gu · Boyuan Zheng · Shijie Chen · Sam Stevens · Boshi Wang · Huan
                Sun · Yu Su
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-73485">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/73485-thumb.png?t=1701672096.3598895" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-73485" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-73485" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-73485">
                        Abstract <i id="caret-73485" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-73485">
                <div class="abstract-display">
                    <p>We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the
                        web that can follow language instructions to complete complex tasks on any website. Existing
                        datasets for web agents either use simulated websites or only cover a limited set of websites
                        and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks
                        collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks,
                        Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse
                        domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified
                        ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an
                        initial exploration of using large language models (LLMs) for building generalist web agents.
                        While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that
                        first filtering it with a small LM significantly improves the effectiveness and efficiency of
                        LLMs. Our solution demonstrates a decent level of performance, even on websites or entire
                        domains the model has never seen before, but there is still a substantial room to improve
                        towards truly generalizable agents. We open-source our dataset, model implementation, …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70087">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-351"></span>

            <script>
        add_bookmark_click(
            70087,
             1,
            'bookmark-number-351',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70087">The Geometry of Neural Nets' Parameter Spaces
                    Under Reparametrization</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Agustinus Kristiadi · Felix Dangel · Philipp Hennig</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70087">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70087-thumb.png?t=1701706103.1347988" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70087" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70087" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70087">
                        Abstract <i id="caret-70087" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70087">
                <div class="abstract-display">
                    <p>Model reparametrization, which follows the change-of-variable rule of calculus, is a popular way
                        to improve the training of neural nets. But it can also be problematic since it can induce
                        inconsistencies in, e.g., Hessian-based flatness measures, optimization trajectories, and modes
                        of probability densities. This complicates downstream analyses: e.g. one cannot definitively
                        relate flatness with generalization since arbitrary reparametrization changes their
                        relationship. In this work, we study the invariance of neural nets under reparametrization from
                        the perspective of Riemannian geometry. From this point of view, invariance is an inherent
                        property of any neural net <em>if</em> one explicitly represents the metric and uses the correct
                        associated transformation rules. This is important since although the metric is always present,
                        it is often implicitly assumed as identity, and thus dropped from the notation, then lost under
                        reparametrization. We discuss implications for measuring the flatness of minima, optimization,
                        and for probability-density maximization. Finally, we explore some interesting directions where
                        invariance is useful.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-73598">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-352"></span>

            <script>
        add_bookmark_click(
            73598,
             1,
            'bookmark-number-352',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/73598">DiffInfinite: Large Mask-Image Synthesis via
                    Parallel Random Patch Diffusion in Histopathology</a>
            </div>
            <div class="type_display_name_virtual_card">Poster</div>
            <div class="author-str">Marco Aversa · Gabriel Nobis · Miriam Hägele · Kai Standvoss · Mihaela Chirica ·
                Roderick Murray-Smith · Ahmed Alaa · Lukas Ruff · Daniela Ivanova · Wojciech Samek · Frederick Klauschen
                · Bruno Sanguinetti · Luis Oala
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-73598">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-73598" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-73598" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-73598">
                        Abstract <i id="caret-73598" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-73598">
                <div class="abstract-display">
                    <p>We present DiffInfinite, a hierarchical diffusion model that generates arbitrarily large
                        histological images while preserving long-range correlation structural information. Our approach
                        first generates synthetic segmentation masks, subsequently used as conditions for the
                        high-fidelity generative diffusion process. The proposed sampling method can be scaled up to any
                        desired image size while only requiring small patches for fast training. Moreover, it can be
                        parallelized more efficiently than previous large-content generation methods while avoiding
                        tiling artifacts. The training leverages classifier-free guidance to augment a small, sparsely
                        annotated dataset with unlabelled data. Our method alleviates unique challenges in
                        histopathological imaging practice: large-scale information, costly manual annotation, and
                        protective data handling. The biological plausibility of DiffInfinite data is evaluated in a
                        survey by ten experienced pathologists as well as a downstream classification and segmentation
                        task. Samples from the model score strongly on anti-copying metrics which is relevant for the
                        protection of patient data.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70129">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-353"></span>

            <script>
        add_bookmark_click(
            70129,
             1,
            'bookmark-number-353',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70129">Dynamic Context Pruning for Efficient and
                    Interpretable Autoregressive Transformers</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Sotiris Anagnostidis · Dario Pavllo · Luca Biggio · Lorenzo Noci · Aurelien Lucchi ·
                Thomas Hofmann
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70129">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70129-thumb.png?t=1701802735.8829935" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70129" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70129" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70129">
                        Abstract <i id="caret-70129" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70129">
                <div class="abstract-display">
                    Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long
                    sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt
                    attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In
                    this study, we present a novel approach that dynamically prunes contextual information while
                    preserving the model's expressiveness, resulting in reduced memory and computational requirements
                    during inference. Our method employs a learnable mechanism that determines which uninformative
                    tokens can be dropped from the context at any point across the generation process. By doing so, our
                    approach not only addresses performance concerns but also enhances interpretability, providing
                    valuable insight into the model's decision-making process. Our technique can be applied to existing
                    pre-trained models through a straightforward fine-tuning process, and the pruning strength can be
                    specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can
                    effectively prune up to 80\% of the context without significant performance degradation on
                    downstream tasks, offering a valuable tool for mitigating inference costs. Our reference
                    implementation achieves up to $2\times$ increase in inference throughput and even greater memory
                    savings.
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-69982">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-354"></span>

            <script>
        add_bookmark_click(
            69982,
             1,
            'bookmark-number-354',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/69982">QuIP: 2-Bit Quantization of Large Language
                    Models With Guarantees</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Jerry Chee · Yaohui Cai · Volodymyr Kuleshov · Christopher De Sa</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-69982">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-69982" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-69982" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-69982">
                        Abstract <i id="caret-69982" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-69982">
                <div class="abstract-display">
                    <p>This work studies post-training parameter quantization in large language models (LLMs). We
                        introduce quantization with incoherence processing (QuIP), a new method based on the insight
                        that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights
                        being even in magnitude and the directions in which it is important to round them accurately
                        being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding
                        procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that
                        ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We
                        complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and
                        show that our theory also applies to an existing method, OPTQ. Empirically, we find that our
                        incoherence preprocessing improves several existing quantization algorithms and yields the first
                        LLM quantization methods that produce viable results using only two bits per weight. Our code
                        can be found at https://github.com/jerry-chee/QuIP.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70107">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-355"></span>

            <script>
        add_bookmark_click(
            70107,
             1,
            'bookmark-number-355',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70107">Accelerated Quasi-Newton Proximal
                    Extragradient: Faster Rate for Smooth Convex Optimization</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Ruichen Jiang · Aryan Mokhtari</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70107">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70107" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70107" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70107">
                        Abstract <i id="caret-70107" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70107">
                <div class="abstract-display">
                    In this paper, we propose an accelerated quasi-Newton proximal extragradient method for solving
                    unconstrained smooth convex optimization problems. With access only to the gradients of the
                    objective, we prove that our method can achieve a convergence rate of
                    $\mathcal{O}\bigl(\min\\{\frac{1}{k^2}, \frac{\sqrt{d\log k}}{k^{2.5}}\\}\bigr)$, where $d$ is the
                    problem dimension and $k$ is the number of iterations. In particular, in the regime where $k =
                    \mathcal{O}(d)$, our method matches the _optimal rate_ of $\mathcal{O}(\frac{1}{k^2})$ by Nesterov's
                    accelerated gradient (NAG). Moreover, in the the regime where $k = \Omega(d \log d)$, it outperforms
                    NAG and converges at a _faster rate_ of $\mathcal{O}\bigl(\frac{\sqrt{d\log k}}{k^{2.5}}\bigr)$. To
                    the best of our knowledge, this result is the first to demonstrate a provable gain for a
                    quasi-Newton-type method over NAG in the convex setting. To achieve such results, we build our
                    method on a recent variant of the Monteiro-Svaiter acceleration framework and adopt an online
                    learning perspective to update the Hessian approximation matrices, in which we relate the
                    convergence rate of our method to the dynamic regret of a specific online convex optimization
                    problem in the space of matrices.
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-72156">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-356"></span>

            <script>
        add_bookmark_click(
            72156,
             1,
            'bookmark-number-356',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/72156">ID and OOD Performance Are Sometimes Inversely
                    Correlated on Real-world Datasets</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Damien Teney · Yong Lin · Seong Joon Oh · Ehsan Abbasnejad</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-72156">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/72156-thumb.png?t=1701772199.0753672" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-72156" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-72156" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-72156">
                        Abstract <i id="caret-72156" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-72156">
                <div class="abstract-display">
                    <p>Several studies have compared the in-distribution (ID) and out-of-distribution (OOD) performance
                        of models in computer vision and NLP. They report a frequent positive correlation and some
                        surprisingly never even observe an inverse correlation indicative of a necessary trade-off. The
                        possibility of inverse patterns is important to determine whether ID performance can serve as a
                        proxy for OOD generalization capabilities.This paper shows that inverse correlations between ID
                        and OOD performance do happen with multiple real-world datasets, not only in artificial
                        worst-case settings. We explain theoretically how these cases arise and how past studies missed
                        them because of improper methodologies that examined a biased selection of models.Our
                        observations lead to recommendations that contradict those found in much of the current
                        literature.- High OOD performance sometimes requires trading off ID performance.- Focusing on ID
                        performance alone may not lead to optimal OOD performance. It may produce diminishing
                        (eventually negative) returns in OOD performance.- In these cases, studies on OOD generalization
                        that use ID performance for model selection (a common recommended practice) will necessarily
                        miss the best-performing models, making these studies blind to a whole range of phenomena.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-73705">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-357"></span>

            <script>
        add_bookmark_click(
            73705,
             1,
            'bookmark-number-357',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/73705">Ego4D Goal-Step: Toward Hierarchical
                    Understanding of Procedural Activities</a>
            </div>
            <div class="type_display_name_virtual_card">Poster</div>
            <div class="author-str">Yale Song · Eugene Byrne · Tushar Nagarajan · Huiyu Wang · Miguel Martin · Lorenzo
                Torresani
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-73705">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/73705-thumb.png?t=1702192196.1073394" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-73705" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-73705" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-73705">
                        Abstract <i id="caret-73705" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-73705">
                <div class="abstract-display">
                    <p>Human activities are goal-oriented and hierarchical, comprising primary goals at the top level,
                        sequences of steps and substeps in the middle, and atomic actions at the lowest level.
                        Recognizing human activities thus requires relating atomic actions and steps to their functional
                        objectives (what the actions contribute to) and modeling their sequential and hierarchical
                        dependencies towards achieving the goals. Current activity recognition research has primarily
                        focused on only the lowest levels of this hierarchy, i.e., atomic or low-level actions, often in
                        trimmed videos with annotations spanning only a few seconds. In this work, we introduce Ego4D
                        Goal-Step, a new set of annotations on the recently released Ego4D with a novel hierarchical
                        taxonomy of goal-oriented activity labels. It provides dense annotations for 48K procedural step
                        segments (430 hours) and high-level goal annotations for 2,807 hours of Ego4D videos. Compared
                        to existing procedural video datasets, it is substantially larger in size, contains hierarchical
                        action labels (goals - steps - substeps), and provides goal-oriented auxiliary information
                        including natural language summary description, step completion status, and step-to-goal
                        relevance information. We take a data-driven approach to build our taxonomy, resulting in dense
                        step annotations that do not suffer from poor label-data alignment issues resulting from a …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-73630">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-358"></span>

            <script>
        add_bookmark_click(
            73630,
             1,
            'bookmark-number-358',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/73630">EgoSchema: A Diagnostic Benchmark for Very
                    Long-form Video Language Understanding</a>
            </div>
            <div class="type_display_name_virtual_card">Poster</div>
            <div class="author-str">Karttikeya Mangalam · Raiymbek Akshulakov · Jitendra Malik</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-73630">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-73630" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-73630" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-73630">
                        Abstract <i id="caret-73630" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-73630">
                <div class="abstract-display">
                    <p>We introduce EgoSchema, a very long-form video question-answering dataset, and benchmark to
                        evaluate long video understanding capabilities of modern vision and language systems. Derived
                        from Ego4D, EgoSchema consists of over 5000 human curated multiple choice question answer pairs,
                        spanning over 250 hours of real video data, covering a very broad range of natural human
                        activity and behavior. For each question, EgoSchema requires the correct answer to be selected
                        between five given options based on a three-minute-long video clip. While some prior works have
                        proposed video datasets with long clip lengths, we posit that merely the length of the video
                        clip does not truly capture the temporal difficulty of the video task that is being considered.
                        To remedy this, we introduce temporal certificate sets, a general notion for capturing the
                        intrinsic temporal understanding length associated with a broad range of video understanding
                        tasks &amp; datasets. Based on this metric, we find EgoSchema to have intrinsic temporal lengths
                        over 5.7x longer than the second closest dataset and 10x to 100x longer than any other video
                        understanding dataset. Further, our evaluation of several current state-of-the-art video and
                        language models shows them to be severely lacking in long-term video understanding capabilities.
                        Even models with …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-69942">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-359"></span>

            <script>
        add_bookmark_click(
            69942,
             1,
            'bookmark-number-359',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/69942">Uncertainty Quantification over Graph with
                    Conformalized Graph Neural Networks</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Kexin Huang · Ying Jin · Emmanuel Candes · Jure Leskovec</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-69942">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-69942" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-69942" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-69942">
                        Abstract <i id="caret-69942" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-69942">
                <div class="abstract-display">
                    <p>Graph Neural Networks (GNNs) are powerful machine learning prediction models on graph-structured
                        data. However, GNNs lack rigorous uncertainty estimates, limiting their reliable deployment in
                        settings where the cost of errors is significant. We propose conformalized GNN (CF-GNN),
                        extending conformal prediction (CP) to graph-based models for guaranteed uncertainty estimates.
                        Given an entity in the graph, CF-GNN produces a prediction set/interval that provably contains
                        the true label with pre-defined coverage probability (e.g. 90%). We establish a permutation
                        invariance condition that enables the validity of CP on graph data and provide an exact
                        characterization of the test-time coverage. Moreover, besides valid coverage, it is crucial to
                        reduce the prediction set size/interval length for practical use. We observe a key connection
                        between non-conformity scores and network structures, which motivates us to develop a
                        topology-aware output correction model that learns to update the prediction and produces more
                        efficient prediction sets/intervals. Extensive experiments show that CF-GNN achieves any
                        pre-defined target marginal coverage while significantly reducing the prediction set/interval
                        size by up to 74% over the baselines. It also empirically achieves satisfactory conditional
                        coverage over various raw and network features.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70254">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-360"></span>

            <script>
        add_bookmark_click(
            70254,
             1,
            'bookmark-number-360',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70254">Survival Instinct in Offline Reinforcement
                    Learning</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Anqi Li · Dipendra Misra · Andrey Kolobov · Ching-An Cheng</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70254">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70254" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70254" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70254">
                        Abstract <i id="caret-70254" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70254">
                <div class="abstract-display">
                    <p>We present a novel observation about the behavior of offline reinforcement learning (RL)
                        algorithms: on many benchmark datasets, offline RL can produce well-performing and safe policies
                        even when trained with "wrong" reward labels, such as those that are zero everywhere or are
                        negatives of the true rewards. This phenomenon cannot be easily explained by offline RL's return
                        maximization objective. Moreover, it gives offline RL a degree of robustness that is
                        uncharacteristic of its online RL counterparts, which are known to be sensitive to reward
                        design. We demonstrate that this surprising robustness property is attributable to an interplay
                        between the notion of <em>pessimism</em> in offline RL algorithms and certain implicit biases in
                        common data collection practices. As we prove in this work, pessimism endows the agent with a
                        <em>survival instinct</em>, i.e., an incentive to stay within the data support in the long term,
                        while the limited and biased data coverage further constrains the set of survival policies.
                        Formally, given a reward class -- which may not even contain the true reward -- we identify
                        conditions on the training data distribution that enable offline RL to learn a near-optimal and
                        safe policy from any reward within the class. We argue that …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-69918">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-361"></span>

            <script>
        add_bookmark_click(
            69918,
             1,
            'bookmark-number-361',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/69918">Pre-RMSNorm and Pre-CRMSNorm Transformers:
                    Equivalent and Efficient Pre-LN Transformers</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Zixuan Jiang · Jiaqi Gu · Hanqing Zhu · David Pan</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-69918">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-69918" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-69918" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-69918">
                        Abstract <i id="caret-69918" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-69918">
                <div class="abstract-display">
                    <p>Transformers have achieved great success in machine learning applications.Normalization
                        techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization
                        (RMSNorm), play a critical role in accelerating and stabilizing the training of
                        Transformers.While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the
                        vectors by their RMS value.Despite being more computationally efficient, RMSNorm may compromise
                        the representation ability of Transformers.There is currently no consensus regarding the
                        preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm,
                        especially in recent large language models.It is challenging to convert Transformers with one
                        normalization to the other type.While there is an ongoing disagreement between the two
                        normalization types,we propose a solution to unify two mainstream Transformer architectures,
                        Pre-LN and Pre-RMSNorm Transformers.By removing the inherent redundant mean information in the
                        main branch of Pre-LN Transformers, we can reduce LayerNorm to RMSNorm, achieving higher
                        efficiency.We further propose the Compressed RMSNorm (CRMSNorm) and Pre-CRMSNorm Transformer
                        based on a lossless compression of the zero-mean vectors.We formally establish the equivalence
                        of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in both training and inference.It
                        implies that Pre-LN Transformers can be substituted with Pre-(C)RMSNorm counterparts at almost
                        no cost, offering the same arithmetic functionality along with free efficiency
                        improvement.Experiments …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-69916">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-362"></span>

            <script>
        add_bookmark_click(
            69916,
             1,
            'bookmark-number-362',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/69916">Online List Labeling with Predictions</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Samuel McCauley · Ben Moseley · Aidin Niaparast · Shikha Singh</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-69916">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/69916-thumb.png?t=1701826794.3579745" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-69916" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-69916" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-69916">
                        Abstract <i id="caret-69916" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-69916">
                <div class="abstract-display">
                    A growing line of work shows how learned predictions can be used to break through worst-case
                    barriers to improve the running time of an algorithm. However, incorporating predictions into data
                    structures with strong theoretical guarantees remains underdeveloped. This paper takes a step in
                    this direction by showing that predictions can be leveraged in the fundamental online list labeling
                    problem. In the problem, $n$ items arrive over time and must be stored in sorted order in an array
                    of size $\Theta(n)$. The array slot of an element is its label and the goal is to maintain sorted
                    order while minimizing the total number of elements moved (i.e., relabeled). We design a new list
                    labeling data structure and bound its performance in two models. In the worst-case
                    learning-augmented model, we give guarantees in terms of the error in the predictions. Our data
                    structure provides strong guarantees: it is optimal for any prediction error and guarantees the
                    best-known worst-case bound even when the predictions are entirely erroneous. We also consider a
                    stochastic error model and bound the performance in terms of the expectation and variance of the
                    error. Finally, the theoretical results are demonstrated empirically. In particular, we show that
                    our data structure …
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-69958">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-363"></span>

            <script>
        add_bookmark_click(
            69958,
             1,
            'bookmark-number-363',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/69958">Encoding Time-Series Explanations through
                    Self-Supervised Model Behavior Consistency</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Owen Queen · Tom Hartvigsen · Teddy Koker · Huan He · Theodoros Tsiligkaridis ·
                Marinka Zitnik
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-69958">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-69958" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-69958" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-69958">
                        Abstract <i id="caret-69958" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-69958">
                <div class="abstract-display">
                    <p>Interpreting time series models is uniquely challenging because it requires identifying both the
                        location of time series signals that drive model predictions and their matching to an
                        interpretable temporal pattern. While explainers from other modalities can be applied to time
                        series, their inductive biases do not transfer well to the inherently challenging interpretation
                        of time series. We present TimeX, a time series consistency model for training explainers. TimeX
                        trains an interpretable surrogate to mimic the behavior of a pretrained time series model. It
                        addresses the issue of model faithfulness by introducing model behavior consistency, a novel
                        formulation that preserves relations in the latent space induced by the pretrained model with
                        relations in the latent space induced by TimeX. TimeX provides discrete attribution maps and,
                        unlike existing interpretability methods, it learns a latent space of explanations that can be
                        used in various ways, such as to provide landmarks to visually aggregate similar explanations
                        and easily recognize temporal patterns. We evaluate TimeX on eight synthetic and real-world
                        datasets and compare its performance against state-of-the-art interpretability methods. We also
                        conduct case studies using physiological time series. Quantitative evaluations demonstrate that
                        TimeX achieves the highest or second-highest performance in every metric compared to baselines
                        …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70082">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-364"></span>

            <script>
        add_bookmark_click(
            70082,
             1,
            'bookmark-number-364',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70082">Adaptive whitening with fast gain modulation
                    and slow synaptic plasticity</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Lyndon Duong · Eero Simoncelli · Dmitri Chklovskii · David Lipshutz</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70082">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70082-thumb.png?t=1702229772.2849195" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70082" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70082" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70082">
                        Abstract <i id="caret-70082" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70082">
                <div class="abstract-display">
                    <p>Neurons in early sensory areas rapidly adapt to changing sensory statistics, both by normalizing
                        the variance of their individual responses and by reducing correlations between their responses.
                        Together, these transformations may be viewed as an adaptive form of statistical whitening.
                        Existing mechanistic models of adaptive whitening exclusively use either synaptic plasticity or
                        gain modulation as the biological substrate for adaptation; however, on their own, each of these
                        models has significant limitations. In this work, we unify these approaches in a normative
                        multi-timescale mechanistic model that adaptively whitens its responses with complementary
                        computational roles for synaptic plasticity and gain modulation. Gains are modified on a fast
                        timescale to adapt to the current statistical context, whereas synapses are modified on a slow
                        timescale to match structural properties of the input statistics that are invariant across
                        contexts. Our model is derived from a novel multi-timescale whitening objective that factorizes
                        the inverse whitening matrix into basis vectors, which correspond to synaptic weights, and a
                        diagonal matrix, which corresponds to neuronal gains. We test our model on synthetic and natural
                        datasets and find that the synapses learn optimal configurations over long timescales that
                        enable adaptive whitening on short timescales using gain modulation.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70051">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-365"></span>

            <script>
        add_bookmark_click(
            70051,
             1,
            'bookmark-number-365',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70051">High-dimensional Asymptotics of Denoising
                    Autoencoders</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Hugo Cui · Lenka Zdeborová</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70051">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70051" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70051" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70051">
                        Abstract <i id="caret-70051" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70051">
                <div class="abstract-display">
                    <p>We address the problem of denoising data from a Gaussian mixture using a two-layer non-linear
                        autoencoder with tied weights and a skip connection. We consider the high-dimensional limit
                        where the number of training samples and the input dimension jointly tend to infinity while the
                        number of hidden units remains bounded. We provide closed-form expressions for the denoising
                        mean-squared test error. Building on this result, we quantitatively characterize the advantage
                        of the considered architecture over the autoencoder without the skip connection that relates
                        closely to principal component analysis. We further show that our results capture accurately the
                        learning curves on a range of real datasets.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70044">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-366"></span>

            <script>
        add_bookmark_click(
            70044,
             1,
            'bookmark-number-366',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70044">Maximization of Average Precision for Deep
                    Learning with Adversarial Ranking Robustness</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Gang Li · Wei Tong · Tianbao Yang</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70044">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70044-thumb.png?t=1702008366.3630092" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70044" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70044" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70044">
                        Abstract <i id="caret-70044" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70044">
                <div class="abstract-display">
                    <p>This paper seeks to address a gap in optimizing Average Precision (AP) while ensuring adversarial
                        robustness, an area that has not been extensively explored to the best of our knowledge. AP
                        maximization for deep learning has widespread applications, particularly when there is a
                        significant imbalance between positive and negative examples. Although numerous studies have
                        been conducted on adversarial training, they primarily focus on robustness concerning accuracy,
                        ensuring that the average accuracy on adversarially perturbed examples is well maintained.
                        However, this type of adversarial robustness is insufficient for many applications, as minor
                        perturbations on a single example can significantly impact AP while not greatly influencing the
                        accuracy of the prediction system. To tackle this issue, we introduce a novel formulation that
                        combines an AP surrogate loss with a regularization term representing adversarial ranking
                        robustness, which maintains the consistency between ranking of clean data and that of perturbed
                        data. We then devise an efficient stochastic optimization algorithm to optimize the resulting
                        objective. Our empirical studies, which compare our method to current leading adversarial
                        training baselines and other robust AP maximization strategies, demonstrate the effectiveness of
                        the proposed approach. Notably, our methods outperform a state-of-the-art method (TRADES) by
                        more than 4\% in terms …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-73680">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-367"></span>

            <script>
        add_bookmark_click(
            73680,
             1,
            'bookmark-number-367',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/73680">Understanding Social Reasoning in Language
                    Models with Language Models</a>
            </div>
            <div class="type_display_name_virtual_card">Poster</div>
            <div class="author-str">Kanishk Gandhi · Jan-Philipp Fraenken · Tobias Gerstenberg · Noah Goodman</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-73680">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-73680" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-73680" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-73680">
                        Abstract <i id="caret-73680" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-73680">
                <div class="abstract-display">
                    <p>As Large Language Models (LLMs) become increasingly integrated into our everyday lives,
                        understanding their ability to comprehend human mental states becomes critical for ensuring
                        effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM)
                        reasoning capabilities of LLMs, the degree to which these models can align with human ToM
                        remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1)
                        the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the
                        validity of existing evaluation methodologies. To address these challenges, we present a novel
                        framework for procedurally generating evaluations with LLMs by populating causal templates.
                        Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists
                        of 25 controls and 5,000 model-written evaluations. We find that human participants rate the
                        quality of our benchmark higher than previous crowd-sourced evaluations and comparable to
                        expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a
                        variety of LLMs and compare model performances with human performance. Our results suggest that
                        GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while
                        other LLMs struggle.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70227">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-368"></span>

            <script>
        add_bookmark_click(
            70227,
             1,
            'bookmark-number-368',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70227">Rewiring Neurons in Non-Stationary
                    Environments</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Zhicheng Sun · Yadong Mu</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70227">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70227-thumb.png?t=1696417952.0379667" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70227" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70227" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70227">
                        Abstract <i id="caret-70227" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70227">
                <div class="abstract-display">
                    <p>The human brain rewires itself for neuroplasticity in the presence of new tasks. We are inspired
                        to harness this key process in continual reinforcement learning, prioritizing adaptation to
                        non-stationary environments. In distinction to existing rewiring approaches that rely on pruning
                        or dynamic routing, which may limit network capacity and plasticity, this work presents a novel
                        rewiring scheme by permuting hidden neurons. Specifically, the neuron permutation is
                        parameterized to be end-to-end learnable and can rearrange all available synapses to explore a
                        large span of weight space, thereby promoting adaptivity. In addition, we introduce two main
                        designs to steer the rewiring process in continual reinforcement learning: first, a multi-mode
                        rewiring strategy is proposed which diversifies the policy and encourages exploration when
                        encountering new environments. Secondly, to ensure stability on history tasks, the network is
                        devised to cache each learned wiring while subtly updating its weights, allowing for
                        retrospective recovery of any previous state appropriate for the task. Meanwhile, an alignment
                        mechanism is curated to achieve better plasticity-stability tradeoff by jointly optimizing
                        cached wirings and weights. Our proposed method is comprehensively evaluated on 18 continual
                        reinforcement learning scenarios ranging from locomotion to manipulation, demonstrating its
                        advantages over state-of-the-art competitors in performance-efficiency tradeoffs. Code …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-69972">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-369"></span>

            <script>
        add_bookmark_click(
            69972,
             1,
            'bookmark-number-369',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/69972">WITRAN: Water-wave Information Transmission and
                    Recurrent Acceleration Network for Long-range Time Series Forecasting</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Yuxin Jia · Youfang Lin · Xinyan Hao · Yan Lin · Shengnan Guo · Huaiyu Wan</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-69972">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-69972" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-69972" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-69972">
                        Abstract <i id="caret-69972" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-69972">
                <div class="abstract-display">
                    Capturing semantic information is crucial for accurate long-range time series forecasting, which
                    involves modeling global and local correlations, as well as discovering long- and short-term
                    repetitive patterns. Previous works have partially addressed these issues separately, but have not
                    been able to address all of them simultaneously. Meanwhile, their time and memory complexities are
                    still not sufficiently low for long-range forecasting. To address the challenge of capturing
                    different types of semantic information, we propose a novel Water-wave Information Transmission
                    (WIT) framework. This framework captures both long- and short-term repetitive patterns through
                    bi-granular information transmission. It also models global and local correlations by recursively
                    fusing and selecting information using Horizontal Vertical Gated Selective Unit (HVGSU). In
                    addition, to improve the computing efficiency, we propose a generic Recurrent Acceleration Network
                    (RAN) which reduces the time complexity to $\mathcal{O}(\sqrt{L})$ while maintaining the memory
                    complexity at $\mathcal{O}(L)$. Our proposed method, called Water-wave Information Transmission and
                    Recurrent Acceleration Network (WITRAN), outperforms the state-of-the-art methods by 5.80% and
                    14.28% on long-range and ultra-long-range time series forecasting tasks respectively, as
                    demonstrated by experiments on four benchmark datasets. The code is available at:
                    https://github.com/Water2sea/WITRAN.
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-69981">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-370"></span>

            <script>
        add_bookmark_click(
            69981,
             1,
            'bookmark-number-370',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/69981">Adversarial Robustness in Graph Neural
                    Networks: A Hamiltonian Approach</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Kai Zhao · Qiyu Kang · Yang Song · Rui She · Sijie Wang · Wee Peng Tay</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-69981">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/69981-thumb.png?t=1702012795.7449262" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-69981" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-69981" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-69981">
                        Abstract <i id="caret-69981" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-69981">
                <div class="abstract-display">
                    <p>Graph neural networks (GNNs) are vulnerable to adversarial perturbations, including those that
                        affect both node features and graph topology. This paper investigates GNNs derived from diverse
                        neural flows, concentrating on their connection to various stability notions such as BIBO
                        stability, Lyapunov stability, structural stability, and conservative stability. We argue that
                        Lyapunov stability, despite its common use, does not necessarily ensure adversarial robustness.
                        Inspired by physics principles, we advocate for the use of conservative Hamiltonian neural flows
                        to construct GNNs that are robust to adversarial attacks. The adversarial robustness of
                        different neural flow GNNs is empirically compared on several benchmark datasets under a variety
                        of adversarial attacks. Extensive numerical experiments demonstrate that GNNs leveraging
                        conservative Hamiltonian flows with Lyapunov stability substantially improve robustness against
                        adversarial perturbations. The implementation code of experiments is available at
                        \url{https://github.com/zknus/NeurIPS-2023-HANG-Robustness}.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-69887">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-371"></span>

            <script>
        add_bookmark_click(
            69887,
             1,
            'bookmark-number-371',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/69887">Optimistic Natural Policy Gradient: a Simple
                    Efficient Policy Optimization Framework for Online RL</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Qinghua Liu · Gellert Weisz · András György · Chi Jin · Csaba Szepesvari</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-69887">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/69887-thumb.png?t=1702052686.374528" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-69887" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-69887" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-69887">
                        Abstract <i id="caret-69887" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-69887">
                <div class="abstract-display">
                    While policy optimization algorithms have played an important role in recent empirical success of
                    Reinforcement Learning (RL), the existing theoretical understanding of policy optimization remains
                    rather limited---they are either restricted to tabular MDPs or suffer from highly suboptimal sample
                    complexity, especial in online RL where exploration is necessary. This paper proposes a simple
                    efficient policy optimization framework---Optimistic NPG for online RL. Optimistic NPG can be viewed
                    as simply combining of the classic natural policy gradient (NPG) algorithm [Kakade, 2001] with
                    optimistic policy evaluation subroutines to encourage exploration. For $d$-dimensional linear MDPs,
                    Optimistic NPG is computationally efficient, and learns an $\epsilon$-optimal policy within
                    $\tilde{\mathcal{O}}(d^2/\epsilon^3)$ samples, which is the first computationally efficient
                    algorithm whose sample complexity has the optimal dimension dependence $\tilde{\Theta}(d^2)$. It
                    also improves over state-of-the-art results of policy optimization algorithms [Zanette et al., 2021]
                    by a factor of $d$. For general function approximation that subsumes linear MDPs, Optimistic NPG, to
                    our best knowledge, is also the first policy optimization algorithm that achieves the polynomial
                    sample complexity for learning near-optimal policies.
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-72908">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-372"></span>

            <script>
        add_bookmark_click(
            72908,
             1,
            'bookmark-number-372',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/72908">The Pursuit of Human Labeling: A New
                    Perspective on Unsupervised Learning</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Artyom Gadetsky · Maria Brbic</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-72908">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/72908-thumb.png?t=1701851607.6163518" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-72908" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-72908" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-72908">
                        Abstract <i id="caret-72908" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-72908">
                <div class="abstract-display">
                    <p>We present HUME, a simple model-agnostic framework for inferring human labeling of a given
                        dataset without any external supervision. The key insight behind our approach is that classes
                        defined by many human labelings are linearly separable regardless of the representation space
                        used to represent a dataset. HUME utilizes this insight to guide the search over all possible
                        labelings of a dataset to discover an underlying human labeling. We show that the proposed
                        optimization objective is strikingly well-correlated with the ground truth labeling of the
                        dataset. In effect, we only train linear classifiers on top of pretrained representations that
                        remain fixed during training, making our framework compatible with any large pretrained and
                        self-supervised model. Despite its simplicity, HUME outperforms a supervised linear classifier
                        on top of self-supervised representations on the STL-10 dataset by a large margin and achieves
                        comparable performance on the CIFAR-10 dataset. Compared to the existing unsupervised baselines,
                        HUME achieves state-of-the-art performance on four benchmark image classification datasets
                        including the large-scale ImageNet-1000 dataset. Altogether, our work provides a fundamentally
                        new view to tackle unsupervised learning by searching for consistent labelings between different
                        representation spaces.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-69923">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-373"></span>

            <script>
        add_bookmark_click(
            69923,
             1,
            'bookmark-number-373',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/69923">Would I have gotten that reward? Long-term
                    credit assignment by counterfactual contribution analysis</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Alexander Meulemans · Simon Schug · Seijin Kobayashi · nathaniel daw · Gregory
                Wayne
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-69923">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-69923" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-69923" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-69923">
                        Abstract <i id="caret-69923" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-69923">
                <div class="abstract-display">
                    <p>To make reinforcement learning more sample efficient, we need better credit assignment methods
                        that measure an action’s influence on future rewards. Building upon Hindsight Credit Assignment
                        (HCA), we introduce Counterfactual Contribution Analysis (COCOA), a new family of model-based
                        credit assignment algorithms. Our algorithms achieve precise credit assignment by measuring the
                        contribution of actions upon obtaining subsequent rewards, by quantifying a counterfactual
                        query: ‘Would the agent still have reached this reward if it had taken another action?’. We show
                        that measuring contributions w.r.t. rewarding <em>states</em>, as is done in HCA, results in
                        spurious estimates of contributions, causing HCA to degrade towards the high-variance REINFORCE
                        estimator in many relevant environments. Instead, we measure contributions w.r.t. rewards or
                        learned representations of the rewarding objects, resulting in gradient estimates with lower
                        variance. We run experiments on a suite of problems specifically designed to evaluate long-term
                        credit assignment capabilities. By using dynamic programming, we measure ground-truth policy
                        gradients and show that the improved performance of our new model-based credit assignment
                        methods is due to lower bias and variance compared to HCA and common baselines. Our results
                        demonstrate how modeling action contributions towards rewarding outcomes can be leveraged for
                        credit assignment, opening a new path …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70006">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-374"></span>

            <script>
        add_bookmark_click(
            70006,
             1,
            'bookmark-number-374',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70006">Robust Model Reasoning and Fitting via Dual
                    Sparsity Pursuit</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Xingyu Jiang · Jiayi Ma</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70006">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70006-thumb.png?t=1699536198.900598" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70006" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70006" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70006">
                        Abstract <i id="caret-70006" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70006">
                <div class="abstract-display">
                    <p>In this paper, we contribute to solving a threefold problem: outlier rejection, true model
                        reasoning and parameter estimation with a unified optimization modeling. To this end, we first
                        pose this task as a sparse subspace recovering problem, to search a maximum of independent bases
                        under an over-embedded data space. Then we convert the objective into a continuous optimization
                        paradigm that estimates sparse solutions for both bases and errors. Wherein a fast and robust
                        solver is proposed to accurately estimate the sparse subspace parameters and error entries,
                        which is implemented by a proximal approximation method under the alternating optimization
                        framework with the ``optimal'' sub-gradient descent. Extensive experiments regarding known and
                        unknown model fitting on synthetic and challenging real datasets have demonstrated the
                        superiority of our method against the state-of-the-art. We also apply our method to multi-class
                        multi-model fitting and loop closure detection, and achieve promising results both in accuracy
                        and efficiency. Code is released at: https://github.com/StaRainJ/DSP.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70033">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-375"></span>

            <script>
        add_bookmark_click(
            70033,
             1,
            'bookmark-number-375',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70033">Online Constrained Meta-Learning: Provable
                    Guarantees for Generalization</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Siyuan Xu · Minghui Zhu</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70033">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70033-thumb.png?t=1698085053.8965786" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70033" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70033" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70033">
                        Abstract <i id="caret-70033" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70033">
                <div class="abstract-display">
                    <p>Meta-learning has attracted attention due to its strong ability to learn experiences from known
                        tasks, which can speed up and enhance the learning process for new tasks. However, most existing
                        meta-learning approaches only can learn from tasks without any constraint. This paper proposes
                        an online constrained meta-learning framework, which continuously learns meta-knowledge from
                        sequential learning tasks, and the learning tasks are subject to hard constraints. Beyond
                        existing meta-learning analyses, we provide the upper bounds of optimality gaps and constraint
                        violations produced by the proposed framework, which considers the dynamic regret of online
                        learning, as well as the generalization ability of the task-specific models. Moreover, we
                        provide a practical algorithm for the framework, and validate its superior effectiveness through
                        experiments conducted on meta-imitation learning and few-shot image classification.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70052">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-376"></span>

            <script>
        add_bookmark_click(
            70052,
             1,
            'bookmark-number-376',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70052">Provable benefits of score matching</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Chirag Pabbaraju · Dhruv Rohatgi · Anish Prasad Sevekari · Holden Lee · Ankur Moitra
                · Andrej Risteski
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70052">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70052" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70052" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70052">
                        Abstract <i id="caret-70052" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70052">
                <div class="abstract-display">
                    <p>Score matching is an alternative to maximum likelihood (ML) for estimating a probability
                        distribution parametrized up to a constant of proportionality. By fitting the ''score'' of the
                        distribution, it sidesteps the need to compute this constant of proportionality (which is often
                        intractable).While score matching and variants thereof are popular in practice, precise
                        theoretical understanding of the benefits and tradeoffs with maximum likelihood---both
                        computational and statistical---are not well understood. In this work, we give the first example
                        of a natural exponential family of distributions such that the score matching loss is
                        computationally efficient to optimize, and has a comparable statistical efficiency to ML, while
                        the ML loss is intractable to optimize using a gradient-based method. The family consists of
                        exponentials of polynomials of fixed degree, and our result can be viewed as a continuous
                        analogue of recent developments in the discrete setting. Precisely, we show: (1) Designing a
                        zeroth-order or first-order oracle for optimizing the maximum likelihood loss is NP-hard. (2)
                        Maximum likelihood has a statistical efficiency polynomial in the ambient dimension and the
                        radius of the parameters of the family. (3) Minimizing the score matching loss is both
                        computationally and statistically efficient, with complexity polynomial in the ambient
                        dimension.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70115">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-377"></span>

            <script>
        add_bookmark_click(
            70115,
             1,
            'bookmark-number-377',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70115">MVDiffusion: Enabling Holistic Multi-view Image
                    Generation with Correspondence-Aware Diffusion</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Shitao Tang · Fuyang Zhang · Jiacheng Chen · Peng Wang · Yasutaka Furukawa</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70115">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70115-thumb.png?t=1699426074.3985507" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70115" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70115" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70115">
                        Abstract <i id="caret-70115" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70115">
                <div class="abstract-display">
                    <p>This paper introduces MVDiffusion, a simple yet effective multi-view image generation method for
                        scenarios where pixel-to-pixel correspondences are available, such as perspective crops from
                        panorama or multi-view images given geometry (depth maps and poses). Unlike prior methods that
                        rely on iterative image warping and inpainting, MVDiffusion concurrently generates all images
                        with a global awareness, encompassing high resolution and rich content, effectively addressing
                        the error accumulation prevalent in preceding models. MVDiffusion specifically incorporates a
                        correspondence-aware attention mechanism, enabling effective cross-view interaction. This
                        mechanism underpins three pivotal modules: 1) a generation module that produces low-resolution
                        images while maintaining global correspondence, 2) an interpolation module that densifies
                        spatial coverage between images, and 3) a super-resolution module that upscales into
                        high-resolution images. In terms of panoramic imagery, MVDiffusion generates high-resolution
                        photorealistic images up to 1024*1024 pixels. For geometry-conditioned multi-view image
                        generation, MVDiffusion demonstrates state-of-the-art performance on texture-map generation for
                        a given scene mesh. We recommend referring to our Arxiv version at
                        https://arxiv.org/pdf/2307.01097.pdf for the latest update. The project page is at
                        https://mvdiffusion.github.io/.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70011">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-378"></span>

            <script>
        add_bookmark_click(
            70011,
             1,
            'bookmark-number-378',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70011">Proximity-Informed Calibration for Deep Neural
                    Networks</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Miao Xiong · Ailin Deng · Pang Wei Koh · Jiaying Wu · Shen Li · Jianqing Xu · Bryan
                Hooi
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70011">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70011" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70011" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70011">
                        Abstract <i id="caret-70011" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70011">
                <div class="abstract-display">
                    Confidence calibration is central to providing accurate and interpretable uncertainty estimates,
                    especially under safety-critical scenarios. However, we find that existing calibration algorithms
                    often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident
                    in low proximity data (i.e., data lying in the sparse region of the data distribution) compared to
                    high proximity samples, and thus suffer from inconsistent miscalibration across different proximity
                    samples. We examine the problem over $504$ pretrained ImageNet models and observe that: 1) Proximity
                    bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are
                    relatively more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even
                    after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit
                    more heavily on low proximity samples than on high proximity samples. Motivated by the empirical
                    findings, we propose ProCal, a plug-and-play algorithm with a theoretical guarantee to adjust sample
                    confidence based on proximity. To further quantify the effectiveness of calibration algorithms in
                    mitigating proximity bias, we introduce proximity-informed expected calibration error (PIECE) with
                    theoretical analysis. We show that ProCal is effective in addressing proximity bias and improving
                    calibration on balanced, long-tail, and distribution-shift settings under four …
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-73692">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-379"></span>

            <script>
        add_bookmark_click(
            73692,
             1,
            'bookmark-number-379',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/73692">Into the Single Cell Multiverse: an End-to-End
                    Dataset for Procedural Knowledge Extraction in Biomedical Texts</a>
            </div>
            <div class="type_display_name_virtual_card">Poster</div>
            <div class="author-str">Ruth Dannenfelser · Jeffrey Zhong · Ran Zhang · Vicky Yao</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-73692">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/73692-thumb.png?t=1699485107.2381167" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-73692" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-73692" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-73692">
                        Abstract <i id="caret-73692" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-73692">
                <div class="abstract-display">
                    <p>Many of the most commonly explored natural language processing (NLP) information extraction tasks
                        can be thought of as evaluations of declarative knowledge, or fact-based information extraction.
                        Procedural knowledge extraction, i.e., breaking down a described process into a series of steps,
                        has received much less attention, perhaps in part due to the lack of structured datasets that
                        capture the knowledge extraction process from end-to-end. To address this unmet need, we present
                        FlaMBé (Flow annotations for Multiverse Biological entities), a collection of expert-curated
                        datasets across a series of complementary tasks that capture procedural knowledge in biomedical
                        texts. This dataset is inspired by the observation that one ubiquitous source of procedural
                        knowledge that is described as unstructured text is within academic papers describing their
                        methodology. The workflows annotated in FlaMBé are from texts in the burgeoning field of single
                        cell research, a research area that has become notorious for the number of software tools and
                        complexity of workflows used. Additionally, FlaMBé provides, to our knowledge, the largest
                        manually curated named entity recognition (NER) and disambiguation (NED) datasets for
                        tissue/cell type, a fundamental biological entity that is critical for knowledge extraction in
                        the biomedical research domain. Beyond providing a valuable dataset to enable further …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-69882">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-380"></span>

            <script>
        add_bookmark_click(
            69882,
             1,
            'bookmark-number-380',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/69882">Curriculum Learning With Infant Egocentric
                    Videos</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Saber Sheybani · Himanshu Hansaria · Justin Wood · Linda Smith · Zoran Tiganj</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-69882">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/69882-thumb.png?t=1701719319.2997134" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-69882" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-69882" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-69882">
                        Abstract <i id="caret-69882" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-69882">
                <div class="abstract-display">
                    <p>Infants possess a remarkable ability to rapidly learn and process visual inputs. As an infant's
                        mobility increases, so does the variety and dynamics of their visual inputs. Is this change in
                        the properties of the visual inputs beneficial or even critical for the proper development of
                        the visual system? To address this question, we used video recordings from infants wearing
                        head-mounted cameras to train a variety of self-supervised learning models. Critically, we
                        separated the infant data by age group and evaluated the importance of training with a
                        curriculum aligned with developmental order. We found that initiating learning with the data
                        from the youngest age group provided the strongest learning signal and led to the best learning
                        outcomes in terms of downstream task performance. We then showed that the benefits of the data
                        from the youngest age group are due to the slowness and simplicity of the visual experience. The
                        results provide strong empirical evidence for the importance of the properties of the early
                        infant experience and developmental progression in training. More broadly, our approach and
                        findings take a noteworthy step towards reverse engineering the learning mechanisms in newborn
                        brains using image-computable models from artificial intelligence.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70194">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-381"></span>

            <script>
        add_bookmark_click(
            70194,
             1,
            'bookmark-number-381',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70194">Stable Diffusion is Unstable</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Chengbin Du · Yanxi Li · Zhongwei Qiu · Chang Xu</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70194">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70194-thumb.png?t=1699940866.5189335" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70194" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70194" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70194">
                        Abstract <i id="caret-70194" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70194">
                <div class="abstract-display">
                    <p>Recently, text-to-image models have been thriving. Despite their powerful generative capacity,
                        our research has uncovered a lack of robustness in this generation process. Specifically, the
                        introduction of small perturbations to the text prompts can result in the blending of primary
                        subjects with other categories or their complete disappearance in the generated images. In this
                        paper, we propose <strong>Auto-attack on Text-to-image Models (ATM)</strong>, a gradient-based
                        approach, to effectively and efficiently generate such perturbations. By learning a Gumbel
                        Softmax distribution, we can make the discrete process of word replacement or extension
                        continuous, thus ensuring the differentiability of the perturbation generation. Once the
                        distribution is learned, ATM can sample multiple attack samples simultaneously. These attack
                        samples can prevent the generative model from generating the desired subjects without tampering
                        with the category keywords in the prompt. ATM has achieved a 91.1\% success rate in short-text
                        attacks and an 81.2\% success rate in long-text attacks. Further empirical analysis revealed
                        three attack patterns based on: 1) variability in generation speed, 2) similarity of
                        coarse-grained characteristics, and 3) polysemy of words. The code is available at
                        https://github.com/duchengbin8/Stable<em>Diffusion</em>is_Unstable</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-69953">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-382"></span>

            <script>
        add_bookmark_click(
            69953,
             1,
            'bookmark-number-382',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/69953">Mechanism Design for Collaborative Normal Mean
                    Estimation</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Yiding Chen · Jerry Zhu · Kirthevasan Kandasamy</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-69953">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-69953" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-69953" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-69953">
                        Abstract <i id="caret-69953" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-69953">
                <div class="abstract-display">
                    We study collaborative normal mean estimation, where $m$ strategic agents collect i.i.d samples from
                    a normal distribution $\mathcal{N}(\mu, \sigma^2)$ at a cost. They all wish to estimate the mean
                    $\mu$. By sharing data with each other, agents can obtain better estimates while keeping the cost of
                    data collection small. To facilitate this collaboration, we wish to design mechanisms that encourage
                    agents to collect a sufficient amount of data and share it truthfully, so that they are all better
                    off than working alone. In naive mechanisms, such as simply pooling and sharing all the data, an
                    individual agent might find it beneficial to under-collect and/or fabricate data, which can lead to
                    poor social outcomes. We design a novel mechanism that overcomes these challenges via two key
                    techniques: first, when sharing the others' data with an agent, the mechanism corrupts this dataset
                    proportional to how much the data reported by the agent differs from the others; second, we design
                    minimax optimal estimators for the corrupted dataset. Our mechanism, which is Nash incentive
                    compatible and individually rational, achieves a social penalty (sum of all agents' estimation
                    errors and data collection costs) that is at most a factor 2 of the global minimum. When …
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-73643">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-383"></span>

            <script>
        add_bookmark_click(
            73643,
             1,
            'bookmark-number-383',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/73643">LLaVA-Med: Training a Large Language-and-Vision
                    Assistant for Biomedicine in One Day</a>
            </div>
            <div class="type_display_name_virtual_card">Poster</div>
            <div class="author-str">Chunyuan Li · Cliff Wong · Sheng Zhang · Naoto Usuyama · Haotian Liu · Jianwei Yang
                · Tristan Naumann · Hoifung Poon · Jianfeng Gao
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-73643">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-73643" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-73643" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-73643">
                        Abstract <i id="caret-73643" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-73643">
                <div class="abstract-display">
                    <p>Conversational generative AI has demonstrated remarkable promise for empowering biomedical
                        practitioners, but current investigations focus on unimodal text. Multimodal conversational AI
                        has seen rapid progress by leveraging billions of image-text pairs from the public web, but such
                        general-domain vision-language models still lack sophistication in understanding and conversing
                        about biomedical images. In this paper, we propose a cost-efficient approach for training a
                        vision-language conversational assistant that can answer open-ended research questions of
                        biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical
                        figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended
                        instruction-following data from the captions, and then fine-tune a large general-domain
                        vision-language model using a novel curriculum learning method. Specifically, the model first
                        learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to
                        master open-ended conversational semantics using GPT-4 generated instruction-following data,
                        broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to
                        train a Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less than 15 hours
                        (with eight A100s). LLaVA-Med exhibits excellent multimodal conversational capability and can
                        follow open-ended instruction to assist with inquiries about a biomedical image. On three
                        standard biomedical visual question answering datasets, LLaVA-Med outperforms …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-69909">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-384"></span>

            <script>
        add_bookmark_click(
            69909,
             1,
            'bookmark-number-384',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/69909">Quasi-Monte Carlo Graph Random Features</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Isaac Reid · Adrian Weller · Krzysztof M Choromanski</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-69909">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/69909-thumb.png?t=1701775687.852298" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-69909" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-69909" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-69909">
                        Abstract <i id="caret-69909" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-69909">
                <div class="abstract-display">
                    We present a novel mechanism to improve the accuracy of the recently-introduced class of graph
                    random features (GRFs). Our method induces negative correlations between the lengths of the
                    algorithm's random walks by imposing antithetic termination: a procedure to sample more diverse
                    random walks which may be of independent interest. It has a trivial drop-in implementation. We
                    derive strong theoretical guarantees on the properties of these quasi-Monte Carlo GRFs (q-GRFs),
                    proving that they yield lower-variance estimators of the $2$-regularised Laplacian kernel under mild
                    conditions. Remarkably, our results hold for any graph topology. We demonstrate empirical accuracy
                    improvements on a variety of tasks including a new practical application: time-efficient
                    approximation of the graph diffusion process. To our knowledge, q-GRFs constitute the first
                    rigorously studied quasi-Monte Carlo scheme for kernels defined on combinatorial objects, inviting
                    new research on correlations between graph random walks.
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70116">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-385"></span>

            <script>
        add_bookmark_click(
            70116,
             1,
            'bookmark-number-385',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70116">Deep Neural Collapse Is Provably Optimal for
                    the Deep Unconstrained Features Model</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Peter Súkeník · Marco Mondelli · Christoph Lampert</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70116">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70116-thumb.png?t=1701824570.6761992" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70116" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70116" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70116">
                        Abstract <i id="caret-70116" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70116">
                <div class="abstract-display">
                    <p>Neural collapse (NC) refers to the surprising structure of the last layer of deep neural networks
                        in the terminal phase of gradient descent training. Recently, an increasing amount of
                        experimental evidence has pointed to the propagation of NC to earlier layers of neural networks.
                        However, while the NC in the last layer is well studied theoretically, much less is known about
                        its multi-layered counterpart - deep neural collapse (DNC). In particular, existing work focuses
                        either on linear layers or only on the last two layers at the price of an extra assumption. Our
                        work fills this gap by generalizing the established analytical framework for NC - the
                        unconstrained features model - to multiple non-linear layers. Our key technical contribution is
                        to show that, in a deep unconstrained features model, the unique global optimum for binary
                        classification exhibits all the properties typical of DNC. This explains the existing
                        experimental evidence of DNC. We also empirically show that (i) by optimizing deep unconstrained
                        features models via gradient descent, the resulting solution agrees well with our theory, and
                        (ii) trained networks recover the unconstrained features suitable for the occurrence of DNC,
                        thus supporting the validity of this modeling principle.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70160">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-386"></span>

            <script>
        add_bookmark_click(
            70160,
             1,
            'bookmark-number-386',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70160">Exploring Geometry of Blind Spots in Vision
                    models</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Sriram Balasubramanian · Gaurang Sriramanan · Vinu Sankar Sadasivan · Soheil Feizi
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70160">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70160" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70160" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70160">
                        Abstract <i id="caret-70160" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70160">
                <div class="abstract-display">
                    <p>Despite the remarkable success of deep neural networks in a myriad of settings, several works
                        have demonstrated their overwhelming sensitivity to near-imperceptible perturbations, known as
                        adversarial attacks. On the other hand, prior works have also observed that deep networks can be
                        under-sensitive, wherein large-magnitude perturbations in input space do not induce appreciable
                        changes to network activations. In this work, we study in detail the phenomenon of
                        under-sensitivity in vision models such as CNNs and Transformers, and present techniques to
                        study the geometry and extent of “equi-confidence” level sets of such networks. We propose a
                        Level Set Traversal algorithm that iteratively explores regions of high confidence with respect
                        to the input space using orthogonal components of the local gradients. Given a source image, we
                        use this algorithm to identify inputs that lie in the same equi-confidence level set as the
                        source image despite being perceptually similar to arbitrary images from other classes. We
                        further observe that the source image is linearly connected by a high-confidence path to these
                        inputs, uncovering a star-like structure for level sets of deep networks. Furthermore, we
                        attempt to identify and estimate the extent of these connected higher-dimensional regions over
                        which the model maintains a high degree …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70092">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-387"></span>

            <script>
        add_bookmark_click(
            70092,
             1,
            'bookmark-number-387',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70092">Conditional score-based diffusion models for
                    Bayesian inference in infinite dimensions</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Lorenzo Baldassari · Ali Siahkoohi · Josselin Garnier · Knut Solna · Maarten V. de
                Hoop
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70092">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70092-thumb.png?t=1699576320.2739203" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70092" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70092" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70092">
                        Abstract <i id="caret-70092" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70092">
                <div class="abstract-display">
                    <p>Since their initial introduction, score-based diffusion models (SDMs) have been successfully
                        applied to solve a variety of linear inverse problems in finite-dimensional vector spaces due to
                        their ability to efficiently approximate the posterior distribution. However, using SDMs for
                        inverse problems in infinite-dimensional function spaces has only been addressed recently,
                        primarily through methods that learn the unconditional score. While this approach is
                        advantageous for some inverse problems, it is mostly heuristic and involves numerous
                        computationally costly forward operator evaluations during posterior sampling. To address these
                        limitations, we propose a theoretically grounded method for sampling from the posterior of
                        infinite-dimensional Bayesian linear inverse problems based on amortized conditional SDMs. In
                        particular, we prove that one of the most successful approaches for estimating the conditional
                        score in finite dimensions—the conditional denoising estimator—can also be applied in infinite
                        dimensions. A significant part of our analysis is dedicated to demonstrating that extending
                        infinite-dimensional SDMs to the conditional setting requires careful consideration, as the
                        conditional score typically blows up for small times, contrarily to the unconditional score. We
                        conclude by presenting stylized and large-scale numerical examples that validate our approach,
                        offer additional insights, and demonstrate that our method enables large-scale,
                        discretization-invariant Bayesian inference.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70230">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-388"></span>

            <script>
        add_bookmark_click(
            70230,
             1,
            'bookmark-number-388',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70230">Trans-Dimensional Generative Modeling via Jump
                    Diffusion Models</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Andrew Campbell · William Harvey · Christian Weilbach · Valentin De Bortoli · Thomas
                Rainforth · Arnaud Doucet
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70230">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70230-thumb.png?t=1701376311.099529" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70230" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70230" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70230">
                        Abstract <i id="caret-70230" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70230">
                <div class="abstract-display">
                    <p>We propose a new class of generative model that naturally handles data of varying dimensionality
                        by jointly modeling the state and dimension of each datapoint. The generative process is
                        formulated as a jump diffusion process that makes jumps between different dimensional spaces. We
                        first define a dimension destroying forward noising process, before deriving the dimension
                        creating time-reversed generative process along with a novel evidence lower bound training
                        objective for learning to approximate it.Simulating our learned approximation to the
                        time-reversed generative process then provides an effective way of sampling data of varying
                        dimensionality by jointly generating state values and dimensions. We demonstrate our approach on
                        molecular and video datasets of varying dimensionality, reporting better compatibility with
                        test-time diffusion guidance imputation tasks and improved interpolation capabilities versus
                        fixed dimensional models that generate state values and dimensions separately.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-73675">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-389"></span>

            <script>
        add_bookmark_click(
            73675,
             1,
            'bookmark-number-389',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/73675">Digital Typhoon: Long-term Satellite Image
                    Dataset for the Spatio-Temporal Modeling of Tropical Cyclones</a>
            </div>
            <div class="type_display_name_virtual_card">Poster</div>
            <div class="author-str">Asanobu Kitamoto · Jared Hwang · Bastien Vuillod · Lucas Gautier · Yingtao Tian ·
                Tarin Clanuwat
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-73675">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/73675-thumb.png?t=1702028389.3579435" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-73675" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-73675" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-73675">
                        Abstract <i id="caret-73675" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-73675">
                <div class="abstract-display">
                    <p>This paper presents the official release of the Digital Typhoon dataset, the longest typhoon
                        satellite image dataset for 40+ years aimed at benchmarking machine learning models for
                        long-term spatio-temporal data. To build the dataset, we developed a workflow to create an
                        infrared typhoon-centered image for cropping using Lambert azimuthal equal-area projection
                        referring to the best track data. We also address data quality issues such as inter-satellite
                        calibration to create a homogeneous dataset. To take advantage of the dataset, we organized
                        machine learning tasks by the types and targets of inference, with other tasks for
                        meteorological analysis, societal impact, and climate change. The benchmarking results on the
                        analysis, forecasting, and reanalysis for the intensity suggest that the dataset is challenging
                        for recent deep learning models, due to many choices that affect the performance of various
                        models. This dataset reduces the barrier for machine learning researchers to meet large-scale
                        real-world events called tropical cyclones and develop machine learning models that may
                        contribute to advancing scientific knowledge on tropical cyclones as well as solving societal
                        and sustainability issues such as disaster reduction and climate change. The dataset is publicly
                        available at http://agora.ex.nii.ac.jp/digital-typhoon/dataset/ and
                        https://github.com/kitamoto-lab/digital-typhoon/.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70192">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-390"></span>

            <script>
        add_bookmark_click(
            70192,
             1,
            'bookmark-number-390',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70192">Bypassing spike sorting: Density-based decoding
                    using spike localization from dense multielectrode probes</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Yizi Zhang · Tianxiao He · Julien Boussard · Charles Windolf · Olivier Winter · Eric
                Trautmann · Noam Roth · Hailey Barrell · Mark Churchland · Nicholas A Steinmetz · Erdem Varol · Cole
                Hurwitz · Liam Paninski
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70192">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70192-thumb.png?t=1701909927.3173501" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70192" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70192" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70192">
                        Abstract <i id="caret-70192" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70192">
                <div class="abstract-display">
                    <p>Neural decoding and its applications to brain computer interfaces (BCI) are essential for
                        understanding the association between neural activity and behavior. A prerequisite for many
                        decoding approaches is spike sorting, the assignment of action potentials (spikes) to individual
                        neurons. Current spike sorting algorithms, however, can be inaccurate and do not properly model
                        uncertainty of spike assignments, therefore discarding information that could potentially
                        improve decoding performance. Recent advances in high-density probes (e.g., Neuropixels) and
                        computational methods now allow for extracting a rich set of spike features from unsorted data;
                        these features can in turn be used to directly decode behavioral correlates. To this end, we
                        propose a spike sorting-free decoding method that directly models the distribution of extracted
                        spike features using a mixture of Gaussians (MoG) encoding the uncertainty of spike assignments,
                        without aiming to solve the spike clustering problem explicitly. We allow the mixing proportion
                        of the MoG to change over time in response to the behavior and develop variational inference
                        methods to fit the resulting model and to perform decoding. We benchmark our method with an
                        extensive suite of recordings from different animals and probe geometries, demonstrating that
                        our proposed decoder can consistently outperform current methods based on thresholding …</p>
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70168">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-391"></span>

            <script>
        add_bookmark_click(
            70168,
             1,
            'bookmark-number-391',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70168">Real-World Image Variation by Aligning
                    Diffusion Inversion Chain</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Yuechen Zhang · Jinbo Xing · Eric Lo · Jiaya Jia</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70168">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/70168-thumb.png?t=1695891142.0924964" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70168" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70168" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70168">
                        Abstract <i id="caret-70168" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70168">
                <div class="abstract-display">
                    <p>Recent diffusion model advancements have enabled high-fidelity images to be generated using text
                        prompts. However, a domain gap exists between generated images and real-world images, which
                        poses a challenge in generating high-quality variations of real-world images. Our investigation
                        uncovers that this domain gap originates from a latents' distribution gap in different diffusion
                        processes. To address this issue, we propose a novel inference pipeline called Real-world Image
                        Variation by ALignment (RIVAL) that utilizes diffusion models to generate image variations from
                        a single image exemplar. Our pipeline enhances the generation quality of image variations by
                        aligning the image generation process to the source image's inversion chain. Specifically, we
                        demonstrate that step-wise latent distribution alignment is essential for generating
                        high-quality variations. To attain this, we design a cross-image self-attention injection for
                        feature interaction and a step-wise distribution normalization to align the latent features.
                        Incorporating these alignment processes into a diffusion model allows RIVAL to generate
                        high-quality image variations without further parameter optimization. Our experimental results
                        demonstrate that our proposed approach outperforms existing methods concerning semantic
                        similarity and perceptual quality. This generalized inference pipeline can be easily applied to
                        other diffusion-based generation tasks, such as image-conditioned text-to-image generation and
                        stylization. Project page: https://rival-diff.github.io</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-69890">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-392"></span>

            <script>
        add_bookmark_click(
            69890,
             1,
            'bookmark-number-392',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/69890">Unpaired Multi-Domain Causal Representation
                    Learning</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Nils Sturma · Chandler Squires · Mathias Drton · Caroline Uhler</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-69890">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/69890-thumb.png?t=1699436115.7543657" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-69890" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-69890" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-69890">
                        Abstract <i id="caret-69890" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-69890">
                <div class="abstract-display">
                    <p>The goal of causal representation learning is to find a representation of data that consists of
                        causally related latent variables. We consider a setup where one has access to data from
                        multiple domains that potentially share a causal representation. Crucially, observations in
                        different domains are assumed to be unpaired, that is, we only observe the marginal distribution
                        in each domain but not their joint distribution. In this paper, we give sufficient conditions
                        for identifiability of the joint distribution and the shared causal graph in a linear setup.
                        Identifiability holds if we can uniquely recover the joint distribution and the shared causal
                        representation from the marginal distributions in each domain. We transform our results into a
                        practical method to recover the shared latent causal graph.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-69959">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-393"></span>

            <script>
        add_bookmark_click(
            69959,
             1,
            'bookmark-number-393',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/69959">Convergence of Adam Under Relaxed
                    Assumptions</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Haochuan Li · Alexander Rakhlin · Ali Jadbabaie</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-69959">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-69959" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-69959" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-69959">
                        Abstract <i id="caret-69959" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-69959">
                <div class="abstract-display">
                    In this paper, we provide a rigorous proof of convergence of the Adaptive Moment Estimate (Adam)
                    algorithm for a wide class of optimization objectives. Despite the popularity and efficiency of the
                    Adam algorithm in training deep neural networks, its theoretical properties are not yet fully
                    understood, and existing convergence proofs require unrealistically strong assumptions, such as
                    globally bounded gradients, to show the convergence to stationary points. In this paper, we show
                    that Adam provably converges to $\epsilon$-stationary points with $\mathcal{O}(\epsilon^{-4})$
                    gradient complexity under far more realistic conditions. The key to our analysis is a new proof of
                    boundedness of gradients along the optimization trajectory of Adam, under a generalized smoothness
                    assumption according to which the local smoothness (i.e., Hessian norm when it exists) is bounded by
                    a sub-quadratic function of the gradient norm. Moreover, we propose a variance-reduced version of
                    Adam with an accelerated gradient complexity of $\mathcal{O}(\epsilon^{-3})$.
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-73697">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-394"></span>

            <script>
        add_bookmark_click(
            73697,
             1,
            'bookmark-number-394',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/73697">The Waymo Open Sim Agents Challenge</a>
            </div>
            <div class="type_display_name_virtual_card">Poster</div>
            <div class="author-str">Nico Montali · John Lambert · Paul Mougin · Alex Kuefler · Nicholas Rhinehart ·
                Michelle Li · Cole Gulino · Tristan Emrich · Zoey Yang · Shimon Whiteson · Brandyn White · Dragomir
                Anguelov
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-73697">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/media/PosterPDFs/NeurIPS%202023/73697-thumb.png?t=1701870348.9940581" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-73697" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-73697" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-73697">
                        Abstract <i id="caret-73697" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-73697">
                <div class="abstract-display">
                    <p>Simulation with realistic, interactive agents represents a key task for autonomous vehicle
                        software development. In this work, we introduce the Waymo Open Sim Agents Challenge (WOSAC).
                        WOSAC is the first public challenge to tackle this task and propose corresponding metrics. The
                        goal of the challenge is to stimulate the design of realistic simulators that can be used to
                        evaluate and train a behavior model for autonomous driving. We outline our evaluation
                        methodology, present results for a number of different baseline simulation agent methods, and
                        analyze several submissions to the 2023 competition which ran from March 16, 2023 to May 23,
                        2023. The WOSAC evaluation server remains open for submissions and we discuss open problems for
                        the task.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-70220">


            <span class="bookmark-cell far fa-bookmark fa-lg bump20" title="Add/Remove bookmark for this event"
                  id="bookmark-number-395"></span>

            <script>
        add_bookmark_click(
            70220,
             1,
            'bookmark-number-395',
            'Add/Remove bookmark for this event',
            'Add/Remove bookmark for this event'

        )

            </script>


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/70220">A Robust and Opponent-Aware League Training
                    Method for StarCraft II</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Ruozi Huang · Xipeng Wu · Hongsheng Yu · Zhong Fan · Haobo Fu · Qiang Fu · Wei
                Yang
            </div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-70220">Fri 15 Dec 12:00 AM CET</div>


            <p style="font-size:.9em;">[ Great Hall &amp; Hall B1+B2 (level 1) ]</p>


            <img src="/static/core/img/NeurIPS-logo.svg" class="social-img-thumb" alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-70220" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-70220" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-70220">
                        Abstract <i id="caret-70220" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-70220">
                <div class="abstract-display">
                    <p>It is extremely difficult to train a superhuman Artificial Intelligence (AI) for games of similar
                        size to StarCraft II. AlphaStar is the first AI that beat human professionals in the full game
                        of StarCraft II, using a league training framework that is inspired by a game-theoretic
                        approach. In this paper, we improve AlphaStar's league training in two significant aspects. We
                        train goal-conditioned exploiters, whose abilities of spotting weaknesses in the main agent and
                        the entire league are greatly improved compared to the unconditioned exploiters in AlphaStar. In
                        addition, we endow the agents in the league with the new ability of opponent modeling, which
                        makes the agent more responsive to the opponent's real-time strategy. Based on these
                        improvements, we train a better and superhuman AI with orders of magnitude less resources than
                        AlphaStar (see Table 1 for a full comparison). Considering the iconic role of StarCraft II in
                        game AI research, we believe our method and results on StarCraft II provide valuable design
                        principles on how one would utilize the general league training framework for obtaining a
                        least-exploitable strategy in various, large-scale, real-world games.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-72691">


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/72691">Physics-Driven ML-Based Modelling for
                    Correcting Inverse Estimation</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">ruiyuan kang · Tingting Mu · Panagiotis Liatsis · Dimitrios Kyritsis</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-72691"></div>


            <img src="/media/PosterPDFs/NeurIPS%202023/72691-thumb.png?t=1698474029.4360125" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-72691" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-72691" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-72691">
                        Abstract <i id="caret-72691" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-72691">
                <div class="abstract-display">
                    <p>When deploying machine learning estimators in science and engineering (SAE) domains, it is
                        critical to avoid failed estimations that can have disastrous consequences, e.g., in aero engine
                        design. This work focuses on detecting and correcting failed state estimations before adopting
                        them in SAE inverse problems, by utilizing simulations and performance metrics guided by
                        physical laws. We suggest to flag a machine learning estimation when its physical model error
                        exceeds a feasible threshold, and propose a novel approach, GEESE, to correct it through
                        optimization, aiming at delivering both low error and high efficiency. The key designs of GEESE
                        include (1) a hybrid surrogate error model to provide fast error estimations to reduce
                        simulation cost and to enable gradient based backpropagation of error feedback, and (2) two
                        generative models to approximate the probability distributions of the candidate states for
                        simulating the exploitation and exploration behaviours. All three models are constructed as
                        neural networks. GEESE is tested on three real-world SAE inverse problems and compared to a
                        number of state-of-the-art optimization/search approaches. Results show that it fails the least
                        number of times in terms of finding a feasible state correction, and requires physical
                        evaluations less frequently in general.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-71728">


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/71728">Vulnerabilities in Video Quality Assessment
                    Models: The Challenge of Adversarial Attacks</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Aoxiang Zhang · Yu Ran · Weixuan Tang · Yuan-Gen Wang</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-71728"></div>


            <img src="/media/PosterPDFs/NeurIPS%202023/71728-thumb.png?t=1697640604.36304" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-71728" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-71728" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-71728">
                        Abstract <i id="caret-71728" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-71728">
                <div class="abstract-display">
                    No-Reference Video Quality Assessment (NR-VQA) plays an essential role in improving the viewing
                    experience of end-users. Driven by deep learning, recent NR-VQA models based on Convolutional Neural
                    Networks (CNNs) and Transformers have achieved outstanding performance. To build a reliable and
                    practical assessment system, it is of great necessity to evaluate their robustness. However, such
                    issue has received little attention in the academic community. In this paper, we make the first
                    attempt to evaluate the robustness of NR-VQA models againstadversarial attacks, and propose a
                    patch-based random search method for black-box attack. Specifically, considering both the attack
                    effect on quality score and the visual quality of adversarial video, the attack problem is
                    formulated as misleading the estimated quality score under the constraint of just-noticeable
                    difference (JND). Built upon such formulation, a novel loss function called Score-Reversed Boundary
                    Loss is designed to push the adversarial video’s estimated quality score far away from its
                    ground-truth score towards a specific boundary, and the JND constraint is modeled as a strict $L_2$
                    and $L_\infty$ norm restriction. By this means, both white-box and black-box attacks can be launched
                    in an effective and imperceptible manner. The source code is available at
                    https://github.com/GZHU-DVL/AttackVQA.
                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-73645">


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/73645">An NLP Benchmark Dataset for Assessing
                    Corporate Climate Policy Engagement</a>
            </div>
            <div class="type_display_name_virtual_card">Poster</div>
            <div class="author-str">Gaku Morio · Christopher D Manning</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-73645"></div>


            <img src="/media/PosterPDFs/NeurIPS%202023/73645-thumb.png?t=1700613803.9372616" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-73645" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-73645" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-73645">
                        Abstract <i id="caret-73645" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-73645">
                <div class="abstract-display">
                    <p>As societal awareness of climate change grows, corporate climate policy engagements are
                        attracting attention.We propose a dataset to estimate corporate climate policy engagement from
                        various PDF-formatted documents.Our dataset comes from LobbyMap (a platform operated by global
                        think tank InfluenceMap) that provides engagement categories and stances on the documents.To
                        convert the LobbyMap data into the structured dataset, we developed a pipeline using text
                        extraction and OCR.Our contributions are: (i) Building an NLP dataset including 10K documents on
                        corporate climate policy engagement. (ii) Analyzing the properties and challenges of the
                        dataset. (iii) Providing experiments for the dataset using pre-trained language models.The
                        results show that while Longformer outperforms baselines and other pre-trained models, there is
                        still room for significant improvement.We hope our work begins to bridge research on NLP and
                        climate change.</p>

                </div>
            </div>

        </div>

        <div class="displaycards touchup-date" id="event-72460">


            <div style="width:80%;margin:auto;">
                <a class="small-title" href="/virtual/2023/poster/72460">Optimizing Prompts for Text-to-Image
                    Generation</a>
            </div>
            <div class="type_display_name_virtual_card">Spotlight Poster</div>
            <div class="author-str">Yaru Hao · Zewen Chi · Li Dong · Furu Wei</div>
            <div class="author-str higher"></div>
            <div class="text-muted touchup-date-div" id="touchup-date-event-72460"></div>


            <img src="/media/PosterPDFs/NeurIPS%202023/72460-thumb.png?t=1699880412.42927" class="social-img-thumb"
                 alt="thumbnail">


            <div class="abstract-section">

                <div>
                    <a id="abstract-link-72460" class="abstract-link" data-toggle="collapse"
                       href="#collapse-event-abstract-72460" role="button" aria-expanded="false"
                       aria-controls="collapse-event-abstract-72460">
                        Abstract <i id="caret-72460" class="fas fa-caret-right"></i>
                    </a>
                </div>


            </div>
            <div class="collapse" id="collapse-event-abstract-72460">
                <div class="abstract-display">
                    <p>Well-designed prompts can guide text-to-image models to generate amazing images. However, the
                        performant prompts are often model-specific and misaligned with user input. Instead of laborious
                        human engineering, we propose prompt adaptation, a general framework that automatically adapts
                        original user input to model-preferred prompts. Specifically, we first perform supervised
                        fine-tuning with a pretrained language model on a small collection of manually engineered
                        prompts. Then we use reinforcement learning to explore better prompts. We define a reward
                        function that encourages the policy to generate more aesthetically pleasing images while
                        preserving the original user intentions. Experimental results on Stable Diffusion show that our
                        method outperforms manual prompt engineering in terms of both automatic metrics and human
                        preference ratings. Moreover, reinforcement learning further boosts performance, especially on
                        out-of-domain prompts.</p>

                </div>
            </div>

        </div>

    </div>
</div>